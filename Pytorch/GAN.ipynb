{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets \n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_var(x):\n",
    "    if torch.cuda.is_available():\n",
    "        x = x.cuda()\n",
    "    return Variable(x)\n",
    "\n",
    "def denorm(x):\n",
    "    out = (x + 1) / 2\n",
    "    return out.clamp(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=(0.5, 0.5, 0.5), \n",
    "                                     std=(0.5, 0.5, 0.5))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# MNIST dataset\n",
    "mnist = datasets.MNIST(root='./data/',\n",
    "                       train=True,\n",
    "                       transform=transform,\n",
    "                       download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data loader\n",
    "data_loader = torch.utils.data.DataLoader(dataset=mnist,\n",
    "                                          batch_size=100, \n",
    "                                          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Discriminator\n",
    "D = nn.Sequential(\n",
    "    nn.Linear(784, 256),\n",
    "    nn.LeakyReLU(0.2),\n",
    "    nn.Linear(256, 256),\n",
    "    nn.LeakyReLU(0.2),\n",
    "    nn.Linear(256, 1),\n",
    "    nn.Sigmoid())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential (\n",
       "  (0): Linear (784 -> 256)\n",
       "  (1): LeakyReLU (0.2)\n",
       "  (2): Linear (256 -> 256)\n",
       "  (3): LeakyReLU (0.2)\n",
       "  (4): Linear (256 -> 1)\n",
       "  (5): Sigmoid ()\n",
       ")"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generator \n",
    "G = nn.Sequential(  \n",
    "\n",
    "    nn.Linear(64, 256),\n",
    "    nn.LeakyReLU(0.2),\n",
    "    nn.Linear(256, 256),\n",
    "    nn.LeakyReLU(0.2),\n",
    "    nn.Linear(256, 784),\n",
    "    nn.Tanh())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential (\n",
       "  (0): Linear (64 -> 256)\n",
       "  (1): LeakyReLU (0.2)\n",
       "  (2): Linear (256 -> 256)\n",
       "  (3): LeakyReLU (0.2)\n",
       "  (4): Linear (256 -> 784)\n",
       "  (5): Tanh ()\n",
       ")"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    D.cuda()\n",
    "    G.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Binary cross entropy loss and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "d_optimizer = torch.optim.Adam(D.parameters(), lr=0.0003)\n",
    "g_optimizer = torch.optim.Adam(G.parameters(), lr=0.0003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cs/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:767: UserWarning: Using a target size (torch.Size([100])) that is different to the input size (torch.Size([100, 1])) is deprecated. Please ensure they have the same size.\n",
      "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/200], Step[300/600], d_loss: 0.2191, g_loss: 5.2262, D(x): 0.95, D(G(z)): 0.13\n",
      "Epoch [0/200], Step[600/600], d_loss: 0.1543, g_loss: 3.9950, D(x): 0.95, D(G(z)): 0.08\n",
      "Epoch [1/200], Step[300/600], d_loss: 4.8604, g_loss: 0.9520, D(x): 0.25, D(G(z)): 0.75\n",
      "Epoch [1/200], Step[600/600], d_loss: 1.8055, g_loss: 1.2955, D(x): 0.44, D(G(z)): 0.33\n",
      "Epoch [2/200], Step[300/600], d_loss: 0.6069, g_loss: 2.1154, D(x): 0.84, D(G(z)): 0.30\n",
      "Epoch [2/200], Step[600/600], d_loss: 0.5387, g_loss: 1.6506, D(x): 0.78, D(G(z)): 0.20\n",
      "Epoch [3/200], Step[300/600], d_loss: 0.8337, g_loss: 1.6816, D(x): 0.69, D(G(z)): 0.28\n",
      "Epoch [3/200], Step[600/600], d_loss: 0.6997, g_loss: 5.5467, D(x): 0.90, D(G(z)): 0.28\n",
      "Epoch [4/200], Step[300/600], d_loss: 0.4559, g_loss: 3.3642, D(x): 0.85, D(G(z)): 0.18\n",
      "Epoch [4/200], Step[600/600], d_loss: 1.1886, g_loss: 2.0739, D(x): 0.61, D(G(z)): 0.25\n",
      "Epoch [5/200], Step[300/600], d_loss: 1.0453, g_loss: 1.5698, D(x): 0.67, D(G(z)): 0.38\n",
      "Epoch [5/200], Step[600/600], d_loss: 0.8311, g_loss: 1.9252, D(x): 0.64, D(G(z)): 0.21\n",
      "Epoch [6/200], Step[300/600], d_loss: 0.9862, g_loss: 1.6386, D(x): 0.70, D(G(z)): 0.39\n",
      "Epoch [6/200], Step[600/600], d_loss: 0.8633, g_loss: 1.3973, D(x): 0.70, D(G(z)): 0.34\n",
      "Epoch [7/200], Step[300/600], d_loss: 0.7592, g_loss: 1.8511, D(x): 0.76, D(G(z)): 0.34\n",
      "Epoch [7/200], Step[600/600], d_loss: 1.5019, g_loss: 0.9333, D(x): 0.50, D(G(z)): 0.44\n",
      "Epoch [8/200], Step[300/600], d_loss: 1.3668, g_loss: 2.1773, D(x): 0.56, D(G(z)): 0.41\n",
      "Epoch [8/200], Step[600/600], d_loss: 0.6222, g_loss: 4.0819, D(x): 0.80, D(G(z)): 0.20\n",
      "Epoch [9/200], Step[300/600], d_loss: 0.7855, g_loss: 2.0439, D(x): 0.71, D(G(z)): 0.25\n",
      "Epoch [9/200], Step[600/600], d_loss: 1.4197, g_loss: 1.2657, D(x): 0.65, D(G(z)): 0.50\n",
      "Epoch [10/200], Step[300/600], d_loss: 0.4269, g_loss: 2.1556, D(x): 0.83, D(G(z)): 0.18\n",
      "Epoch [10/200], Step[600/600], d_loss: 1.3505, g_loss: 0.8873, D(x): 0.62, D(G(z)): 0.47\n",
      "Epoch [11/200], Step[300/600], d_loss: 0.9111, g_loss: 2.0425, D(x): 0.65, D(G(z)): 0.26\n",
      "Epoch [11/200], Step[600/600], d_loss: 0.7145, g_loss: 2.0636, D(x): 0.81, D(G(z)): 0.26\n",
      "Epoch [12/200], Step[300/600], d_loss: 1.1494, g_loss: 1.5180, D(x): 0.67, D(G(z)): 0.34\n",
      "Epoch [12/200], Step[600/600], d_loss: 1.3041, g_loss: 1.8656, D(x): 0.58, D(G(z)): 0.29\n",
      "Epoch [13/200], Step[300/600], d_loss: 1.8257, g_loss: 1.1906, D(x): 0.62, D(G(z)): 0.59\n",
      "Epoch [13/200], Step[600/600], d_loss: 1.0432, g_loss: 1.4575, D(x): 0.71, D(G(z)): 0.37\n",
      "Epoch [14/200], Step[300/600], d_loss: 0.6402, g_loss: 2.2127, D(x): 0.84, D(G(z)): 0.26\n",
      "Epoch [14/200], Step[600/600], d_loss: 0.8475, g_loss: 2.0013, D(x): 0.77, D(G(z)): 0.30\n",
      "Epoch [15/200], Step[300/600], d_loss: 0.9678, g_loss: 1.5165, D(x): 0.76, D(G(z)): 0.37\n",
      "Epoch [15/200], Step[600/600], d_loss: 0.6663, g_loss: 2.9056, D(x): 0.70, D(G(z)): 0.16\n",
      "Epoch [16/200], Step[300/600], d_loss: 0.7285, g_loss: 2.7220, D(x): 0.75, D(G(z)): 0.21\n",
      "Epoch [16/200], Step[600/600], d_loss: 0.6539, g_loss: 3.7253, D(x): 0.75, D(G(z)): 0.16\n",
      "Epoch [17/200], Step[300/600], d_loss: 0.5996, g_loss: 1.9501, D(x): 0.85, D(G(z)): 0.27\n",
      "Epoch [17/200], Step[600/600], d_loss: 0.7238, g_loss: 2.4787, D(x): 0.74, D(G(z)): 0.16\n",
      "Epoch [18/200], Step[300/600], d_loss: 1.1542, g_loss: 1.5314, D(x): 0.64, D(G(z)): 0.33\n",
      "Epoch [18/200], Step[600/600], d_loss: 1.0970, g_loss: 1.8423, D(x): 0.72, D(G(z)): 0.32\n",
      "Epoch [19/200], Step[300/600], d_loss: 0.7900, g_loss: 2.1163, D(x): 0.75, D(G(z)): 0.25\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-115-16ef5e473314>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mg_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0mg_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    154\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \"\"\"\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 98\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "for epoch in range(20):\n",
    "    for i, (images, _) in enumerate(data_loader):\n",
    "        # Build mini-batch dataset\n",
    "        batch_size = images.size(0)\n",
    "        images = to_var(images.view(batch_size, -1))\n",
    "        \n",
    "        # Create the labels which are later used as input for the BCE loss\n",
    "        real_labels = to_var(torch.ones(batch_size))\n",
    "        fake_labels = to_var(torch.zeros(batch_size))\n",
    "\n",
    "        #============= Train the discriminator =============#\n",
    "        # Compute BCE_Loss using real images where BCE_Loss(x, y): - y * log(D(x)) - (1-y) * log(1 - D(x))\n",
    "        # Second term of the loss is always zero since real_labels == 1\n",
    "        outputs = D(images)\n",
    "        d_loss_real = criterion(outputs, real_labels)\n",
    "        real_score = outputs\n",
    "        \n",
    "        # Compute BCELoss using fake images\n",
    "        # First term of the loss is always zero since fake_labels == 0\n",
    "        z = to_var(torch.randn(batch_size, 64))\n",
    "        fake_images = G(z)\n",
    "        outputs = D(fake_images)\n",
    "        d_loss_fake = criterion(outputs, fake_labels)\n",
    "        fake_score = outputs\n",
    "        \n",
    "        # Backprop + Optimize\n",
    "        d_loss = d_loss_real + d_loss_fake\n",
    "        D.zero_grad()\n",
    "        d_loss.backward()\n",
    "        d_optimizer.step()\n",
    "        \n",
    "        #=============== Train the generator ===============#\n",
    "        # Compute loss with fake images\n",
    "        z = to_var(torch.randn(batch_size, 64))\n",
    "        fake_images = G(z)\n",
    "        outputs = D(fake_images)\n",
    "        \n",
    "        # We train G to maximize log(D(G(z)) instead of minimizing log(1-D(G(z)))\n",
    "        # For the reason, see the last paragraph of section 3. https://arxiv.org/pdf/1406.2661.pdf\n",
    "        g_loss = criterion(outputs, real_labels)\n",
    "        \n",
    "        # Backprop + Optimize\n",
    "        D.zero_grad()\n",
    "        G.zero_grad()\n",
    "        g_loss.backward()\n",
    "        g_optimizer.step()\n",
    "        \n",
    "        if (i+1) % 300 == 0:\n",
    "            print('Epoch [%d/%d], Step[%d/%d], d_loss: %.4f, '\n",
    "                  'g_loss: %.4f, D(x): %.2f, D(G(z)): %.2f' \n",
    "                  %(epoch, 200, i+1, 600, d_loss.data[0], g_loss.data[0],\n",
    "                    real_score.data.mean(), fake_score.data.mean()))\n",
    "    \n",
    "    # Save real images\n",
    "    if (epoch+1) == 1:\n",
    "        images = images.view(images.size(0), 1, 28, 28)\n",
    "        save_image(denorm(images.data), './data/real_images.png')\n",
    "    \n",
    "    # Save sampled images\n",
    "    fake_images = fake_images.view(fake_images.size(0), 1, 28, 28)\n",
    "    save_image(denorm(fake_images.data), './data/fake_images-%d.png' %(epoch+1))\n",
    "\n",
    "# Save the trained parameters \n",
    "torch.save(G.state_dict(), './generator.pkl')\n",
    "torch.save(D.state_dict(), './discriminator.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'to_var' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-57d7c8fae97d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# D(images)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_var\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnz\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'to_var' is not defined"
     ]
    }
   ],
   "source": [
    "# D(images)\n",
    "batch_size=100\n",
    "z = to_var(torch.randn(batch_size, nz,1,1))\n",
    "G(z).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 1, 1, 1])"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = to_var(torch.randn(batch_size, 1,22,22))\n",
    "D(z).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "nc=1\n",
    "nz=100\n",
    "ngf=64\n",
    "ndf=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# custom weights initialization called on netG and netD\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential (\n",
       "  (0): ConvTranspose2d(100, 512, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "  (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
       "  (2): ReLU (inplace)\n",
       "  (3): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "  (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "  (5): ReLU (inplace)\n",
       "  (6): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "  (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "  (8): ReLU (inplace)\n",
       "  (9): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "  (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "  (11): ReLU (inplace)\n",
       "  (12): ConvTranspose2d(64, 1, kernel_size=(2, 2), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "  (13): Tanh ()\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    D.cuda()\n",
    "    G.cuda()\n",
    "D.apply(weights_init)\n",
    "G.apply(weights_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Binary cross entropy loss and optimizer\n",
    "beta1=0.5\n",
    "criterion = nn.BCELoss()\n",
    "optimizerD = torch.optim.Adam(D.parameters(), lr=0.00001, betas=(beta1, 0.999))\n",
    "optimizerG = torch.optim.Adam(G.parameters(), lr=0.00001, betas=(beta1, 0.999))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self,nc,ndf):\n",
    "        super(Discriminator,self).__init__()\n",
    "        # 32 x 32\n",
    "        self.layer1 = nn.Sequential(nn.Conv2d(nc,ndf,kernel_size=4,stride=2,padding=1),\n",
    "                                 nn.BatchNorm2d(ndf),\n",
    "                                 nn.LeakyReLU(0.2,inplace=True))\n",
    "        # 16 x 16\n",
    "        self.layer2 = nn.Sequential(nn.Conv2d(ndf,ndf*2,kernel_size=4,stride=2,padding=1),\n",
    "                                 nn.BatchNorm2d(ndf*2),\n",
    "                                 nn.LeakyReLU(0.2,inplace=True))\n",
    "        # 8 x 8\n",
    "        self.layer3 = nn.Sequential(nn.Conv2d(ndf*2,ndf*4,kernel_size=4,stride=2,padding=1),\n",
    "                                 nn.BatchNorm2d(ndf*4),\n",
    "                                 nn.LeakyReLU(0.2,inplace=True))\n",
    "        # 4 x 4\n",
    "        self.layer4 = nn.Sequential(nn.Conv2d(ndf*4,1,kernel_size=4,stride=1,padding=0),\n",
    "                                 nn.Sigmoid())\n",
    "\n",
    "    def forward(self,x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, nc, ngf, nz):\n",
    "        super(Generator,self).__init__()\n",
    "        self.layer1 = nn.Sequential(nn.ConvTranspose2d(nz,ngf*4,kernel_size=4),\n",
    "                                 nn.BatchNorm2d(ngf*4),\n",
    "                                 nn.ReLU())\n",
    "        # 4 x 4\n",
    "        self.layer2 = nn.Sequential(nn.ConvTranspose2d(ngf*4,ngf*2,kernel_size=4,stride=2,padding=1),\n",
    "                                 nn.BatchNorm2d(ngf*2),\n",
    "                                 nn.ReLU())\n",
    "        # 8 x 8\n",
    "        self.layer3 = nn.Sequential(nn.ConvTranspose2d(ngf*2,ngf,kernel_size=4,stride=2,padding=1),\n",
    "                                 nn.BatchNorm2d(ngf),\n",
    "                                 nn.ReLU())\n",
    "        # 16 x 16\n",
    "        self.layer4 = nn.Sequential(nn.ConvTranspose2d(ngf,nc,kernel_size=4,stride=2,padding=1),\n",
    "                                 nn.Tanh())\n",
    "\n",
    "    def forward(self,x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "D = Discriminator(nc, ndf).cuda()\n",
    "G = Generator(nc, ngf, nz).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary cross entropy loss and optimizer\n",
    "beta1=0.5\n",
    "criterion = nn.BCELoss()\n",
    "optimizerD = torch.optim.Adam(D.parameters(), lr=0.00001, betas=(beta1, 0.999))\n",
    "optimizerG = torch.optim.Adam(G.parameters(), lr=0.00001, betas=(beta1, 0.999))\n",
    "z = torch.FloatTensor(batch_size, nz, 1, 1)\n",
    "z = Variable(z).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cs/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:767: UserWarning: Using a target size (torch.Size([100])) that is different to the input size (torch.Size([100, 1, 1, 1])) is deprecated. Please ensure they have the same size.\n",
      "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/200], Step[300/600], d_loss: 0.4787, g_loss: 1.4048, D(x): 0.85, D(G(z)): 0.26\n",
      "Epoch [0/200], Step[600/600], d_loss: 0.2817, g_loss: 2.1890, D(x): 0.87, D(G(z)): 0.13\n",
      "Epoch [1/200], Step[300/600], d_loss: 0.2124, g_loss: 2.4150, D(x): 0.91, D(G(z)): 0.11\n",
      "Epoch [1/200], Step[600/600], d_loss: 0.1622, g_loss: 2.7221, D(x): 0.93, D(G(z)): 0.09\n",
      "Epoch [2/200], Step[300/600], d_loss: 0.1296, g_loss: 3.1097, D(x): 0.93, D(G(z)): 0.05\n",
      "Epoch [2/200], Step[600/600], d_loss: 0.0659, g_loss: 3.5999, D(x): 0.97, D(G(z)): 0.03\n",
      "Epoch [3/200], Step[300/600], d_loss: 0.0449, g_loss: 4.0774, D(x): 0.98, D(G(z)): 0.02\n",
      "Epoch [3/200], Step[600/600], d_loss: 0.0396, g_loss: 4.2771, D(x): 0.98, D(G(z)): 0.02\n",
      "Epoch [4/200], Step[300/600], d_loss: 0.0198, g_loss: 4.8313, D(x): 0.99, D(G(z)): 0.01\n",
      "Epoch [4/200], Step[600/600], d_loss: 0.0166, g_loss: 5.7256, D(x): 0.99, D(G(z)): 0.00\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "for epoch in range(5):\n",
    "    for i, (images, _) in enumerate(data_loader):\n",
    "        # Build mini-batch dataset\n",
    "        batch_size = images.size(0)\n",
    "        images.view(batch_size, -1)\n",
    "        images = to_var(images.view(batch_size, 1,32,32))\n",
    "        \n",
    "        # Create the labels which are later used as input for the BCE loss\n",
    "        real_labels = to_var(torch.ones(batch_size))\n",
    "        fake_labels = to_var(torch.zeros(batch_size))\n",
    "        \n",
    "        #============= Train the discriminator =============#\n",
    "        # Compute BCE_Loss using real images where BCE_Loss(x, y): - y * log(D(x)) - (1-y) * log(1 - D(x))\n",
    "        # Second term of the loss is always zero since real_labels == 1\n",
    "        D.zero_grad()\n",
    "        outputs = D(images)\n",
    "        d_loss_real = criterion(outputs, real_labels)\n",
    "        d_loss_real.backward()\n",
    "        real_score = outputs\n",
    "        \n",
    "        # Compute BCELoss using fake images\n",
    "        # First term of the loss is always zero since fake_labels == 0\n",
    "        z.data.resize_(images.size(0), nz, 1, 1)\n",
    "        z.data.normal_(0,1)\n",
    "        fake_images = G(z)\n",
    "        outputs = D(fake_images.detach())\n",
    "        d_loss_fake = criterion(outputs, fake_labels)\n",
    "        fake_score = outputs\n",
    "        d_loss_fake.backward()\n",
    "        # Backprop + Optimize\n",
    "        \n",
    "#         D.zero_grad()\n",
    "        d_loss = d_loss_real + d_loss_fake\n",
    "      \n",
    "       \n",
    "        optimizerD.step()\n",
    "        \n",
    "        #=============== Train the generator ===============#\n",
    "        # Compute loss with fake images\n",
    "        G.zero_grad()\n",
    "#         z = to_var(torch.randn(batch_size, nz,1,1))\n",
    "#         fake_images = G(z)\n",
    "        outputs = D(fake_images)\n",
    "        \n",
    "        # We train G to maximize log(D(G(z)) instead of minimizing log(1-D(G(z)))\n",
    "        # For the reason, see the last paragraph of section 3. https://arxiv.org/pdf/1406.2661.pdf\n",
    "        g_loss = criterion(outputs, real_labels)\n",
    "        \n",
    "        # Backprop + Optimize\n",
    "#         D.zero_grad()\n",
    "        \n",
    "        g_loss.backward()\n",
    "        optimizerG.step()\n",
    "        \n",
    "        if (i+1) % 300 == 0:\n",
    "            print('Epoch [%d/%d], Step[%d/%d], d_loss: %.4f, '\n",
    "                  'g_loss: %.4f, D(x): %.2f, D(G(z)): %.2f' \n",
    "                  %(epoch, 200, i+1, 600, d_loss.data[0], g_loss.data[0],\n",
    "                    real_score.data.mean(), fake_score.data.mean()))\n",
    "    \n",
    "    # Save real images\n",
    "    if (epoch+1) == 1:\n",
    "        images = images.view(images.size(0), 1, images.size(2), images.size(3))\n",
    "        save_image(denorm(images.data), './data/real_images.png')\n",
    "    \n",
    "    # Save sampled images\n",
    "    fake_images = fake_images.view(fake_images.size(0), 1, fake_images.size(2), fake_images.size(3))\n",
    "    save_image(denorm(fake_images.data), './data/FFFFfake_images-%d.png' %(epoch+1))\n",
    "\n",
    "# Save the trained parameters \n",
    "torch.save(G.state_dict(), './dcgenerator.pkl')\n",
    "torch.save(D.state_dict(), './dcdiscriminator.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f7207dc4198>"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmgMFtQXGAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f728e3e66d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib.pyplot import imshow\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# imshow(c,cmap='Greys_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "import torchvision.datasets as datasets\n",
    "from torch.autograd import Variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Image processing \n",
    "img_size=32\n",
    "transform = transforms.Compose([transforms.Scale(img_size),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=(0.5, 0.5, 0.5), \n",
    "                                     std=(0.5, 0.5, 0.5))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST dataset\n",
    "mnist = datasets.MNIST(root='./data/',\n",
    "                       train=True,\n",
    "                       transform=transform,\n",
    "                       download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loader\n",
    "batch_size=100\n",
    "nc=1\n",
    "nz=100\n",
    "ngf=64\n",
    "ndf=64\n",
    "niter=10\n",
    "outf='./'\n",
    "data_loader = torch.utils.data.DataLoader(dataset=mnist,\n",
    "                                          batch_size=100, \n",
    "                                          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########   GLOBAL VARIABLES   ###########\n",
    "noise = torch.FloatTensor(batch_size, nz, 1, 1)\n",
    "real = torch.FloatTensor(batch_size, nc, img_size, img_size)\n",
    "label = torch.FloatTensor(batch_size)\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "\n",
    "noise = Variable(noise)\n",
    "real = Variable(real)\n",
    "label = Variable(label)\n",
    "\n",
    "noise = noise.cuda()\n",
    "real = real.cuda()\n",
    "label = label.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "netD = Discriminator(nc, ndf).cuda()\n",
    "netG = Generator(nc, ngf, nz=100).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary cross entropy loss and optimizer\n",
    "beta1=0.5\n",
    "criterion = nn.BCELoss()\n",
    "optimizerD = torch.optim.Adam(netD.parameters(), lr=0.00001, betas=(beta1, 0.999))\n",
    "optimizerG = torch.optim.Adam(netG.parameters(), lr=0.00001, betas=(beta1, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cs/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:767: UserWarning: Using a target size (torch.Size([100])) that is different to the input size (torch.Size([100, 1, 1, 1])) is deprecated. Please ensure they have the same size.\n",
      "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/10][0/600] Loss_D: 1.4021 Loss_G: 0.7209 \n",
      "[1/10][1/600] Loss_D: 1.3885 Loss_G: 0.7217 \n",
      "[1/10][2/600] Loss_D: 1.3907 Loss_G: 0.7023 \n",
      "[1/10][3/600] Loss_D: 1.3742 Loss_G: 0.7025 \n",
      "[1/10][4/600] Loss_D: 1.3365 Loss_G: 0.7266 \n",
      "[1/10][5/600] Loss_D: 1.3444 Loss_G: 0.7121 \n",
      "[1/10][6/600] Loss_D: 1.3375 Loss_G: 0.7119 \n",
      "[1/10][7/600] Loss_D: 1.3018 Loss_G: 0.7134 \n",
      "[1/10][8/600] Loss_D: 1.3039 Loss_G: 0.7106 \n",
      "[1/10][9/600] Loss_D: 1.3008 Loss_G: 0.7011 \n",
      "[1/10][10/600] Loss_D: 1.2674 Loss_G: 0.7195 \n",
      "[1/10][11/600] Loss_D: 1.2565 Loss_G: 0.7146 \n",
      "[1/10][12/600] Loss_D: 1.2712 Loss_G: 0.7059 \n",
      "[1/10][13/600] Loss_D: 1.2356 Loss_G: 0.7220 \n",
      "[1/10][14/600] Loss_D: 1.2382 Loss_G: 0.7145 \n",
      "[1/10][15/600] Loss_D: 1.2099 Loss_G: 0.7247 \n",
      "[1/10][16/600] Loss_D: 1.1878 Loss_G: 0.7390 \n",
      "[1/10][17/600] Loss_D: 1.1867 Loss_G: 0.7364 \n",
      "[1/10][18/600] Loss_D: 1.1692 Loss_G: 0.7310 \n",
      "[1/10][19/600] Loss_D: 1.1638 Loss_G: 0.7242 \n",
      "[1/10][20/600] Loss_D: 1.1714 Loss_G: 0.7230 \n",
      "[1/10][21/600] Loss_D: 1.1540 Loss_G: 0.7295 \n",
      "[1/10][22/600] Loss_D: 1.1374 Loss_G: 0.7263 \n",
      "[1/10][23/600] Loss_D: 1.0992 Loss_G: 0.7628 \n",
      "[1/10][24/600] Loss_D: 1.1179 Loss_G: 0.7332 \n",
      "[1/10][25/600] Loss_D: 1.1070 Loss_G: 0.7344 \n",
      "[1/10][26/600] Loss_D: 1.1035 Loss_G: 0.7409 \n",
      "[1/10][27/600] Loss_D: 1.1056 Loss_G: 0.7457 \n",
      "[1/10][28/600] Loss_D: 1.0998 Loss_G: 0.7445 \n",
      "[1/10][29/600] Loss_D: 1.0703 Loss_G: 0.7553 \n",
      "[1/10][30/600] Loss_D: 1.0811 Loss_G: 0.7544 \n",
      "[1/10][31/600] Loss_D: 1.0790 Loss_G: 0.7483 \n",
      "[1/10][32/600] Loss_D: 1.0685 Loss_G: 0.7384 \n",
      "[1/10][33/600] Loss_D: 1.0687 Loss_G: 0.7375 \n",
      "[1/10][34/600] Loss_D: 1.0466 Loss_G: 0.7536 \n",
      "[1/10][35/600] Loss_D: 1.0574 Loss_G: 0.7334 \n",
      "[1/10][36/600] Loss_D: 1.0369 Loss_G: 0.7650 \n",
      "[1/10][37/600] Loss_D: 1.0181 Loss_G: 0.7577 \n",
      "[1/10][38/600] Loss_D: 1.0416 Loss_G: 0.7449 \n",
      "[1/10][39/600] Loss_D: 1.0365 Loss_G: 0.7436 \n",
      "[1/10][40/600] Loss_D: 1.0238 Loss_G: 0.7517 \n",
      "[1/10][41/600] Loss_D: 1.0399 Loss_G: 0.7376 \n",
      "[1/10][42/600] Loss_D: 1.0226 Loss_G: 0.7540 \n",
      "[1/10][43/600] Loss_D: 0.9993 Loss_G: 0.7735 \n",
      "[1/10][44/600] Loss_D: 0.9963 Loss_G: 0.7587 \n",
      "[1/10][45/600] Loss_D: 1.0235 Loss_G: 0.7393 \n",
      "[1/10][46/600] Loss_D: 1.0084 Loss_G: 0.7487 \n",
      "[1/10][47/600] Loss_D: 0.9966 Loss_G: 0.7526 \n",
      "[1/10][48/600] Loss_D: 1.0167 Loss_G: 0.7369 \n",
      "[1/10][49/600] Loss_D: 1.0112 Loss_G: 0.7422 \n",
      "[1/10][50/600] Loss_D: 0.9886 Loss_G: 0.7546 \n",
      "[1/10][51/600] Loss_D: 1.0010 Loss_G: 0.7297 \n",
      "[1/10][52/600] Loss_D: 0.9997 Loss_G: 0.7414 \n",
      "[1/10][53/600] Loss_D: 1.0114 Loss_G: 0.7338 \n",
      "[1/10][54/600] Loss_D: 1.0226 Loss_G: 0.7233 \n",
      "[1/10][55/600] Loss_D: 1.0096 Loss_G: 0.7349 \n",
      "[1/10][56/600] Loss_D: 1.0273 Loss_G: 0.7145 \n",
      "[1/10][57/600] Loss_D: 0.9869 Loss_G: 0.7427 \n",
      "[1/10][58/600] Loss_D: 0.9972 Loss_G: 0.7203 \n",
      "[1/10][59/600] Loss_D: 0.9988 Loss_G: 0.7217 \n",
      "[1/10][60/600] Loss_D: 0.9933 Loss_G: 0.7234 \n",
      "[1/10][61/600] Loss_D: 1.0047 Loss_G: 0.7247 \n",
      "[1/10][62/600] Loss_D: 1.0057 Loss_G: 0.7289 \n",
      "[1/10][63/600] Loss_D: 0.9981 Loss_G: 0.7384 \n",
      "[1/10][64/600] Loss_D: 0.9810 Loss_G: 0.7384 \n",
      "[1/10][65/600] Loss_D: 0.9901 Loss_G: 0.7222 \n",
      "[1/10][66/600] Loss_D: 1.0081 Loss_G: 0.7211 \n",
      "[1/10][67/600] Loss_D: 1.0074 Loss_G: 0.7143 \n",
      "[1/10][68/600] Loss_D: 0.9729 Loss_G: 0.7498 \n",
      "[1/10][69/600] Loss_D: 0.9858 Loss_G: 0.7387 \n",
      "[1/10][70/600] Loss_D: 0.9602 Loss_G: 0.7486 \n",
      "[1/10][71/600] Loss_D: 0.9617 Loss_G: 0.7570 \n",
      "[1/10][72/600] Loss_D: 0.9640 Loss_G: 0.7595 \n",
      "[1/10][73/600] Loss_D: 0.9660 Loss_G: 0.7496 \n",
      "[1/10][74/600] Loss_D: 0.9462 Loss_G: 0.7592 \n",
      "[1/10][75/600] Loss_D: 0.9351 Loss_G: 0.7903 \n",
      "[1/10][76/600] Loss_D: 0.9567 Loss_G: 0.7763 \n",
      "[1/10][77/600] Loss_D: 0.9317 Loss_G: 0.7799 \n",
      "[1/10][78/600] Loss_D: 0.9395 Loss_G: 0.7819 \n",
      "[1/10][79/600] Loss_D: 0.9623 Loss_G: 0.7735 \n",
      "[1/10][80/600] Loss_D: 0.9274 Loss_G: 0.8138 \n",
      "[1/10][81/600] Loss_D: 0.9493 Loss_G: 0.7849 \n",
      "[1/10][82/600] Loss_D: 0.9341 Loss_G: 0.8053 \n",
      "[1/10][83/600] Loss_D: 0.9065 Loss_G: 0.8165 \n",
      "[1/10][84/600] Loss_D: 0.9215 Loss_G: 0.8089 \n",
      "[1/10][85/600] Loss_D: 0.9189 Loss_G: 0.8173 \n",
      "[1/10][86/600] Loss_D: 0.9093 Loss_G: 0.8232 \n",
      "[1/10][87/600] Loss_D: 0.9162 Loss_G: 0.8206 \n",
      "[1/10][88/600] Loss_D: 0.8988 Loss_G: 0.8415 \n",
      "[1/10][89/600] Loss_D: 0.8786 Loss_G: 0.8537 \n",
      "[1/10][90/600] Loss_D: 0.8866 Loss_G: 0.8521 \n",
      "[1/10][91/600] Loss_D: 0.9141 Loss_G: 0.8370 \n",
      "[1/10][92/600] Loss_D: 0.8874 Loss_G: 0.8478 \n",
      "[1/10][93/600] Loss_D: 0.8912 Loss_G: 0.8469 \n",
      "[1/10][94/600] Loss_D: 0.8959 Loss_G: 0.8561 \n",
      "[1/10][95/600] Loss_D: 0.9046 Loss_G: 0.8727 \n",
      "[1/10][96/600] Loss_D: 0.8671 Loss_G: 0.8579 \n",
      "[1/10][97/600] Loss_D: 0.8594 Loss_G: 0.8708 \n",
      "[1/10][98/600] Loss_D: 0.9001 Loss_G: 0.8615 \n",
      "[1/10][99/600] Loss_D: 0.8840 Loss_G: 0.8792 \n",
      "[1/10][100/600] Loss_D: 0.8799 Loss_G: 0.8812 \n",
      "[1/10][101/600] Loss_D: 0.8505 Loss_G: 0.8998 \n",
      "[1/10][102/600] Loss_D: 0.8357 Loss_G: 0.9067 \n",
      "[1/10][103/600] Loss_D: 0.8655 Loss_G: 0.9026 \n",
      "[1/10][104/600] Loss_D: 0.8527 Loss_G: 0.8867 \n",
      "[1/10][105/600] Loss_D: 0.8445 Loss_G: 0.9237 \n",
      "[1/10][106/600] Loss_D: 0.8502 Loss_G: 0.9101 \n",
      "[1/10][107/600] Loss_D: 0.8255 Loss_G: 0.9126 \n",
      "[1/10][108/600] Loss_D: 0.8112 Loss_G: 0.9314 \n",
      "[1/10][109/600] Loss_D: 0.8355 Loss_G: 0.9260 \n",
      "[1/10][110/600] Loss_D: 0.8321 Loss_G: 0.9153 \n",
      "[1/10][111/600] Loss_D: 0.8350 Loss_G: 0.9323 \n",
      "[1/10][112/600] Loss_D: 0.7940 Loss_G: 0.9532 \n",
      "[1/10][113/600] Loss_D: 0.8342 Loss_G: 0.9254 \n",
      "[1/10][114/600] Loss_D: 0.8299 Loss_G: 0.9583 \n",
      "[1/10][115/600] Loss_D: 0.8422 Loss_G: 0.9595 \n",
      "[1/10][116/600] Loss_D: 0.8207 Loss_G: 0.9409 \n",
      "[1/10][117/600] Loss_D: 0.8249 Loss_G: 0.9682 \n",
      "[1/10][118/600] Loss_D: 0.8320 Loss_G: 0.9505 \n",
      "[1/10][119/600] Loss_D: 0.8020 Loss_G: 0.9699 \n",
      "[1/10][120/600] Loss_D: 0.8001 Loss_G: 0.9832 \n",
      "[1/10][121/600] Loss_D: 0.8214 Loss_G: 0.9505 \n",
      "[1/10][122/600] Loss_D: 0.8324 Loss_G: 0.9365 \n",
      "[1/10][123/600] Loss_D: 0.8000 Loss_G: 0.9509 \n",
      "[1/10][124/600] Loss_D: 0.8086 Loss_G: 0.9575 \n",
      "[1/10][125/600] Loss_D: 0.8013 Loss_G: 0.9649 \n",
      "[1/10][126/600] Loss_D: 0.8090 Loss_G: 0.9548 \n",
      "[1/10][127/600] Loss_D: 0.8554 Loss_G: 0.9625 \n",
      "[1/10][128/600] Loss_D: 0.8159 Loss_G: 0.9655 \n",
      "[1/10][129/600] Loss_D: 0.8346 Loss_G: 0.9848 \n",
      "[1/10][130/600] Loss_D: 0.8213 Loss_G: 0.9497 \n",
      "[1/10][131/600] Loss_D: 0.8022 Loss_G: 0.9572 \n",
      "[1/10][132/600] Loss_D: 0.8235 Loss_G: 0.9457 \n",
      "[1/10][133/600] Loss_D: 0.8418 Loss_G: 0.9583 \n",
      "[1/10][134/600] Loss_D: 0.8134 Loss_G: 0.9661 \n",
      "[1/10][135/600] Loss_D: 0.8492 Loss_G: 0.9480 \n",
      "[1/10][136/600] Loss_D: 0.8407 Loss_G: 0.9382 \n",
      "[1/10][137/600] Loss_D: 0.7989 Loss_G: 0.9690 \n",
      "[1/10][138/600] Loss_D: 0.8138 Loss_G: 0.9786 \n",
      "[1/10][139/600] Loss_D: 0.8263 Loss_G: 0.9505 \n",
      "[1/10][140/600] Loss_D: 0.7939 Loss_G: 0.9649 \n",
      "[1/10][141/600] Loss_D: 0.8076 Loss_G: 0.9716 \n",
      "[1/10][142/600] Loss_D: 0.8157 Loss_G: 0.9661 \n",
      "[1/10][143/600] Loss_D: 0.8090 Loss_G: 0.9596 \n",
      "[1/10][144/600] Loss_D: 0.8057 Loss_G: 0.9775 \n",
      "[1/10][145/600] Loss_D: 0.8087 Loss_G: 0.9844 \n",
      "[1/10][146/600] Loss_D: 0.8248 Loss_G: 0.9756 \n",
      "[1/10][147/600] Loss_D: 0.8249 Loss_G: 0.9769 \n",
      "[1/10][148/600] Loss_D: 0.7977 Loss_G: 0.9879 \n",
      "[1/10][149/600] Loss_D: 0.7900 Loss_G: 1.0146 \n",
      "[1/10][150/600] Loss_D: 0.8267 Loss_G: 0.9936 \n",
      "[1/10][151/600] Loss_D: 0.8033 Loss_G: 0.9858 \n",
      "[1/10][152/600] Loss_D: 0.7887 Loss_G: 1.0071 \n",
      "[1/10][153/600] Loss_D: 0.8057 Loss_G: 0.9860 \n",
      "[1/10][154/600] Loss_D: 0.7879 Loss_G: 1.0101 \n",
      "[1/10][155/600] Loss_D: 0.7605 Loss_G: 1.0340 \n",
      "[1/10][156/600] Loss_D: 0.7997 Loss_G: 1.0205 \n",
      "[1/10][157/600] Loss_D: 0.7570 Loss_G: 1.0324 \n",
      "[1/10][158/600] Loss_D: 0.7595 Loss_G: 1.0480 \n",
      "[1/10][159/600] Loss_D: 0.7799 Loss_G: 1.0283 \n",
      "[1/10][160/600] Loss_D: 0.7902 Loss_G: 1.0397 \n",
      "[1/10][161/600] Loss_D: 0.7539 Loss_G: 1.0586 \n",
      "[1/10][162/600] Loss_D: 0.7460 Loss_G: 1.0933 \n",
      "[1/10][163/600] Loss_D: 0.7587 Loss_G: 1.0719 \n",
      "[1/10][164/600] Loss_D: 0.7348 Loss_G: 1.0677 \n",
      "[1/10][165/600] Loss_D: 0.7599 Loss_G: 1.0741 \n",
      "[1/10][166/600] Loss_D: 0.7228 Loss_G: 1.0737 \n",
      "[1/10][167/600] Loss_D: 0.7307 Loss_G: 1.0879 \n",
      "[1/10][168/600] Loss_D: 0.7291 Loss_G: 1.0782 \n",
      "[1/10][169/600] Loss_D: 0.7276 Loss_G: 1.0832 \n",
      "[1/10][170/600] Loss_D: 0.7634 Loss_G: 1.0891 \n",
      "[1/10][171/600] Loss_D: 0.7218 Loss_G: 1.0765 \n",
      "[1/10][172/600] Loss_D: 0.7200 Loss_G: 1.1011 \n",
      "[1/10][173/600] Loss_D: 0.7036 Loss_G: 1.1084 \n",
      "[1/10][174/600] Loss_D: 0.7355 Loss_G: 1.0928 \n",
      "[1/10][175/600] Loss_D: 0.6960 Loss_G: 1.1152 \n",
      "[1/10][176/600] Loss_D: 0.7320 Loss_G: 1.1079 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/10][177/600] Loss_D: 0.6952 Loss_G: 1.1113 \n",
      "[1/10][178/600] Loss_D: 0.6861 Loss_G: 1.1175 \n",
      "[1/10][179/600] Loss_D: 0.7073 Loss_G: 1.1003 \n",
      "[1/10][180/600] Loss_D: 0.7062 Loss_G: 1.0912 \n",
      "[1/10][181/600] Loss_D: 0.7056 Loss_G: 1.1114 \n",
      "[1/10][182/600] Loss_D: 0.7236 Loss_G: 1.1155 \n",
      "[1/10][183/600] Loss_D: 0.7395 Loss_G: 1.1368 \n",
      "[1/10][184/600] Loss_D: 0.7333 Loss_G: 1.0944 \n",
      "[1/10][185/600] Loss_D: 0.6767 Loss_G: 1.1436 \n",
      "[1/10][186/600] Loss_D: 0.7077 Loss_G: 1.1203 \n",
      "[1/10][187/600] Loss_D: 0.6903 Loss_G: 1.1295 \n",
      "[1/10][188/600] Loss_D: 0.6873 Loss_G: 1.1335 \n",
      "[1/10][189/600] Loss_D: 0.7043 Loss_G: 1.1175 \n",
      "[1/10][190/600] Loss_D: 0.7071 Loss_G: 1.1145 \n",
      "[1/10][191/600] Loss_D: 0.6888 Loss_G: 1.1386 \n",
      "[1/10][192/600] Loss_D: 0.7017 Loss_G: 1.1517 \n",
      "[1/10][193/600] Loss_D: 0.6769 Loss_G: 1.1447 \n",
      "[1/10][194/600] Loss_D: 0.6747 Loss_G: 1.1438 \n",
      "[1/10][195/600] Loss_D: 0.7085 Loss_G: 1.1203 \n",
      "[1/10][196/600] Loss_D: 0.6775 Loss_G: 1.1357 \n",
      "[1/10][197/600] Loss_D: 0.6445 Loss_G: 1.1404 \n",
      "[1/10][198/600] Loss_D: 0.7012 Loss_G: 1.1584 \n",
      "[1/10][199/600] Loss_D: 0.6133 Loss_G: 1.1917 \n",
      "[1/10][200/600] Loss_D: 0.6737 Loss_G: 1.1825 \n",
      "[1/10][201/600] Loss_D: 0.6975 Loss_G: 1.1495 \n",
      "[1/10][202/600] Loss_D: 0.6864 Loss_G: 1.1499 \n",
      "[1/10][203/600] Loss_D: 0.6846 Loss_G: 1.1479 \n",
      "[1/10][204/600] Loss_D: 0.6512 Loss_G: 1.1776 \n",
      "[1/10][205/600] Loss_D: 0.6542 Loss_G: 1.1788 \n",
      "[1/10][206/600] Loss_D: 0.6575 Loss_G: 1.1949 \n",
      "[1/10][207/600] Loss_D: 0.6610 Loss_G: 1.1602 \n",
      "[1/10][208/600] Loss_D: 0.6638 Loss_G: 1.1772 \n",
      "[1/10][209/600] Loss_D: 0.6496 Loss_G: 1.1758 \n",
      "[1/10][210/600] Loss_D: 0.6877 Loss_G: 1.1595 \n",
      "[1/10][211/600] Loss_D: 0.6819 Loss_G: 1.1954 \n",
      "[1/10][212/600] Loss_D: 0.6749 Loss_G: 1.2023 \n",
      "[1/10][213/600] Loss_D: 0.6342 Loss_G: 1.1868 \n",
      "[1/10][214/600] Loss_D: 0.6257 Loss_G: 1.1882 \n",
      "[1/10][215/600] Loss_D: 0.6365 Loss_G: 1.1652 \n",
      "[1/10][216/600] Loss_D: 0.6505 Loss_G: 1.2118 \n",
      "[1/10][217/600] Loss_D: 0.6562 Loss_G: 1.1912 \n",
      "[1/10][218/600] Loss_D: 0.6560 Loss_G: 1.1668 \n",
      "[1/10][219/600] Loss_D: 0.6320 Loss_G: 1.1876 \n",
      "[1/10][220/600] Loss_D: 0.6560 Loss_G: 1.1756 \n",
      "[1/10][221/600] Loss_D: 0.6558 Loss_G: 1.1668 \n",
      "[1/10][222/600] Loss_D: 0.6676 Loss_G: 1.1815 \n",
      "[1/10][223/600] Loss_D: 0.6460 Loss_G: 1.1804 \n",
      "[1/10][224/600] Loss_D: 0.6555 Loss_G: 1.1559 \n",
      "[1/10][225/600] Loss_D: 0.6076 Loss_G: 1.2060 \n",
      "[1/10][226/600] Loss_D: 0.6151 Loss_G: 1.1909 \n",
      "[1/10][227/600] Loss_D: 0.6195 Loss_G: 1.2054 \n",
      "[1/10][228/600] Loss_D: 0.6436 Loss_G: 1.1817 \n",
      "[1/10][229/600] Loss_D: 0.6161 Loss_G: 1.1676 \n",
      "[1/10][230/600] Loss_D: 0.6389 Loss_G: 1.2022 \n",
      "[1/10][231/600] Loss_D: 0.6698 Loss_G: 1.1768 \n",
      "[1/10][232/600] Loss_D: 0.6248 Loss_G: 1.1749 \n",
      "[1/10][233/600] Loss_D: 0.6369 Loss_G: 1.1964 \n",
      "[1/10][234/600] Loss_D: 0.6338 Loss_G: 1.1862 \n",
      "[1/10][235/600] Loss_D: 0.6689 Loss_G: 1.1601 \n",
      "[1/10][236/600] Loss_D: 0.6360 Loss_G: 1.1699 \n",
      "[1/10][237/600] Loss_D: 0.6039 Loss_G: 1.1729 \n",
      "[1/10][238/600] Loss_D: 0.6453 Loss_G: 1.1496 \n",
      "[1/10][239/600] Loss_D: 0.5993 Loss_G: 1.1847 \n",
      "[1/10][240/600] Loss_D: 0.6487 Loss_G: 1.1931 \n",
      "[1/10][241/600] Loss_D: 0.6259 Loss_G: 1.2082 \n",
      "[1/10][242/600] Loss_D: 0.6463 Loss_G: 1.1647 \n",
      "[1/10][243/600] Loss_D: 0.6273 Loss_G: 1.1592 \n",
      "[1/10][244/600] Loss_D: 0.6315 Loss_G: 1.1594 \n",
      "[1/10][245/600] Loss_D: 0.6454 Loss_G: 1.1897 \n",
      "[1/10][246/600] Loss_D: 0.6456 Loss_G: 1.1784 \n",
      "[1/10][247/600] Loss_D: 0.6451 Loss_G: 1.1676 \n",
      "[1/10][248/600] Loss_D: 0.6569 Loss_G: 1.1820 \n",
      "[1/10][249/600] Loss_D: 0.6462 Loss_G: 1.1799 \n",
      "[1/10][250/600] Loss_D: 0.6598 Loss_G: 1.1472 \n",
      "[1/10][251/600] Loss_D: 0.6843 Loss_G: 1.1444 \n",
      "[1/10][252/600] Loss_D: 0.6595 Loss_G: 1.1725 \n",
      "[1/10][253/600] Loss_D: 0.6737 Loss_G: 1.1276 \n",
      "[1/10][254/600] Loss_D: 0.6391 Loss_G: 1.1500 \n",
      "[1/10][255/600] Loss_D: 0.6547 Loss_G: 1.1615 \n",
      "[1/10][256/600] Loss_D: 0.7022 Loss_G: 1.1336 \n",
      "[1/10][257/600] Loss_D: 0.6565 Loss_G: 1.1510 \n",
      "[1/10][258/600] Loss_D: 0.6414 Loss_G: 1.1450 \n",
      "[1/10][259/600] Loss_D: 0.6551 Loss_G: 1.1661 \n",
      "[1/10][260/600] Loss_D: 0.6696 Loss_G: 1.1529 \n",
      "[1/10][261/600] Loss_D: 0.6817 Loss_G: 1.1655 \n",
      "[1/10][262/600] Loss_D: 0.6027 Loss_G: 1.1837 \n",
      "[1/10][263/600] Loss_D: 0.6518 Loss_G: 1.1679 \n",
      "[1/10][264/600] Loss_D: 0.6419 Loss_G: 1.1625 \n",
      "[1/10][265/600] Loss_D: 0.6180 Loss_G: 1.1708 \n",
      "[1/10][266/600] Loss_D: 0.6393 Loss_G: 1.1897 \n",
      "[1/10][267/600] Loss_D: 0.6349 Loss_G: 1.1649 \n",
      "[1/10][268/600] Loss_D: 0.6605 Loss_G: 1.1581 \n",
      "[1/10][269/600] Loss_D: 0.5917 Loss_G: 1.2005 \n",
      "[1/10][270/600] Loss_D: 0.6433 Loss_G: 1.1696 \n",
      "[1/10][271/600] Loss_D: 0.6257 Loss_G: 1.1733 \n",
      "[1/10][272/600] Loss_D: 0.6879 Loss_G: 1.1749 \n",
      "[1/10][273/600] Loss_D: 0.6191 Loss_G: 1.2111 \n",
      "[1/10][274/600] Loss_D: 0.6523 Loss_G: 1.2087 \n",
      "[1/10][275/600] Loss_D: 0.6209 Loss_G: 1.2119 \n",
      "[1/10][276/600] Loss_D: 0.6232 Loss_G: 1.2293 \n",
      "[1/10][277/600] Loss_D: 0.6250 Loss_G: 1.2066 \n",
      "[1/10][278/600] Loss_D: 0.5946 Loss_G: 1.2558 \n",
      "[1/10][279/600] Loss_D: 0.6373 Loss_G: 1.2167 \n",
      "[1/10][280/600] Loss_D: 0.6292 Loss_G: 1.2264 \n",
      "[1/10][281/600] Loss_D: 0.5828 Loss_G: 1.2517 \n",
      "[1/10][282/600] Loss_D: 0.5917 Loss_G: 1.2768 \n",
      "[1/10][283/600] Loss_D: 0.6075 Loss_G: 1.2757 \n",
      "[1/10][284/600] Loss_D: 0.5601 Loss_G: 1.3071 \n",
      "[1/10][285/600] Loss_D: 0.5719 Loss_G: 1.3131 \n",
      "[1/10][286/600] Loss_D: 0.5724 Loss_G: 1.3124 \n",
      "[1/10][287/600] Loss_D: 0.5934 Loss_G: 1.3398 \n",
      "[1/10][288/600] Loss_D: 0.5779 Loss_G: 1.2939 \n",
      "[1/10][289/600] Loss_D: 0.5787 Loss_G: 1.3352 \n",
      "[1/10][290/600] Loss_D: 0.5763 Loss_G: 1.3248 \n",
      "[1/10][291/600] Loss_D: 0.5787 Loss_G: 1.3129 \n",
      "[1/10][292/600] Loss_D: 0.5720 Loss_G: 1.3233 \n",
      "[1/10][293/600] Loss_D: 0.5669 Loss_G: 1.3408 \n",
      "[1/10][294/600] Loss_D: 0.5539 Loss_G: 1.3445 \n",
      "[1/10][295/600] Loss_D: 0.5695 Loss_G: 1.3318 \n",
      "[1/10][296/600] Loss_D: 0.5540 Loss_G: 1.3638 \n",
      "[1/10][297/600] Loss_D: 0.5616 Loss_G: 1.3395 \n",
      "[1/10][298/600] Loss_D: 0.5253 Loss_G: 1.3767 \n",
      "[1/10][299/600] Loss_D: 0.5535 Loss_G: 1.3857 \n",
      "[1/10][300/600] Loss_D: 0.5356 Loss_G: 1.3530 \n",
      "[1/10][301/600] Loss_D: 0.5529 Loss_G: 1.3690 \n",
      "[1/10][302/600] Loss_D: 0.5174 Loss_G: 1.3602 \n",
      "[1/10][303/600] Loss_D: 0.5274 Loss_G: 1.3674 \n",
      "[1/10][304/600] Loss_D: 0.5252 Loss_G: 1.3508 \n",
      "[1/10][305/600] Loss_D: 0.5572 Loss_G: 1.3681 \n",
      "[1/10][306/600] Loss_D: 0.5656 Loss_G: 1.3623 \n",
      "[1/10][307/600] Loss_D: 0.5696 Loss_G: 1.3391 \n",
      "[1/10][308/600] Loss_D: 0.5477 Loss_G: 1.3376 \n",
      "[1/10][309/600] Loss_D: 0.5653 Loss_G: 1.3187 \n",
      "[1/10][310/600] Loss_D: 0.5563 Loss_G: 1.3402 \n",
      "[1/10][311/600] Loss_D: 0.5660 Loss_G: 1.3367 \n",
      "[1/10][312/600] Loss_D: 0.5404 Loss_G: 1.3216 \n",
      "[1/10][313/600] Loss_D: 0.5578 Loss_G: 1.3126 \n",
      "[1/10][314/600] Loss_D: 0.5489 Loss_G: 1.3461 \n",
      "[1/10][315/600] Loss_D: 0.5892 Loss_G: 1.3054 \n",
      "[1/10][316/600] Loss_D: 0.5182 Loss_G: 1.3365 \n",
      "[1/10][317/600] Loss_D: 0.5288 Loss_G: 1.3721 \n",
      "[1/10][318/600] Loss_D: 0.5469 Loss_G: 1.3365 \n",
      "[1/10][319/600] Loss_D: 0.5629 Loss_G: 1.3156 \n",
      "[1/10][320/600] Loss_D: 0.5738 Loss_G: 1.3112 \n",
      "[1/10][321/600] Loss_D: 0.5600 Loss_G: 1.3224 \n",
      "[1/10][322/600] Loss_D: 0.5520 Loss_G: 1.3536 \n",
      "[1/10][323/600] Loss_D: 0.5186 Loss_G: 1.3491 \n",
      "[1/10][324/600] Loss_D: 0.5590 Loss_G: 1.3437 \n",
      "[1/10][325/600] Loss_D: 0.5445 Loss_G: 1.3317 \n",
      "[1/10][326/600] Loss_D: 0.5650 Loss_G: 1.3477 \n",
      "[1/10][327/600] Loss_D: 0.5630 Loss_G: 1.3503 \n",
      "[1/10][328/600] Loss_D: 0.5654 Loss_G: 1.3736 \n",
      "[1/10][329/600] Loss_D: 0.5402 Loss_G: 1.3673 \n",
      "[1/10][330/600] Loss_D: 0.6030 Loss_G: 1.3281 \n",
      "[1/10][331/600] Loss_D: 0.5679 Loss_G: 1.3676 \n",
      "[1/10][332/600] Loss_D: 0.5915 Loss_G: 1.3848 \n",
      "[1/10][333/600] Loss_D: 0.5500 Loss_G: 1.3871 \n",
      "[1/10][334/600] Loss_D: 0.5449 Loss_G: 1.3846 \n",
      "[1/10][335/600] Loss_D: 0.5542 Loss_G: 1.3966 \n",
      "[1/10][336/600] Loss_D: 0.5459 Loss_G: 1.4034 \n",
      "[1/10][337/600] Loss_D: 0.5252 Loss_G: 1.4322 \n",
      "[1/10][338/600] Loss_D: 0.5676 Loss_G: 1.4276 \n",
      "[1/10][339/600] Loss_D: 0.5342 Loss_G: 1.4516 \n",
      "[1/10][340/600] Loss_D: 0.5487 Loss_G: 1.4577 \n",
      "[1/10][341/600] Loss_D: 0.5206 Loss_G: 1.4955 \n",
      "[1/10][342/600] Loss_D: 0.5129 Loss_G: 1.5086 \n",
      "[1/10][343/600] Loss_D: 0.5387 Loss_G: 1.5086 \n",
      "[1/10][344/600] Loss_D: 0.5154 Loss_G: 1.4823 \n",
      "[1/10][345/600] Loss_D: 0.5354 Loss_G: 1.5046 \n",
      "[1/10][346/600] Loss_D: 0.5559 Loss_G: 1.4831 \n",
      "[1/10][347/600] Loss_D: 0.4948 Loss_G: 1.5011 \n",
      "[1/10][348/600] Loss_D: 0.4953 Loss_G: 1.5271 \n",
      "[1/10][349/600] Loss_D: 0.5103 Loss_G: 1.5377 \n",
      "[1/10][350/600] Loss_D: 0.5313 Loss_G: 1.5196 \n",
      "[1/10][351/600] Loss_D: 0.5326 Loss_G: 1.5115 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/10][352/600] Loss_D: 0.5041 Loss_G: 1.5250 \n",
      "[1/10][353/600] Loss_D: 0.4950 Loss_G: 1.5051 \n",
      "[1/10][354/600] Loss_D: 0.4977 Loss_G: 1.5358 \n",
      "[1/10][355/600] Loss_D: 0.5088 Loss_G: 1.5275 \n",
      "[1/10][356/600] Loss_D: 0.5369 Loss_G: 1.5215 \n",
      "[1/10][357/600] Loss_D: 0.4900 Loss_G: 1.5076 \n",
      "[1/10][358/600] Loss_D: 0.5145 Loss_G: 1.5083 \n",
      "[1/10][359/600] Loss_D: 0.5179 Loss_G: 1.5101 \n",
      "[1/10][360/600] Loss_D: 0.5068 Loss_G: 1.5159 \n",
      "[1/10][361/600] Loss_D: 0.5141 Loss_G: 1.4897 \n",
      "[1/10][362/600] Loss_D: 0.5072 Loss_G: 1.5213 \n",
      "[1/10][363/600] Loss_D: 0.5031 Loss_G: 1.5283 \n",
      "[1/10][364/600] Loss_D: 0.5057 Loss_G: 1.5065 \n",
      "[1/10][365/600] Loss_D: 0.5099 Loss_G: 1.5288 \n",
      "[1/10][366/600] Loss_D: 0.5330 Loss_G: 1.5271 \n",
      "[1/10][367/600] Loss_D: 0.5080 Loss_G: 1.5245 \n",
      "[1/10][368/600] Loss_D: 0.5402 Loss_G: 1.5144 \n",
      "[1/10][369/600] Loss_D: 0.4957 Loss_G: 1.5401 \n",
      "[1/10][370/600] Loss_D: 0.5014 Loss_G: 1.5419 \n",
      "[1/10][371/600] Loss_D: 0.5218 Loss_G: 1.5769 \n",
      "[1/10][372/600] Loss_D: 0.5259 Loss_G: 1.5441 \n",
      "[1/10][373/600] Loss_D: 0.5033 Loss_G: 1.5596 \n",
      "[1/10][374/600] Loss_D: 0.5082 Loss_G: 1.5678 \n",
      "[1/10][375/600] Loss_D: 0.5002 Loss_G: 1.5822 \n",
      "[1/10][376/600] Loss_D: 0.4780 Loss_G: 1.5839 \n",
      "[1/10][377/600] Loss_D: 0.4868 Loss_G: 1.5686 \n",
      "[1/10][378/600] Loss_D: 0.5034 Loss_G: 1.6026 \n",
      "[1/10][379/600] Loss_D: 0.4997 Loss_G: 1.6253 \n",
      "[1/10][380/600] Loss_D: 0.4929 Loss_G: 1.6265 \n",
      "[1/10][381/600] Loss_D: 0.5317 Loss_G: 1.5992 \n",
      "[1/10][382/600] Loss_D: 0.5064 Loss_G: 1.6150 \n",
      "[1/10][383/600] Loss_D: 0.4683 Loss_G: 1.6369 \n",
      "[1/10][384/600] Loss_D: 0.4552 Loss_G: 1.6255 \n",
      "[1/10][385/600] Loss_D: 0.5302 Loss_G: 1.6056 \n",
      "[1/10][386/600] Loss_D: 0.4506 Loss_G: 1.6484 \n",
      "[1/10][387/600] Loss_D: 0.4806 Loss_G: 1.6172 \n",
      "[1/10][388/600] Loss_D: 0.5063 Loss_G: 1.5875 \n",
      "[1/10][389/600] Loss_D: 0.4794 Loss_G: 1.6766 \n",
      "[1/10][390/600] Loss_D: 0.4726 Loss_G: 1.6375 \n",
      "[1/10][391/600] Loss_D: 0.4893 Loss_G: 1.6871 \n",
      "[1/10][392/600] Loss_D: 0.4817 Loss_G: 1.6216 \n",
      "[1/10][393/600] Loss_D: 0.5003 Loss_G: 1.6372 \n",
      "[1/10][394/600] Loss_D: 0.4671 Loss_G: 1.6645 \n",
      "[1/10][395/600] Loss_D: 0.5172 Loss_G: 1.6518 \n",
      "[1/10][396/600] Loss_D: 0.4734 Loss_G: 1.6585 \n",
      "[1/10][397/600] Loss_D: 0.4849 Loss_G: 1.6368 \n",
      "[1/10][398/600] Loss_D: 0.4843 Loss_G: 1.6568 \n",
      "[1/10][399/600] Loss_D: 0.5205 Loss_G: 1.6511 \n",
      "[1/10][400/600] Loss_D: 0.4780 Loss_G: 1.6311 \n",
      "[1/10][401/600] Loss_D: 0.4640 Loss_G: 1.7032 \n",
      "[1/10][402/600] Loss_D: 0.5126 Loss_G: 1.6420 \n",
      "[1/10][403/600] Loss_D: 0.4793 Loss_G: 1.6374 \n",
      "[1/10][404/600] Loss_D: 0.4703 Loss_G: 1.6465 \n",
      "[1/10][405/600] Loss_D: 0.4941 Loss_G: 1.6090 \n",
      "[1/10][406/600] Loss_D: 0.4680 Loss_G: 1.6312 \n",
      "[1/10][407/600] Loss_D: 0.4910 Loss_G: 1.6191 \n",
      "[1/10][408/600] Loss_D: 0.4640 Loss_G: 1.6756 \n",
      "[1/10][409/600] Loss_D: 0.4874 Loss_G: 1.6382 \n",
      "[1/10][410/600] Loss_D: 0.5028 Loss_G: 1.6461 \n",
      "[1/10][411/600] Loss_D: 0.4916 Loss_G: 1.6077 \n",
      "[1/10][412/600] Loss_D: 0.4758 Loss_G: 1.6069 \n",
      "[1/10][413/600] Loss_D: 0.4806 Loss_G: 1.6125 \n",
      "[1/10][414/600] Loss_D: 0.4735 Loss_G: 1.5964 \n",
      "[1/10][415/600] Loss_D: 0.4815 Loss_G: 1.6419 \n",
      "[1/10][416/600] Loss_D: 0.4701 Loss_G: 1.6325 \n",
      "[1/10][417/600] Loss_D: 0.4989 Loss_G: 1.5795 \n",
      "[1/10][418/600] Loss_D: 0.4988 Loss_G: 1.6103 \n",
      "[1/10][419/600] Loss_D: 0.5023 Loss_G: 1.5940 \n",
      "[1/10][420/600] Loss_D: 0.4779 Loss_G: 1.6114 \n",
      "[1/10][421/600] Loss_D: 0.4842 Loss_G: 1.6151 \n",
      "[1/10][422/600] Loss_D: 0.4684 Loss_G: 1.5665 \n",
      "[1/10][423/600] Loss_D: 0.5131 Loss_G: 1.5934 \n",
      "[1/10][424/600] Loss_D: 0.4809 Loss_G: 1.5949 \n",
      "[1/10][425/600] Loss_D: 0.5038 Loss_G: 1.5827 \n",
      "[1/10][426/600] Loss_D: 0.4709 Loss_G: 1.5951 \n",
      "[1/10][427/600] Loss_D: 0.4877 Loss_G: 1.5391 \n",
      "[1/10][428/600] Loss_D: 0.4763 Loss_G: 1.5927 \n",
      "[1/10][429/600] Loss_D: 0.5257 Loss_G: 1.5484 \n",
      "[1/10][430/600] Loss_D: 0.4681 Loss_G: 1.5421 \n",
      "[1/10][431/600] Loss_D: 0.4595 Loss_G: 1.5573 \n",
      "[1/10][432/600] Loss_D: 0.4534 Loss_G: 1.5724 \n",
      "[1/10][433/600] Loss_D: 0.4682 Loss_G: 1.5548 \n",
      "[1/10][434/600] Loss_D: 0.5090 Loss_G: 1.5887 \n",
      "[1/10][435/600] Loss_D: 0.5035 Loss_G: 1.5263 \n",
      "[1/10][436/600] Loss_D: 0.4753 Loss_G: 1.5275 \n",
      "[1/10][437/600] Loss_D: 0.4932 Loss_G: 1.5548 \n",
      "[1/10][438/600] Loss_D: 0.4833 Loss_G: 1.5346 \n",
      "[1/10][439/600] Loss_D: 0.4570 Loss_G: 1.5451 \n",
      "[1/10][440/600] Loss_D: 0.4845 Loss_G: 1.5358 \n",
      "[1/10][441/600] Loss_D: 0.4773 Loss_G: 1.5342 \n",
      "[1/10][442/600] Loss_D: 0.4899 Loss_G: 1.5495 \n",
      "[1/10][443/600] Loss_D: 0.4595 Loss_G: 1.5386 \n",
      "[1/10][444/600] Loss_D: 0.4467 Loss_G: 1.5839 \n",
      "[1/10][445/600] Loss_D: 0.4435 Loss_G: 1.6081 \n",
      "[1/10][446/600] Loss_D: 0.4520 Loss_G: 1.6104 \n",
      "[1/10][447/600] Loss_D: 0.4454 Loss_G: 1.6274 \n",
      "[1/10][448/600] Loss_D: 0.4165 Loss_G: 1.6247 \n",
      "[1/10][449/600] Loss_D: 0.4194 Loss_G: 1.6836 \n",
      "[1/10][450/600] Loss_D: 0.4159 Loss_G: 1.6784 \n",
      "[1/10][451/600] Loss_D: 0.4082 Loss_G: 1.6781 \n",
      "[1/10][452/600] Loss_D: 0.4078 Loss_G: 1.7057 \n",
      "[1/10][453/600] Loss_D: 0.4043 Loss_G: 1.6982 \n",
      "[1/10][454/600] Loss_D: 0.3826 Loss_G: 1.7362 \n",
      "[1/10][455/600] Loss_D: 0.4374 Loss_G: 1.7119 \n",
      "[1/10][456/600] Loss_D: 0.3576 Loss_G: 1.7745 \n",
      "[1/10][457/600] Loss_D: 0.3862 Loss_G: 1.7583 \n",
      "[1/10][458/600] Loss_D: 0.3789 Loss_G: 1.7845 \n",
      "[1/10][459/600] Loss_D: 0.3449 Loss_G: 1.7581 \n",
      "[1/10][460/600] Loss_D: 0.3649 Loss_G: 1.8263 \n",
      "[1/10][461/600] Loss_D: 0.3574 Loss_G: 1.8185 \n",
      "[1/10][462/600] Loss_D: 0.3496 Loss_G: 1.8136 \n",
      "[1/10][463/600] Loss_D: 0.3616 Loss_G: 1.7915 \n",
      "[1/10][464/600] Loss_D: 0.3562 Loss_G: 1.7969 \n",
      "[1/10][465/600] Loss_D: 0.3330 Loss_G: 1.8325 \n",
      "[1/10][466/600] Loss_D: 0.3496 Loss_G: 1.8034 \n",
      "[1/10][467/600] Loss_D: 0.3458 Loss_G: 1.8324 \n",
      "[1/10][468/600] Loss_D: 0.3625 Loss_G: 1.7724 \n",
      "[1/10][469/600] Loss_D: 0.3729 Loss_G: 1.7661 \n",
      "[1/10][470/600] Loss_D: 0.3293 Loss_G: 1.8417 \n",
      "[1/10][471/600] Loss_D: 0.3634 Loss_G: 1.8240 \n",
      "[1/10][472/600] Loss_D: 0.3512 Loss_G: 1.8062 \n",
      "[1/10][473/600] Loss_D: 0.3639 Loss_G: 1.7996 \n",
      "[1/10][474/600] Loss_D: 0.3238 Loss_G: 1.8064 \n",
      "[1/10][475/600] Loss_D: 0.3652 Loss_G: 1.7734 \n",
      "[1/10][476/600] Loss_D: 0.3630 Loss_G: 1.7841 \n",
      "[1/10][477/600] Loss_D: 0.3759 Loss_G: 1.7146 \n",
      "[1/10][478/600] Loss_D: 0.3788 Loss_G: 1.7414 \n",
      "[1/10][479/600] Loss_D: 0.3637 Loss_G: 1.7689 \n",
      "[1/10][480/600] Loss_D: 0.3854 Loss_G: 1.7119 \n",
      "[1/10][481/600] Loss_D: 0.4045 Loss_G: 1.7485 \n",
      "[1/10][482/600] Loss_D: 0.4063 Loss_G: 1.7512 \n",
      "[1/10][483/600] Loss_D: 0.3988 Loss_G: 1.6811 \n",
      "[1/10][484/600] Loss_D: 0.3921 Loss_G: 1.7115 \n",
      "[1/10][485/600] Loss_D: 0.3986 Loss_G: 1.6873 \n",
      "[1/10][486/600] Loss_D: 0.3876 Loss_G: 1.7518 \n",
      "[1/10][487/600] Loss_D: 0.3746 Loss_G: 1.7277 \n",
      "[1/10][488/600] Loss_D: 0.4123 Loss_G: 1.7160 \n",
      "[1/10][489/600] Loss_D: 0.3963 Loss_G: 1.7549 \n",
      "[1/10][490/600] Loss_D: 0.3923 Loss_G: 1.7352 \n",
      "[1/10][491/600] Loss_D: 0.4238 Loss_G: 1.7411 \n",
      "[1/10][492/600] Loss_D: 0.4137 Loss_G: 1.6919 \n",
      "[1/10][493/600] Loss_D: 0.4063 Loss_G: 1.7527 \n",
      "[1/10][494/600] Loss_D: 0.4455 Loss_G: 1.7008 \n",
      "[1/10][495/600] Loss_D: 0.4336 Loss_G: 1.7057 \n",
      "[1/10][496/600] Loss_D: 0.4415 Loss_G: 1.7317 \n",
      "[1/10][497/600] Loss_D: 0.4315 Loss_G: 1.7004 \n",
      "[1/10][498/600] Loss_D: 0.4441 Loss_G: 1.6709 \n",
      "[1/10][499/600] Loss_D: 0.4319 Loss_G: 1.7159 \n",
      "[1/10][500/600] Loss_D: 0.4296 Loss_G: 1.7099 \n",
      "[1/10][501/600] Loss_D: 0.4215 Loss_G: 1.7459 \n",
      "[1/10][502/600] Loss_D: 0.4754 Loss_G: 1.6740 \n",
      "[1/10][503/600] Loss_D: 0.4360 Loss_G: 1.7421 \n",
      "[1/10][504/600] Loss_D: 0.4095 Loss_G: 1.7718 \n",
      "[1/10][505/600] Loss_D: 0.4259 Loss_G: 1.7525 \n",
      "[1/10][506/600] Loss_D: 0.4372 Loss_G: 1.7137 \n",
      "[1/10][507/600] Loss_D: 0.4332 Loss_G: 1.7554 \n",
      "[1/10][508/600] Loss_D: 0.4584 Loss_G: 1.7459 \n",
      "[1/10][509/600] Loss_D: 0.4774 Loss_G: 1.6835 \n",
      "[1/10][510/600] Loss_D: 0.3896 Loss_G: 1.7749 \n",
      "[1/10][511/600] Loss_D: 0.4499 Loss_G: 1.7281 \n",
      "[1/10][512/600] Loss_D: 0.4845 Loss_G: 1.6702 \n",
      "[1/10][513/600] Loss_D: 0.4644 Loss_G: 1.6598 \n",
      "[1/10][514/600] Loss_D: 0.4429 Loss_G: 1.6830 \n",
      "[1/10][515/600] Loss_D: 0.4600 Loss_G: 1.6415 \n",
      "[1/10][516/600] Loss_D: 0.5059 Loss_G: 1.6395 \n",
      "[1/10][517/600] Loss_D: 0.4952 Loss_G: 1.6843 \n",
      "[1/10][518/600] Loss_D: 0.4621 Loss_G: 1.6543 \n",
      "[1/10][519/600] Loss_D: 0.4996 Loss_G: 1.6772 \n",
      "[1/10][520/600] Loss_D: 0.5408 Loss_G: 1.6450 \n",
      "[1/10][521/600] Loss_D: 0.4706 Loss_G: 1.6116 \n",
      "[1/10][522/600] Loss_D: 0.5221 Loss_G: 1.5808 \n",
      "[1/10][523/600] Loss_D: 0.5221 Loss_G: 1.6116 \n",
      "[1/10][524/600] Loss_D: 0.5304 Loss_G: 1.6102 \n",
      "[1/10][525/600] Loss_D: 0.5537 Loss_G: 1.5790 \n",
      "[1/10][526/600] Loss_D: 0.5474 Loss_G: 1.5686 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/10][527/600] Loss_D: 0.5205 Loss_G: 1.5958 \n",
      "[1/10][528/600] Loss_D: 0.5289 Loss_G: 1.6068 \n",
      "[1/10][529/600] Loss_D: 0.5161 Loss_G: 1.6071 \n",
      "[1/10][530/600] Loss_D: 0.5343 Loss_G: 1.6490 \n",
      "[1/10][531/600] Loss_D: 0.4924 Loss_G: 1.6410 \n",
      "[1/10][532/600] Loss_D: 0.5000 Loss_G: 1.6645 \n",
      "[1/10][533/600] Loss_D: 0.4966 Loss_G: 1.6640 \n",
      "[1/10][534/600] Loss_D: 0.5185 Loss_G: 1.6399 \n",
      "[1/10][535/600] Loss_D: 0.4838 Loss_G: 1.6336 \n",
      "[1/10][536/600] Loss_D: 0.4856 Loss_G: 1.6752 \n",
      "[1/10][537/600] Loss_D: 0.4676 Loss_G: 1.6816 \n",
      "[1/10][538/600] Loss_D: 0.5120 Loss_G: 1.7004 \n",
      "[1/10][539/600] Loss_D: 0.4974 Loss_G: 1.6942 \n",
      "[1/10][540/600] Loss_D: 0.4962 Loss_G: 1.7057 \n",
      "[1/10][541/600] Loss_D: 0.5391 Loss_G: 1.6257 \n",
      "[1/10][542/600] Loss_D: 0.4961 Loss_G: 1.6594 \n",
      "[1/10][543/600] Loss_D: 0.5111 Loss_G: 1.6919 \n",
      "[1/10][544/600] Loss_D: 0.5012 Loss_G: 1.7254 \n",
      "[1/10][545/600] Loss_D: 0.4664 Loss_G: 1.7480 \n",
      "[1/10][546/600] Loss_D: 0.4371 Loss_G: 1.7307 \n",
      "[1/10][547/600] Loss_D: 0.4561 Loss_G: 1.7805 \n",
      "[1/10][548/600] Loss_D: 0.4752 Loss_G: 1.7665 \n",
      "[1/10][549/600] Loss_D: 0.4820 Loss_G: 1.7595 \n",
      "[1/10][550/600] Loss_D: 0.4938 Loss_G: 1.7329 \n",
      "[1/10][551/600] Loss_D: 0.5399 Loss_G: 1.7271 \n",
      "[1/10][552/600] Loss_D: 0.4774 Loss_G: 1.6806 \n",
      "[1/10][553/600] Loss_D: 0.4548 Loss_G: 1.7483 \n",
      "[1/10][554/600] Loss_D: 0.4913 Loss_G: 1.7141 \n",
      "[1/10][555/600] Loss_D: 0.4847 Loss_G: 1.7693 \n",
      "[1/10][556/600] Loss_D: 0.5057 Loss_G: 1.7381 \n",
      "[1/10][557/600] Loss_D: 0.4602 Loss_G: 1.7744 \n",
      "[1/10][558/600] Loss_D: 0.4646 Loss_G: 1.7244 \n",
      "[1/10][559/600] Loss_D: 0.4351 Loss_G: 1.7911 \n",
      "[1/10][560/600] Loss_D: 0.4607 Loss_G: 1.8002 \n",
      "[1/10][561/600] Loss_D: 0.4835 Loss_G: 1.7030 \n",
      "[1/10][562/600] Loss_D: 0.4647 Loss_G: 1.7453 \n",
      "[1/10][563/600] Loss_D: 0.4251 Loss_G: 1.8150 \n",
      "[1/10][564/600] Loss_D: 0.4638 Loss_G: 1.7850 \n",
      "[1/10][565/600] Loss_D: 0.4400 Loss_G: 1.7534 \n",
      "[1/10][566/600] Loss_D: 0.4838 Loss_G: 1.7230 \n",
      "[1/10][567/600] Loss_D: 0.4704 Loss_G: 1.6748 \n",
      "[1/10][568/600] Loss_D: 0.4353 Loss_G: 1.7716 \n",
      "[1/10][569/600] Loss_D: 0.4825 Loss_G: 1.7613 \n",
      "[1/10][570/600] Loss_D: 0.4214 Loss_G: 1.8061 \n",
      "[1/10][571/600] Loss_D: 0.4753 Loss_G: 1.8112 \n",
      "[1/10][572/600] Loss_D: 0.4856 Loss_G: 1.7319 \n",
      "[1/10][573/600] Loss_D: 0.4814 Loss_G: 1.7851 \n",
      "[1/10][574/600] Loss_D: 0.4810 Loss_G: 1.7029 \n",
      "[1/10][575/600] Loss_D: 0.3752 Loss_G: 1.8186 \n",
      "[1/10][576/600] Loss_D: 0.4486 Loss_G: 1.7479 \n",
      "[1/10][577/600] Loss_D: 0.4496 Loss_G: 1.7598 \n",
      "[1/10][578/600] Loss_D: 0.4582 Loss_G: 1.8013 \n",
      "[1/10][579/600] Loss_D: 0.4636 Loss_G: 1.7614 \n",
      "[1/10][580/600] Loss_D: 0.4802 Loss_G: 1.6888 \n",
      "[1/10][581/600] Loss_D: 0.4525 Loss_G: 1.7041 \n",
      "[1/10][582/600] Loss_D: 0.4572 Loss_G: 1.7776 \n",
      "[1/10][583/600] Loss_D: 0.4292 Loss_G: 1.7982 \n",
      "[1/10][584/600] Loss_D: 0.4817 Loss_G: 1.7322 \n",
      "[1/10][585/600] Loss_D: 0.4699 Loss_G: 1.7028 \n",
      "[1/10][586/600] Loss_D: 0.4525 Loss_G: 1.7739 \n",
      "[1/10][587/600] Loss_D: 0.4760 Loss_G: 1.6763 \n",
      "[1/10][588/600] Loss_D: 0.5403 Loss_G: 1.6347 \n",
      "[1/10][589/600] Loss_D: 0.4652 Loss_G: 1.7328 \n",
      "[1/10][590/600] Loss_D: 0.4902 Loss_G: 1.6410 \n",
      "[1/10][591/600] Loss_D: 0.4119 Loss_G: 1.7769 \n",
      "[1/10][592/600] Loss_D: 0.5215 Loss_G: 1.5933 \n",
      "[1/10][593/600] Loss_D: 0.5097 Loss_G: 1.6261 \n",
      "[1/10][594/600] Loss_D: 0.4346 Loss_G: 1.7426 \n",
      "[1/10][595/600] Loss_D: 0.4988 Loss_G: 1.6310 \n",
      "[1/10][596/600] Loss_D: 0.5081 Loss_G: 1.6646 \n",
      "[1/10][597/600] Loss_D: 0.4642 Loss_G: 1.6813 \n",
      "[1/10][598/600] Loss_D: 0.4839 Loss_G: 1.6739 \n",
      "[1/10][599/600] Loss_D: 0.4932 Loss_G: 1.6685 \n",
      "[2/10][0/600] Loss_D: 0.4891 Loss_G: 1.5770 \n",
      "[2/10][1/600] Loss_D: 0.5259 Loss_G: 1.6794 \n",
      "[2/10][2/600] Loss_D: 0.4947 Loss_G: 1.6282 \n",
      "[2/10][3/600] Loss_D: 0.4908 Loss_G: 1.6251 \n",
      "[2/10][4/600] Loss_D: 0.5051 Loss_G: 1.5750 \n",
      "[2/10][5/600] Loss_D: 0.4669 Loss_G: 1.6571 \n",
      "[2/10][6/600] Loss_D: 0.5408 Loss_G: 1.5228 \n",
      "[2/10][7/600] Loss_D: 0.5264 Loss_G: 1.5827 \n",
      "[2/10][8/600] Loss_D: 0.5400 Loss_G: 1.4815 \n",
      "[2/10][9/600] Loss_D: 0.5165 Loss_G: 1.5731 \n",
      "[2/10][10/600] Loss_D: 0.5358 Loss_G: 1.5961 \n",
      "[2/10][11/600] Loss_D: 0.5383 Loss_G: 1.5523 \n",
      "[2/10][12/600] Loss_D: 0.5819 Loss_G: 1.4656 \n",
      "[2/10][13/600] Loss_D: 0.5135 Loss_G: 1.5010 \n",
      "[2/10][14/600] Loss_D: 0.5161 Loss_G: 1.5481 \n",
      "[2/10][15/600] Loss_D: 0.4907 Loss_G: 1.5563 \n",
      "[2/10][16/600] Loss_D: 0.5599 Loss_G: 1.5145 \n",
      "[2/10][17/600] Loss_D: 0.5529 Loss_G: 1.5027 \n",
      "[2/10][18/600] Loss_D: 0.5408 Loss_G: 1.5281 \n",
      "[2/10][19/600] Loss_D: 0.5373 Loss_G: 1.5129 \n",
      "[2/10][20/600] Loss_D: 0.5410 Loss_G: 1.4911 \n",
      "[2/10][21/600] Loss_D: 0.5421 Loss_G: 1.5284 \n",
      "[2/10][22/600] Loss_D: 0.5269 Loss_G: 1.5402 \n",
      "[2/10][23/600] Loss_D: 0.5023 Loss_G: 1.5406 \n",
      "[2/10][24/600] Loss_D: 0.5483 Loss_G: 1.5072 \n",
      "[2/10][25/600] Loss_D: 0.4936 Loss_G: 1.6377 \n",
      "[2/10][26/600] Loss_D: 0.5302 Loss_G: 1.5249 \n",
      "[2/10][27/600] Loss_D: 0.5163 Loss_G: 1.5621 \n",
      "[2/10][28/600] Loss_D: 0.5291 Loss_G: 1.5638 \n",
      "[2/10][29/600] Loss_D: 0.5287 Loss_G: 1.4887 \n",
      "[2/10][30/600] Loss_D: 0.4810 Loss_G: 1.5191 \n",
      "[2/10][31/600] Loss_D: 0.4586 Loss_G: 1.7006 \n",
      "[2/10][32/600] Loss_D: 0.4724 Loss_G: 1.6498 \n",
      "[2/10][33/600] Loss_D: 0.5338 Loss_G: 1.5291 \n",
      "[2/10][34/600] Loss_D: 0.4790 Loss_G: 1.6528 \n",
      "[2/10][35/600] Loss_D: 0.4753 Loss_G: 1.6071 \n",
      "[2/10][36/600] Loss_D: 0.4503 Loss_G: 1.6059 \n",
      "[2/10][37/600] Loss_D: 0.4743 Loss_G: 1.6171 \n",
      "[2/10][38/600] Loss_D: 0.4957 Loss_G: 1.6395 \n",
      "[2/10][39/600] Loss_D: 0.4551 Loss_G: 1.6272 \n",
      "[2/10][40/600] Loss_D: 0.4551 Loss_G: 1.6358 \n",
      "[2/10][41/600] Loss_D: 0.4449 Loss_G: 1.6479 \n",
      "[2/10][42/600] Loss_D: 0.4161 Loss_G: 1.7046 \n",
      "[2/10][43/600] Loss_D: 0.4667 Loss_G: 1.6136 \n",
      "[2/10][44/600] Loss_D: 0.4965 Loss_G: 1.6076 \n",
      "[2/10][45/600] Loss_D: 0.4921 Loss_G: 1.5930 \n",
      "[2/10][46/600] Loss_D: 0.4901 Loss_G: 1.5961 \n",
      "[2/10][47/600] Loss_D: 0.4693 Loss_G: 1.6641 \n",
      "[2/10][48/600] Loss_D: 0.4716 Loss_G: 1.5905 \n",
      "[2/10][49/600] Loss_D: 0.4947 Loss_G: 1.5754 \n",
      "[2/10][50/600] Loss_D: 0.4863 Loss_G: 1.6059 \n",
      "[2/10][51/600] Loss_D: 0.5127 Loss_G: 1.6068 \n",
      "[2/10][52/600] Loss_D: 0.4847 Loss_G: 1.5012 \n",
      "[2/10][53/600] Loss_D: 0.5291 Loss_G: 1.4703 \n",
      "[2/10][54/600] Loss_D: 0.5221 Loss_G: 1.5420 \n",
      "[2/10][55/600] Loss_D: 0.5201 Loss_G: 1.4619 \n",
      "[2/10][56/600] Loss_D: 0.5286 Loss_G: 1.5221 \n",
      "[2/10][57/600] Loss_D: 0.5138 Loss_G: 1.6015 \n",
      "[2/10][58/600] Loss_D: 0.5324 Loss_G: 1.5488 \n",
      "[2/10][59/600] Loss_D: 0.5425 Loss_G: 1.4767 \n",
      "[2/10][60/600] Loss_D: 0.5443 Loss_G: 1.4563 \n",
      "[2/10][61/600] Loss_D: 0.5331 Loss_G: 1.4826 \n",
      "[2/10][62/600] Loss_D: 0.5655 Loss_G: 1.4604 \n",
      "[2/10][63/600] Loss_D: 0.5643 Loss_G: 1.4500 \n",
      "[2/10][64/600] Loss_D: 0.5279 Loss_G: 1.4722 \n",
      "[2/10][65/600] Loss_D: 0.5646 Loss_G: 1.4922 \n",
      "[2/10][66/600] Loss_D: 0.5226 Loss_G: 1.5348 \n",
      "[2/10][67/600] Loss_D: 0.5712 Loss_G: 1.4778 \n",
      "[2/10][68/600] Loss_D: 0.5132 Loss_G: 1.4809 \n",
      "[2/10][69/600] Loss_D: 0.5286 Loss_G: 1.5336 \n",
      "[2/10][70/600] Loss_D: 0.5245 Loss_G: 1.4837 \n",
      "[2/10][71/600] Loss_D: 0.5538 Loss_G: 1.5240 \n",
      "[2/10][72/600] Loss_D: 0.5089 Loss_G: 1.5491 \n",
      "[2/10][73/600] Loss_D: 0.5166 Loss_G: 1.5388 \n",
      "[2/10][74/600] Loss_D: 0.5226 Loss_G: 1.5051 \n",
      "[2/10][75/600] Loss_D: 0.4991 Loss_G: 1.5949 \n",
      "[2/10][76/600] Loss_D: 0.5053 Loss_G: 1.5645 \n",
      "[2/10][77/600] Loss_D: 0.4633 Loss_G: 1.6082 \n",
      "[2/10][78/600] Loss_D: 0.5098 Loss_G: 1.5580 \n",
      "[2/10][79/600] Loss_D: 0.4625 Loss_G: 1.6370 \n",
      "[2/10][80/600] Loss_D: 0.4811 Loss_G: 1.6065 \n",
      "[2/10][81/600] Loss_D: 0.4769 Loss_G: 1.6457 \n",
      "[2/10][82/600] Loss_D: 0.4531 Loss_G: 1.7205 \n",
      "[2/10][83/600] Loss_D: 0.4313 Loss_G: 1.7078 \n",
      "[2/10][84/600] Loss_D: 0.4658 Loss_G: 1.6503 \n",
      "[2/10][85/600] Loss_D: 0.4242 Loss_G: 1.7295 \n",
      "[2/10][86/600] Loss_D: 0.4796 Loss_G: 1.6692 \n",
      "[2/10][87/600] Loss_D: 0.4520 Loss_G: 1.6953 \n",
      "[2/10][88/600] Loss_D: 0.4815 Loss_G: 1.6529 \n",
      "[2/10][89/600] Loss_D: 0.5109 Loss_G: 1.6421 \n",
      "[2/10][90/600] Loss_D: 0.4270 Loss_G: 1.7389 \n",
      "[2/10][91/600] Loss_D: 0.4249 Loss_G: 1.7646 \n",
      "[2/10][92/600] Loss_D: 0.4097 Loss_G: 1.8191 \n",
      "[2/10][93/600] Loss_D: 0.4446 Loss_G: 1.7245 \n",
      "[2/10][94/600] Loss_D: 0.4405 Loss_G: 1.7824 \n",
      "[2/10][95/600] Loss_D: 0.4462 Loss_G: 1.7462 \n",
      "[2/10][96/600] Loss_D: 0.4206 Loss_G: 1.7849 \n",
      "[2/10][97/600] Loss_D: 0.4154 Loss_G: 1.7417 \n",
      "[2/10][98/600] Loss_D: 0.4301 Loss_G: 1.8012 \n",
      "[2/10][99/600] Loss_D: 0.4171 Loss_G: 1.7836 \n",
      "[2/10][100/600] Loss_D: 0.4370 Loss_G: 1.7816 \n",
      "[2/10][101/600] Loss_D: 0.4443 Loss_G: 1.8219 \n",
      "[2/10][102/600] Loss_D: 0.4241 Loss_G: 1.7783 \n",
      "[2/10][103/600] Loss_D: 0.4250 Loss_G: 1.7682 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2/10][104/600] Loss_D: 0.4257 Loss_G: 1.8030 \n",
      "[2/10][105/600] Loss_D: 0.3965 Loss_G: 1.8558 \n",
      "[2/10][106/600] Loss_D: 0.3808 Loss_G: 1.8610 \n",
      "[2/10][107/600] Loss_D: 0.4405 Loss_G: 1.8057 \n",
      "[2/10][108/600] Loss_D: 0.4010 Loss_G: 1.8355 \n",
      "[2/10][109/600] Loss_D: 0.4312 Loss_G: 1.8090 \n",
      "[2/10][110/600] Loss_D: 0.4347 Loss_G: 1.7933 \n",
      "[2/10][111/600] Loss_D: 0.4024 Loss_G: 1.8093 \n",
      "[2/10][112/600] Loss_D: 0.4270 Loss_G: 1.7926 \n",
      "[2/10][113/600] Loss_D: 0.4040 Loss_G: 1.8197 \n",
      "[2/10][114/600] Loss_D: 0.4015 Loss_G: 1.8113 \n",
      "[2/10][115/600] Loss_D: 0.3889 Loss_G: 1.8304 \n",
      "[2/10][116/600] Loss_D: 0.4420 Loss_G: 1.7871 \n",
      "[2/10][117/600] Loss_D: 0.4342 Loss_G: 1.7901 \n",
      "[2/10][118/600] Loss_D: 0.4775 Loss_G: 1.7496 \n",
      "[2/10][119/600] Loss_D: 0.4279 Loss_G: 1.7765 \n",
      "[2/10][120/600] Loss_D: 0.4252 Loss_G: 1.7581 \n",
      "[2/10][121/600] Loss_D: 0.4096 Loss_G: 1.7888 \n",
      "[2/10][122/600] Loss_D: 0.4113 Loss_G: 1.7572 \n",
      "[2/10][123/600] Loss_D: 0.4812 Loss_G: 1.7383 \n",
      "[2/10][124/600] Loss_D: 0.4861 Loss_G: 1.7202 \n",
      "[2/10][125/600] Loss_D: 0.4413 Loss_G: 1.7103 \n",
      "[2/10][126/600] Loss_D: 0.4540 Loss_G: 1.7092 \n",
      "[2/10][127/600] Loss_D: 0.4568 Loss_G: 1.7169 \n",
      "[2/10][128/600] Loss_D: 0.5251 Loss_G: 1.6813 \n",
      "[2/10][129/600] Loss_D: 0.4924 Loss_G: 1.6409 \n",
      "[2/10][130/600] Loss_D: 0.4524 Loss_G: 1.7376 \n",
      "[2/10][131/600] Loss_D: 0.4512 Loss_G: 1.6718 \n",
      "[2/10][132/600] Loss_D: 0.4738 Loss_G: 1.7480 \n",
      "[2/10][133/600] Loss_D: 0.4651 Loss_G: 1.7039 \n",
      "[2/10][134/600] Loss_D: 0.4683 Loss_G: 1.6743 \n",
      "[2/10][135/600] Loss_D: 0.4671 Loss_G: 1.6888 \n",
      "[2/10][136/600] Loss_D: 0.4516 Loss_G: 1.7090 \n",
      "[2/10][137/600] Loss_D: 0.4340 Loss_G: 1.7594 \n",
      "[2/10][138/600] Loss_D: 0.4882 Loss_G: 1.6683 \n",
      "[2/10][139/600] Loss_D: 0.4892 Loss_G: 1.7222 \n",
      "[2/10][140/600] Loss_D: 0.4694 Loss_G: 1.6452 \n",
      "[2/10][141/600] Loss_D: 0.4884 Loss_G: 1.6914 \n",
      "[2/10][142/600] Loss_D: 0.4799 Loss_G: 1.7003 \n",
      "[2/10][143/600] Loss_D: 0.4307 Loss_G: 1.7284 \n",
      "[2/10][144/600] Loss_D: 0.4235 Loss_G: 1.7785 \n",
      "[2/10][145/600] Loss_D: 0.4622 Loss_G: 1.6869 \n",
      "[2/10][146/600] Loss_D: 0.4310 Loss_G: 1.7722 \n",
      "[2/10][147/600] Loss_D: 0.4314 Loss_G: 1.7521 \n",
      "[2/10][148/600] Loss_D: 0.4344 Loss_G: 1.7982 \n",
      "[2/10][149/600] Loss_D: 0.4525 Loss_G: 1.8071 \n",
      "[2/10][150/600] Loss_D: 0.3806 Loss_G: 1.8414 \n",
      "[2/10][151/600] Loss_D: 0.3947 Loss_G: 1.8356 \n",
      "[2/10][152/600] Loss_D: 0.4372 Loss_G: 1.8074 \n",
      "[2/10][153/600] Loss_D: 0.3776 Loss_G: 1.8753 \n",
      "[2/10][154/600] Loss_D: 0.4168 Loss_G: 1.8873 \n",
      "[2/10][155/600] Loss_D: 0.3686 Loss_G: 1.8804 \n",
      "[2/10][156/600] Loss_D: 0.4225 Loss_G: 1.8501 \n",
      "[2/10][157/600] Loss_D: 0.3705 Loss_G: 1.8748 \n",
      "[2/10][158/600] Loss_D: 0.3830 Loss_G: 1.9272 \n",
      "[2/10][159/600] Loss_D: 0.4092 Loss_G: 1.8606 \n",
      "[2/10][160/600] Loss_D: 0.3743 Loss_G: 1.8806 \n",
      "[2/10][161/600] Loss_D: 0.3988 Loss_G: 1.8904 \n",
      "[2/10][162/600] Loss_D: 0.3525 Loss_G: 1.9055 \n",
      "[2/10][163/600] Loss_D: 0.3953 Loss_G: 1.9378 \n",
      "[2/10][164/600] Loss_D: 0.3517 Loss_G: 1.9105 \n",
      "[2/10][165/600] Loss_D: 0.3434 Loss_G: 1.9508 \n",
      "[2/10][166/600] Loss_D: 0.3515 Loss_G: 1.9842 \n",
      "[2/10][167/600] Loss_D: 0.4095 Loss_G: 1.8653 \n",
      "[2/10][168/600] Loss_D: 0.3613 Loss_G: 1.9104 \n",
      "[2/10][169/600] Loss_D: 0.3254 Loss_G: 1.9323 \n",
      "[2/10][170/600] Loss_D: 0.3620 Loss_G: 1.9856 \n",
      "[2/10][171/600] Loss_D: 0.3542 Loss_G: 1.9441 \n",
      "[2/10][172/600] Loss_D: 0.3526 Loss_G: 1.9784 \n",
      "[2/10][173/600] Loss_D: 0.3341 Loss_G: 1.9807 \n",
      "[2/10][174/600] Loss_D: 0.3726 Loss_G: 1.9152 \n",
      "[2/10][175/600] Loss_D: 0.3525 Loss_G: 1.9037 \n",
      "[2/10][176/600] Loss_D: 0.3793 Loss_G: 1.8892 \n",
      "[2/10][177/600] Loss_D: 0.3657 Loss_G: 1.9148 \n",
      "[2/10][178/600] Loss_D: 0.3576 Loss_G: 1.9212 \n",
      "[2/10][179/600] Loss_D: 0.3909 Loss_G: 1.8202 \n",
      "[2/10][180/600] Loss_D: 0.3703 Loss_G: 1.9013 \n",
      "[2/10][181/600] Loss_D: 0.3739 Loss_G: 1.9007 \n",
      "[2/10][182/600] Loss_D: 0.3809 Loss_G: 1.8868 \n",
      "[2/10][183/600] Loss_D: 0.3785 Loss_G: 1.8595 \n",
      "[2/10][184/600] Loss_D: 0.4056 Loss_G: 1.8325 \n",
      "[2/10][185/600] Loss_D: 0.4268 Loss_G: 1.7325 \n",
      "[2/10][186/600] Loss_D: 0.4388 Loss_G: 1.7572 \n",
      "[2/10][187/600] Loss_D: 0.4176 Loss_G: 1.8047 \n",
      "[2/10][188/600] Loss_D: 0.3869 Loss_G: 1.8209 \n",
      "[2/10][189/600] Loss_D: 0.4115 Loss_G: 1.7726 \n",
      "[2/10][190/600] Loss_D: 0.3946 Loss_G: 1.8554 \n",
      "[2/10][191/600] Loss_D: 0.4417 Loss_G: 1.7385 \n",
      "[2/10][192/600] Loss_D: 0.4097 Loss_G: 1.8192 \n",
      "[2/10][193/600] Loss_D: 0.4157 Loss_G: 1.8022 \n",
      "[2/10][194/600] Loss_D: 0.4213 Loss_G: 1.8057 \n",
      "[2/10][195/600] Loss_D: 0.3762 Loss_G: 1.8259 \n",
      "[2/10][196/600] Loss_D: 0.3760 Loss_G: 1.8358 \n",
      "[2/10][197/600] Loss_D: 0.3726 Loss_G: 1.8953 \n",
      "[2/10][198/600] Loss_D: 0.4343 Loss_G: 1.7650 \n",
      "[2/10][199/600] Loss_D: 0.3920 Loss_G: 1.8420 \n",
      "[2/10][200/600] Loss_D: 0.3939 Loss_G: 1.8199 \n",
      "[2/10][201/600] Loss_D: 0.3520 Loss_G: 1.9394 \n",
      "[2/10][202/600] Loss_D: 0.3661 Loss_G: 1.9306 \n",
      "[2/10][203/600] Loss_D: 0.3496 Loss_G: 1.9337 \n",
      "[2/10][204/600] Loss_D: 0.3908 Loss_G: 1.9689 \n",
      "[2/10][205/600] Loss_D: 0.3846 Loss_G: 1.9639 \n",
      "[2/10][206/600] Loss_D: 0.3440 Loss_G: 1.9480 \n",
      "[2/10][207/600] Loss_D: 0.3575 Loss_G: 1.9103 \n",
      "[2/10][208/600] Loss_D: 0.3784 Loss_G: 1.9322 \n",
      "[2/10][209/600] Loss_D: 0.3373 Loss_G: 1.9626 \n",
      "[2/10][210/600] Loss_D: 0.3398 Loss_G: 1.9902 \n",
      "[2/10][211/600] Loss_D: 0.3119 Loss_G: 2.0165 \n",
      "[2/10][212/600] Loss_D: 0.3490 Loss_G: 2.0227 \n",
      "[2/10][213/600] Loss_D: 0.3379 Loss_G: 2.0300 \n",
      "[2/10][214/600] Loss_D: 0.3284 Loss_G: 2.0276 \n",
      "[2/10][215/600] Loss_D: 0.3736 Loss_G: 1.9034 \n",
      "[2/10][216/600] Loss_D: 0.3496 Loss_G: 1.8905 \n",
      "[2/10][217/600] Loss_D: 0.4119 Loss_G: 1.9560 \n",
      "[2/10][218/600] Loss_D: 0.3392 Loss_G: 1.9328 \n",
      "[2/10][219/600] Loss_D: 0.3631 Loss_G: 1.9463 \n",
      "[2/10][220/600] Loss_D: 0.3645 Loss_G: 1.9780 \n",
      "[2/10][221/600] Loss_D: 0.3090 Loss_G: 2.0439 \n",
      "[2/10][222/600] Loss_D: 0.3545 Loss_G: 1.9836 \n",
      "[2/10][223/600] Loss_D: 0.3593 Loss_G: 1.9461 \n",
      "[2/10][224/600] Loss_D: 0.3473 Loss_G: 1.9129 \n",
      "[2/10][225/600] Loss_D: 0.3465 Loss_G: 1.9637 \n",
      "[2/10][226/600] Loss_D: 0.3674 Loss_G: 1.9885 \n",
      "[2/10][227/600] Loss_D: 0.3716 Loss_G: 1.9030 \n",
      "[2/10][228/600] Loss_D: 0.3890 Loss_G: 1.9044 \n",
      "[2/10][229/600] Loss_D: 0.4049 Loss_G: 1.8371 \n",
      "[2/10][230/600] Loss_D: 0.3792 Loss_G: 1.8619 \n",
      "[2/10][231/600] Loss_D: 0.3498 Loss_G: 1.9652 \n",
      "[2/10][232/600] Loss_D: 0.3991 Loss_G: 1.8938 \n",
      "[2/10][233/600] Loss_D: 0.4195 Loss_G: 1.8432 \n",
      "[2/10][234/600] Loss_D: 0.4408 Loss_G: 1.8142 \n",
      "[2/10][235/600] Loss_D: 0.3768 Loss_G: 1.8735 \n",
      "[2/10][236/600] Loss_D: 0.4240 Loss_G: 1.8227 \n",
      "[2/10][237/600] Loss_D: 0.4201 Loss_G: 1.8097 \n",
      "[2/10][238/600] Loss_D: 0.4229 Loss_G: 1.8110 \n",
      "[2/10][239/600] Loss_D: 0.4282 Loss_G: 1.8094 \n",
      "[2/10][240/600] Loss_D: 0.4153 Loss_G: 1.8295 \n",
      "[2/10][241/600] Loss_D: 0.3993 Loss_G: 1.7974 \n",
      "[2/10][242/600] Loss_D: 0.3677 Loss_G: 1.9189 \n",
      "[2/10][243/600] Loss_D: 0.4137 Loss_G: 1.8685 \n",
      "[2/10][244/600] Loss_D: 0.3931 Loss_G: 1.8427 \n",
      "[2/10][245/600] Loss_D: 0.4030 Loss_G: 1.9180 \n",
      "[2/10][246/600] Loss_D: 0.4320 Loss_G: 1.8087 \n",
      "[2/10][247/600] Loss_D: 0.3940 Loss_G: 1.7884 \n",
      "[2/10][248/600] Loss_D: 0.3928 Loss_G: 1.9193 \n",
      "[2/10][249/600] Loss_D: 0.3643 Loss_G: 1.9196 \n",
      "[2/10][250/600] Loss_D: 0.4085 Loss_G: 1.8904 \n",
      "[2/10][251/600] Loss_D: 0.3853 Loss_G: 1.8418 \n",
      "[2/10][252/600] Loss_D: 0.3945 Loss_G: 1.8936 \n",
      "[2/10][253/600] Loss_D: 0.3874 Loss_G: 1.8472 \n",
      "[2/10][254/600] Loss_D: 0.3967 Loss_G: 1.8220 \n",
      "[2/10][255/600] Loss_D: 0.3704 Loss_G: 1.9107 \n",
      "[2/10][256/600] Loss_D: 0.3938 Loss_G: 1.9038 \n",
      "[2/10][257/600] Loss_D: 0.3978 Loss_G: 1.8994 \n",
      "[2/10][258/600] Loss_D: 0.4438 Loss_G: 1.8543 \n",
      "[2/10][259/600] Loss_D: 0.3811 Loss_G: 1.8304 \n",
      "[2/10][260/600] Loss_D: 0.4073 Loss_G: 1.8665 \n",
      "[2/10][261/600] Loss_D: 0.4016 Loss_G: 1.8285 \n",
      "[2/10][262/600] Loss_D: 0.3997 Loss_G: 1.8433 \n",
      "[2/10][263/600] Loss_D: 0.4358 Loss_G: 1.8569 \n",
      "[2/10][264/600] Loss_D: 0.3931 Loss_G: 1.9578 \n",
      "[2/10][265/600] Loss_D: 0.4207 Loss_G: 1.8357 \n",
      "[2/10][266/600] Loss_D: 0.4080 Loss_G: 1.8536 \n",
      "[2/10][267/600] Loss_D: 0.3959 Loss_G: 1.8747 \n",
      "[2/10][268/600] Loss_D: 0.3889 Loss_G: 1.8675 \n",
      "[2/10][269/600] Loss_D: 0.4054 Loss_G: 1.9037 \n",
      "[2/10][270/600] Loss_D: 0.3674 Loss_G: 1.9613 \n",
      "[2/10][271/600] Loss_D: 0.3859 Loss_G: 1.8678 \n",
      "[2/10][272/600] Loss_D: 0.3770 Loss_G: 1.8784 \n",
      "[2/10][273/600] Loss_D: 0.4149 Loss_G: 1.8406 \n",
      "[2/10][274/600] Loss_D: 0.4041 Loss_G: 1.9022 \n",
      "[2/10][275/600] Loss_D: 0.3780 Loss_G: 1.8922 \n",
      "[2/10][276/600] Loss_D: 0.3979 Loss_G: 1.8732 \n",
      "[2/10][277/600] Loss_D: 0.4000 Loss_G: 1.9543 \n",
      "[2/10][278/600] Loss_D: 0.3862 Loss_G: 1.9158 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2/10][279/600] Loss_D: 0.3983 Loss_G: 1.9038 \n",
      "[2/10][280/600] Loss_D: 0.3864 Loss_G: 1.9083 \n",
      "[2/10][281/600] Loss_D: 0.3496 Loss_G: 2.0053 \n",
      "[2/10][282/600] Loss_D: 0.3449 Loss_G: 2.0165 \n",
      "[2/10][283/600] Loss_D: 0.3681 Loss_G: 2.0126 \n",
      "[2/10][284/600] Loss_D: 0.3316 Loss_G: 1.9766 \n",
      "[2/10][285/600] Loss_D: 0.3709 Loss_G: 1.9968 \n",
      "[2/10][286/600] Loss_D: 0.3422 Loss_G: 2.0144 \n",
      "[2/10][287/600] Loss_D: 0.3645 Loss_G: 2.0379 \n",
      "[2/10][288/600] Loss_D: 0.3632 Loss_G: 1.9705 \n",
      "[2/10][289/600] Loss_D: 0.3247 Loss_G: 1.9989 \n",
      "[2/10][290/600] Loss_D: 0.3113 Loss_G: 2.0004 \n",
      "[2/10][291/600] Loss_D: 0.3223 Loss_G: 2.0697 \n",
      "[2/10][292/600] Loss_D: 0.3449 Loss_G: 2.0218 \n",
      "[2/10][293/600] Loss_D: 0.3375 Loss_G: 2.0330 \n",
      "[2/10][294/600] Loss_D: 0.3486 Loss_G: 2.0514 \n",
      "[2/10][295/600] Loss_D: 0.3402 Loss_G: 1.9794 \n",
      "[2/10][296/600] Loss_D: 0.3421 Loss_G: 2.0028 \n",
      "[2/10][297/600] Loss_D: 0.3755 Loss_G: 1.9752 \n",
      "[2/10][298/600] Loss_D: 0.3454 Loss_G: 2.0342 \n",
      "[2/10][299/600] Loss_D: 0.3224 Loss_G: 2.0056 \n",
      "[2/10][300/600] Loss_D: 0.3484 Loss_G: 2.0429 \n",
      "[2/10][301/600] Loss_D: 0.3437 Loss_G: 1.9947 \n",
      "[2/10][302/600] Loss_D: 0.3550 Loss_G: 1.9192 \n",
      "[2/10][303/600] Loss_D: 0.3265 Loss_G: 1.9366 \n",
      "[2/10][304/600] Loss_D: 0.3501 Loss_G: 2.0412 \n",
      "[2/10][305/600] Loss_D: 0.3380 Loss_G: 1.9779 \n",
      "[2/10][306/600] Loss_D: 0.3204 Loss_G: 2.0445 \n",
      "[2/10][307/600] Loss_D: 0.3179 Loss_G: 1.9712 \n",
      "[2/10][308/600] Loss_D: 0.3346 Loss_G: 1.9484 \n",
      "[2/10][309/600] Loss_D: 0.3141 Loss_G: 2.0647 \n",
      "[2/10][310/600] Loss_D: 0.3421 Loss_G: 1.9580 \n",
      "[2/10][311/600] Loss_D: 0.3536 Loss_G: 1.9737 \n",
      "[2/10][312/600] Loss_D: 0.3883 Loss_G: 1.8654 \n",
      "[2/10][313/600] Loss_D: 0.3311 Loss_G: 1.9298 \n",
      "[2/10][314/600] Loss_D: 0.3370 Loss_G: 1.9733 \n",
      "[2/10][315/600] Loss_D: 0.3886 Loss_G: 1.9094 \n",
      "[2/10][316/600] Loss_D: 0.3524 Loss_G: 1.9222 \n",
      "[2/10][317/600] Loss_D: 0.3547 Loss_G: 1.8834 \n",
      "[2/10][318/600] Loss_D: 0.3578 Loss_G: 1.9067 \n",
      "[2/10][319/600] Loss_D: 0.3553 Loss_G: 1.9264 \n",
      "[2/10][320/600] Loss_D: 0.3440 Loss_G: 1.9393 \n",
      "[2/10][321/600] Loss_D: 0.3302 Loss_G: 1.9656 \n",
      "[2/10][322/600] Loss_D: 0.3193 Loss_G: 2.0067 \n",
      "[2/10][323/600] Loss_D: 0.4111 Loss_G: 1.8561 \n",
      "[2/10][324/600] Loss_D: 0.3894 Loss_G: 1.8439 \n",
      "[2/10][325/600] Loss_D: 0.3424 Loss_G: 1.9008 \n",
      "[2/10][326/600] Loss_D: 0.3741 Loss_G: 1.9109 \n",
      "[2/10][327/600] Loss_D: 0.3911 Loss_G: 1.8621 \n",
      "[2/10][328/600] Loss_D: 0.3713 Loss_G: 1.8771 \n",
      "[2/10][329/600] Loss_D: 0.3372 Loss_G: 1.9428 \n",
      "[2/10][330/600] Loss_D: 0.3429 Loss_G: 1.9379 \n",
      "[2/10][331/600] Loss_D: 0.3775 Loss_G: 1.8893 \n",
      "[2/10][332/600] Loss_D: 0.3991 Loss_G: 1.9191 \n",
      "[2/10][333/600] Loss_D: 0.3860 Loss_G: 1.9360 \n",
      "[2/10][334/600] Loss_D: 0.3577 Loss_G: 2.0051 \n",
      "[2/10][335/600] Loss_D: 0.3622 Loss_G: 1.8828 \n",
      "[2/10][336/600] Loss_D: 0.3489 Loss_G: 1.9408 \n",
      "[2/10][337/600] Loss_D: 0.3873 Loss_G: 1.9843 \n",
      "[2/10][338/600] Loss_D: 0.3348 Loss_G: 2.0199 \n",
      "[2/10][339/600] Loss_D: 0.3414 Loss_G: 1.9404 \n",
      "[2/10][340/600] Loss_D: 0.3563 Loss_G: 2.0268 \n",
      "[2/10][341/600] Loss_D: 0.3881 Loss_G: 1.9097 \n",
      "[2/10][342/600] Loss_D: 0.3495 Loss_G: 1.9638 \n",
      "[2/10][343/600] Loss_D: 0.3901 Loss_G: 1.9246 \n",
      "[2/10][344/600] Loss_D: 0.3540 Loss_G: 1.9591 \n",
      "[2/10][345/600] Loss_D: 0.3582 Loss_G: 2.0070 \n",
      "[2/10][346/600] Loss_D: 0.3413 Loss_G: 2.0541 \n",
      "[2/10][347/600] Loss_D: 0.3422 Loss_G: 2.0035 \n",
      "[2/10][348/600] Loss_D: 0.3501 Loss_G: 1.9796 \n",
      "[2/10][349/600] Loss_D: 0.3513 Loss_G: 1.9628 \n",
      "[2/10][350/600] Loss_D: 0.3147 Loss_G: 2.0597 \n",
      "[2/10][351/600] Loss_D: 0.3375 Loss_G: 2.0178 \n",
      "[2/10][352/600] Loss_D: 0.3267 Loss_G: 2.0977 \n",
      "[2/10][353/600] Loss_D: 0.3200 Loss_G: 2.0646 \n",
      "[2/10][354/600] Loss_D: 0.3326 Loss_G: 1.9937 \n",
      "[2/10][355/600] Loss_D: 0.2908 Loss_G: 2.1358 \n",
      "[2/10][356/600] Loss_D: 0.3555 Loss_G: 2.0784 \n",
      "[2/10][357/600] Loss_D: 0.3429 Loss_G: 2.0957 \n",
      "[2/10][358/600] Loss_D: 0.3220 Loss_G: 2.0338 \n",
      "[2/10][359/600] Loss_D: 0.3290 Loss_G: 2.0708 \n",
      "[2/10][360/600] Loss_D: 0.3210 Loss_G: 2.1214 \n",
      "[2/10][361/600] Loss_D: 0.3152 Loss_G: 2.0016 \n",
      "[2/10][362/600] Loss_D: 0.3338 Loss_G: 2.0235 \n",
      "[2/10][363/600] Loss_D: 0.3307 Loss_G: 2.0966 \n",
      "[2/10][364/600] Loss_D: 0.3389 Loss_G: 2.0077 \n",
      "[2/10][365/600] Loss_D: 0.3267 Loss_G: 2.0578 \n",
      "[2/10][366/600] Loss_D: 0.3971 Loss_G: 2.0123 \n",
      "[2/10][367/600] Loss_D: 0.3464 Loss_G: 2.0027 \n",
      "[2/10][368/600] Loss_D: 0.3762 Loss_G: 1.9628 \n",
      "[2/10][369/600] Loss_D: 0.3606 Loss_G: 1.9764 \n",
      "[2/10][370/600] Loss_D: 0.3357 Loss_G: 2.0285 \n",
      "[2/10][371/600] Loss_D: 0.3365 Loss_G: 2.0581 \n",
      "[2/10][372/600] Loss_D: 0.3187 Loss_G: 2.0744 \n",
      "[2/10][373/600] Loss_D: 0.3516 Loss_G: 2.0610 \n",
      "[2/10][374/600] Loss_D: 0.3438 Loss_G: 2.0482 \n",
      "[2/10][375/600] Loss_D: 0.3628 Loss_G: 1.9909 \n",
      "[2/10][376/600] Loss_D: 0.3173 Loss_G: 2.0622 \n",
      "[2/10][377/600] Loss_D: 0.3403 Loss_G: 2.0143 \n",
      "[2/10][378/600] Loss_D: 0.3391 Loss_G: 2.0614 \n",
      "[2/10][379/600] Loss_D: 0.3264 Loss_G: 2.0900 \n",
      "[2/10][380/600] Loss_D: 0.3241 Loss_G: 2.0565 \n",
      "[2/10][381/600] Loss_D: 0.3166 Loss_G: 2.1028 \n",
      "[2/10][382/600] Loss_D: 0.3353 Loss_G: 2.0978 \n",
      "[2/10][383/600] Loss_D: 0.3495 Loss_G: 2.0140 \n",
      "[2/10][384/600] Loss_D: 0.3354 Loss_G: 2.0618 \n",
      "[2/10][385/600] Loss_D: 0.3383 Loss_G: 2.0351 \n",
      "[2/10][386/600] Loss_D: 0.3339 Loss_G: 2.0734 \n",
      "[2/10][387/600] Loss_D: 0.3683 Loss_G: 2.0385 \n",
      "[2/10][388/600] Loss_D: 0.3426 Loss_G: 2.0197 \n",
      "[2/10][389/600] Loss_D: 0.3358 Loss_G: 2.1018 \n",
      "[2/10][390/600] Loss_D: 0.4105 Loss_G: 1.9771 \n",
      "[2/10][391/600] Loss_D: 0.3151 Loss_G: 2.0781 \n",
      "[2/10][392/600] Loss_D: 0.3105 Loss_G: 2.0893 \n",
      "[2/10][393/600] Loss_D: 0.3077 Loss_G: 2.1681 \n",
      "[2/10][394/600] Loss_D: 0.3208 Loss_G: 2.1837 \n",
      "[2/10][395/600] Loss_D: 0.3305 Loss_G: 2.0754 \n",
      "[2/10][396/600] Loss_D: 0.3969 Loss_G: 1.9577 \n",
      "[2/10][397/600] Loss_D: 0.3297 Loss_G: 2.0175 \n",
      "[2/10][398/600] Loss_D: 0.3500 Loss_G: 1.9672 \n",
      "[2/10][399/600] Loss_D: 0.3696 Loss_G: 2.0210 \n",
      "[2/10][400/600] Loss_D: 0.3119 Loss_G: 2.0551 \n",
      "[2/10][401/600] Loss_D: 0.3085 Loss_G: 2.1480 \n",
      "[2/10][402/600] Loss_D: 0.3167 Loss_G: 2.0864 \n",
      "[2/10][403/600] Loss_D: 0.3214 Loss_G: 2.0930 \n",
      "[2/10][404/600] Loss_D: 0.3455 Loss_G: 2.0374 \n",
      "[2/10][405/600] Loss_D: 0.3493 Loss_G: 1.9722 \n",
      "[2/10][406/600] Loss_D: 0.3279 Loss_G: 2.0373 \n",
      "[2/10][407/600] Loss_D: 0.3439 Loss_G: 2.0424 \n",
      "[2/10][408/600] Loss_D: 0.3287 Loss_G: 2.0044 \n",
      "[2/10][409/600] Loss_D: 0.2997 Loss_G: 2.1084 \n",
      "[2/10][410/600] Loss_D: 0.3349 Loss_G: 2.0597 \n",
      "[2/10][411/600] Loss_D: 0.3017 Loss_G: 2.1086 \n",
      "[2/10][412/600] Loss_D: 0.3182 Loss_G: 2.0684 \n",
      "[2/10][413/600] Loss_D: 0.3162 Loss_G: 2.0522 \n",
      "[2/10][414/600] Loss_D: 0.3388 Loss_G: 2.0165 \n",
      "[2/10][415/600] Loss_D: 0.3292 Loss_G: 1.9829 \n",
      "[2/10][416/600] Loss_D: 0.3297 Loss_G: 2.1090 \n",
      "[2/10][417/600] Loss_D: 0.3161 Loss_G: 2.0273 \n",
      "[2/10][418/600] Loss_D: 0.3495 Loss_G: 1.9663 \n",
      "[2/10][419/600] Loss_D: 0.3274 Loss_G: 2.0774 \n",
      "[2/10][420/600] Loss_D: 0.3345 Loss_G: 1.9921 \n",
      "[2/10][421/600] Loss_D: 0.3377 Loss_G: 1.9756 \n",
      "[2/10][422/600] Loss_D: 0.3053 Loss_G: 2.0741 \n",
      "[2/10][423/600] Loss_D: 0.3140 Loss_G: 2.0889 \n",
      "[2/10][424/600] Loss_D: 0.3320 Loss_G: 2.0325 \n",
      "[2/10][425/600] Loss_D: 0.3117 Loss_G: 2.1006 \n",
      "[2/10][426/600] Loss_D: 0.3505 Loss_G: 2.0520 \n",
      "[2/10][427/600] Loss_D: 0.3056 Loss_G: 2.1069 \n",
      "[2/10][428/600] Loss_D: 0.3406 Loss_G: 1.9514 \n",
      "[2/10][429/600] Loss_D: 0.3518 Loss_G: 1.9576 \n",
      "[2/10][430/600] Loss_D: 0.3506 Loss_G: 1.9201 \n",
      "[2/10][431/600] Loss_D: 0.2896 Loss_G: 2.0802 \n",
      "[2/10][432/600] Loss_D: 0.3406 Loss_G: 2.0931 \n",
      "[2/10][433/600] Loss_D: 0.3287 Loss_G: 2.1209 \n",
      "[2/10][434/600] Loss_D: 0.2976 Loss_G: 2.0065 \n",
      "[2/10][435/600] Loss_D: 0.3151 Loss_G: 2.0481 \n",
      "[2/10][436/600] Loss_D: 0.3181 Loss_G: 2.1017 \n",
      "[2/10][437/600] Loss_D: 0.3100 Loss_G: 2.0703 \n",
      "[2/10][438/600] Loss_D: 0.3470 Loss_G: 2.0499 \n",
      "[2/10][439/600] Loss_D: 0.3480 Loss_G: 1.9540 \n",
      "[2/10][440/600] Loss_D: 0.3310 Loss_G: 1.9963 \n",
      "[2/10][441/600] Loss_D: 0.3713 Loss_G: 1.9414 \n",
      "[2/10][442/600] Loss_D: 0.3179 Loss_G: 2.0258 \n",
      "[2/10][443/600] Loss_D: 0.3788 Loss_G: 2.0090 \n",
      "[2/10][444/600] Loss_D: 0.3198 Loss_G: 2.0392 \n",
      "[2/10][445/600] Loss_D: 0.3524 Loss_G: 1.9973 \n",
      "[2/10][446/600] Loss_D: 0.3386 Loss_G: 2.0261 \n",
      "[2/10][447/600] Loss_D: 0.3404 Loss_G: 1.9625 \n",
      "[2/10][448/600] Loss_D: 0.3333 Loss_G: 2.0405 \n",
      "[2/10][449/600] Loss_D: 0.3165 Loss_G: 2.0366 \n",
      "[2/10][450/600] Loss_D: 0.3838 Loss_G: 1.9825 \n",
      "[2/10][451/600] Loss_D: 0.3232 Loss_G: 1.9583 \n",
      "[2/10][452/600] Loss_D: 0.3789 Loss_G: 1.9696 \n",
      "[2/10][453/600] Loss_D: 0.3489 Loss_G: 1.9183 \n",
      "[2/10][454/600] Loss_D: 0.3373 Loss_G: 2.0023 \n",
      "[2/10][455/600] Loss_D: 0.3129 Loss_G: 2.0084 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2/10][456/600] Loss_D: 0.3189 Loss_G: 2.1045 \n",
      "[2/10][457/600] Loss_D: 0.3073 Loss_G: 2.1989 \n",
      "[2/10][458/600] Loss_D: 0.3113 Loss_G: 2.0897 \n",
      "[2/10][459/600] Loss_D: 0.3432 Loss_G: 2.0095 \n",
      "[2/10][460/600] Loss_D: 0.3058 Loss_G: 2.0339 \n",
      "[2/10][461/600] Loss_D: 0.2792 Loss_G: 2.1757 \n",
      "[2/10][462/600] Loss_D: 0.2962 Loss_G: 2.1265 \n",
      "[2/10][463/600] Loss_D: 0.3227 Loss_G: 2.1636 \n",
      "[2/10][464/600] Loss_D: 0.2936 Loss_G: 2.1342 \n",
      "[2/10][465/600] Loss_D: 0.3149 Loss_G: 2.0359 \n",
      "[2/10][466/600] Loss_D: 0.3094 Loss_G: 2.0900 \n",
      "[2/10][467/600] Loss_D: 0.3127 Loss_G: 2.1167 \n",
      "[2/10][468/600] Loss_D: 0.2828 Loss_G: 2.1301 \n",
      "[2/10][469/600] Loss_D: 0.3052 Loss_G: 2.1728 \n",
      "[2/10][470/600] Loss_D: 0.2728 Loss_G: 2.2219 \n",
      "[2/10][471/600] Loss_D: 0.3159 Loss_G: 2.0936 \n",
      "[2/10][472/600] Loss_D: 0.2915 Loss_G: 2.2187 \n",
      "[2/10][473/600] Loss_D: 0.2870 Loss_G: 2.1036 \n",
      "[2/10][474/600] Loss_D: 0.2897 Loss_G: 2.1498 \n",
      "[2/10][475/600] Loss_D: 0.2774 Loss_G: 2.1652 \n",
      "[2/10][476/600] Loss_D: 0.2820 Loss_G: 2.1926 \n",
      "[2/10][477/600] Loss_D: 0.3006 Loss_G: 2.2133 \n",
      "[2/10][478/600] Loss_D: 0.2596 Loss_G: 2.3162 \n",
      "[2/10][479/600] Loss_D: 0.3108 Loss_G: 2.1369 \n",
      "[2/10][480/600] Loss_D: 0.3014 Loss_G: 2.0903 \n",
      "[2/10][481/600] Loss_D: 0.2947 Loss_G: 2.1263 \n",
      "[2/10][482/600] Loss_D: 0.2972 Loss_G: 2.1849 \n",
      "[2/10][483/600] Loss_D: 0.2760 Loss_G: 2.1956 \n",
      "[2/10][484/600] Loss_D: 0.2820 Loss_G: 2.1833 \n",
      "[2/10][485/600] Loss_D: 0.2987 Loss_G: 2.1334 \n",
      "[2/10][486/600] Loss_D: 0.2912 Loss_G: 2.1088 \n",
      "[2/10][487/600] Loss_D: 0.2704 Loss_G: 2.1680 \n",
      "[2/10][488/600] Loss_D: 0.2995 Loss_G: 2.1831 \n",
      "[2/10][489/600] Loss_D: 0.3116 Loss_G: 2.0924 \n",
      "[2/10][490/600] Loss_D: 0.2587 Loss_G: 2.1964 \n",
      "[2/10][491/600] Loss_D: 0.2969 Loss_G: 2.1785 \n",
      "[2/10][492/600] Loss_D: 0.3534 Loss_G: 2.0452 \n",
      "[2/10][493/600] Loss_D: 0.3261 Loss_G: 2.0143 \n",
      "[2/10][494/600] Loss_D: 0.3287 Loss_G: 2.0151 \n",
      "[2/10][495/600] Loss_D: 0.3346 Loss_G: 2.0253 \n",
      "[2/10][496/600] Loss_D: 0.2939 Loss_G: 2.1740 \n",
      "[2/10][497/600] Loss_D: 0.3376 Loss_G: 2.1175 \n",
      "[2/10][498/600] Loss_D: 0.2726 Loss_G: 2.1799 \n",
      "[2/10][499/600] Loss_D: 0.3010 Loss_G: 2.1093 \n",
      "[2/10][500/600] Loss_D: 0.3193 Loss_G: 2.0764 \n",
      "[2/10][501/600] Loss_D: 0.3253 Loss_G: 2.0919 \n",
      "[2/10][502/600] Loss_D: 0.2964 Loss_G: 2.1013 \n",
      "[2/10][503/600] Loss_D: 0.3382 Loss_G: 2.0680 \n",
      "[2/10][504/600] Loss_D: 0.3090 Loss_G: 2.1076 \n",
      "[2/10][505/600] Loss_D: 0.2973 Loss_G: 2.1852 \n",
      "[2/10][506/600] Loss_D: 0.3642 Loss_G: 2.0854 \n",
      "[2/10][507/600] Loss_D: 0.3272 Loss_G: 1.9976 \n",
      "[2/10][508/600] Loss_D: 0.3165 Loss_G: 2.0761 \n",
      "[2/10][509/600] Loss_D: 0.2991 Loss_G: 2.2096 \n",
      "[2/10][510/600] Loss_D: 0.2882 Loss_G: 2.2621 \n",
      "[2/10][511/600] Loss_D: 0.3054 Loss_G: 2.1570 \n",
      "[2/10][512/600] Loss_D: 0.3343 Loss_G: 2.1055 \n",
      "[2/10][513/600] Loss_D: 0.3360 Loss_G: 2.1197 \n",
      "[2/10][514/600] Loss_D: 0.3400 Loss_G: 2.0353 \n",
      "[2/10][515/600] Loss_D: 0.3070 Loss_G: 2.1189 \n",
      "[2/10][516/600] Loss_D: 0.2909 Loss_G: 2.2107 \n",
      "[2/10][517/600] Loss_D: 0.3027 Loss_G: 2.1887 \n",
      "[2/10][518/600] Loss_D: 0.2477 Loss_G: 2.3219 \n",
      "[2/10][519/600] Loss_D: 0.2771 Loss_G: 2.3173 \n",
      "[2/10][520/600] Loss_D: 0.2990 Loss_G: 2.2109 \n",
      "[2/10][521/600] Loss_D: 0.2514 Loss_G: 2.2241 \n",
      "[2/10][522/600] Loss_D: 0.2795 Loss_G: 2.2677 \n",
      "[2/10][523/600] Loss_D: 0.2896 Loss_G: 2.2498 \n",
      "[2/10][524/600] Loss_D: 0.3167 Loss_G: 2.2099 \n",
      "[2/10][525/600] Loss_D: 0.3067 Loss_G: 2.0794 \n",
      "[2/10][526/600] Loss_D: 0.2901 Loss_G: 2.1220 \n",
      "[2/10][527/600] Loss_D: 0.2703 Loss_G: 2.3215 \n",
      "[2/10][528/600] Loss_D: 0.2661 Loss_G: 2.3709 \n",
      "[2/10][529/600] Loss_D: 0.2949 Loss_G: 2.2365 \n",
      "[2/10][530/600] Loss_D: 0.2700 Loss_G: 2.1835 \n",
      "[2/10][531/600] Loss_D: 0.3033 Loss_G: 2.1369 \n",
      "[2/10][532/600] Loss_D: 0.2875 Loss_G: 2.1984 \n",
      "[2/10][533/600] Loss_D: 0.3040 Loss_G: 2.1552 \n",
      "[2/10][534/600] Loss_D: 0.2744 Loss_G: 2.2111 \n",
      "[2/10][535/600] Loss_D: 0.3370 Loss_G: 2.1147 \n",
      "[2/10][536/600] Loss_D: 0.2865 Loss_G: 2.1420 \n",
      "[2/10][537/600] Loss_D: 0.2987 Loss_G: 2.1067 \n",
      "[2/10][538/600] Loss_D: 0.3056 Loss_G: 2.0948 \n",
      "[2/10][539/600] Loss_D: 0.3137 Loss_G: 2.0622 \n",
      "[2/10][540/600] Loss_D: 0.3103 Loss_G: 2.1114 \n",
      "[2/10][541/600] Loss_D: 0.2959 Loss_G: 2.1462 \n",
      "[2/10][542/600] Loss_D: 0.3059 Loss_G: 2.0812 \n",
      "[2/10][543/600] Loss_D: 0.3539 Loss_G: 2.0105 \n",
      "[2/10][544/600] Loss_D: 0.3082 Loss_G: 2.0667 \n",
      "[2/10][545/600] Loss_D: 0.3476 Loss_G: 2.0124 \n",
      "[2/10][546/600] Loss_D: 0.2802 Loss_G: 2.1250 \n",
      "[2/10][547/600] Loss_D: 0.2862 Loss_G: 2.1830 \n",
      "[2/10][548/600] Loss_D: 0.2950 Loss_G: 2.1459 \n",
      "[2/10][549/600] Loss_D: 0.3175 Loss_G: 2.1413 \n",
      "[2/10][550/600] Loss_D: 0.3317 Loss_G: 2.0149 \n",
      "[2/10][551/600] Loss_D: 0.3281 Loss_G: 2.0593 \n",
      "[2/10][552/600] Loss_D: 0.2920 Loss_G: 2.1100 \n",
      "[2/10][553/600] Loss_D: 0.3104 Loss_G: 2.1375 \n",
      "[2/10][554/600] Loss_D: 0.2912 Loss_G: 2.1371 \n",
      "[2/10][555/600] Loss_D: 0.2984 Loss_G: 2.1300 \n",
      "[2/10][556/600] Loss_D: 0.3057 Loss_G: 2.1089 \n",
      "[2/10][557/600] Loss_D: 0.2986 Loss_G: 2.1375 \n",
      "[2/10][558/600] Loss_D: 0.2915 Loss_G: 2.1566 \n",
      "[2/10][559/600] Loss_D: 0.3211 Loss_G: 2.1275 \n",
      "[2/10][560/600] Loss_D: 0.2746 Loss_G: 2.1727 \n",
      "[2/10][561/600] Loss_D: 0.2577 Loss_G: 2.2187 \n",
      "[2/10][562/600] Loss_D: 0.2810 Loss_G: 2.1224 \n",
      "[2/10][563/600] Loss_D: 0.3076 Loss_G: 2.1514 \n",
      "[2/10][564/600] Loss_D: 0.2661 Loss_G: 2.2217 \n",
      "[2/10][565/600] Loss_D: 0.2819 Loss_G: 2.1610 \n",
      "[2/10][566/600] Loss_D: 0.2460 Loss_G: 2.2582 \n",
      "[2/10][567/600] Loss_D: 0.2777 Loss_G: 2.2311 \n",
      "[2/10][568/600] Loss_D: 0.2644 Loss_G: 2.2535 \n",
      "[2/10][569/600] Loss_D: 0.2894 Loss_G: 2.1593 \n",
      "[2/10][570/600] Loss_D: 0.2996 Loss_G: 2.2176 \n",
      "[2/10][571/600] Loss_D: 0.2684 Loss_G: 2.1328 \n",
      "[2/10][572/600] Loss_D: 0.2878 Loss_G: 2.1427 \n",
      "[2/10][573/600] Loss_D: 0.2209 Loss_G: 2.3466 \n",
      "[2/10][574/600] Loss_D: 0.2596 Loss_G: 2.3386 \n",
      "[2/10][575/600] Loss_D: 0.2337 Loss_G: 2.4156 \n",
      "[2/10][576/600] Loss_D: 0.2926 Loss_G: 2.1655 \n",
      "[2/10][577/600] Loss_D: 0.2494 Loss_G: 2.2708 \n",
      "[2/10][578/600] Loss_D: 0.2519 Loss_G: 2.2144 \n",
      "[2/10][579/600] Loss_D: 0.2516 Loss_G: 2.2526 \n",
      "[2/10][580/600] Loss_D: 0.2434 Loss_G: 2.3131 \n",
      "[2/10][581/600] Loss_D: 0.2443 Loss_G: 2.3573 \n",
      "[2/10][582/600] Loss_D: 0.2685 Loss_G: 2.2376 \n",
      "[2/10][583/600] Loss_D: 0.2583 Loss_G: 2.2153 \n",
      "[2/10][584/600] Loss_D: 0.2629 Loss_G: 2.2761 \n",
      "[2/10][585/600] Loss_D: 0.2263 Loss_G: 2.3673 \n",
      "[2/10][586/600] Loss_D: 0.2447 Loss_G: 2.2809 \n",
      "[2/10][587/600] Loss_D: 0.2707 Loss_G: 2.2510 \n",
      "[2/10][588/600] Loss_D: 0.2457 Loss_G: 2.3129 \n",
      "[2/10][589/600] Loss_D: 0.2558 Loss_G: 2.2777 \n",
      "[2/10][590/600] Loss_D: 0.2614 Loss_G: 2.2922 \n",
      "[2/10][591/600] Loss_D: 0.2233 Loss_G: 2.3738 \n",
      "[2/10][592/600] Loss_D: 0.2536 Loss_G: 2.3363 \n",
      "[2/10][593/600] Loss_D: 0.2755 Loss_G: 2.2467 \n",
      "[2/10][594/600] Loss_D: 0.2391 Loss_G: 2.2864 \n",
      "[2/10][595/600] Loss_D: 0.2682 Loss_G: 2.2134 \n",
      "[2/10][596/600] Loss_D: 0.2582 Loss_G: 2.2692 \n",
      "[2/10][597/600] Loss_D: 0.2803 Loss_G: 2.1576 \n",
      "[2/10][598/600] Loss_D: 0.2786 Loss_G: 2.2289 \n",
      "[2/10][599/600] Loss_D: 0.2968 Loss_G: 2.1791 \n",
      "[3/10][0/600] Loss_D: 0.2885 Loss_G: 2.1604 \n",
      "[3/10][1/600] Loss_D: 0.2819 Loss_G: 2.1963 \n",
      "[3/10][2/600] Loss_D: 0.2691 Loss_G: 2.2850 \n",
      "[3/10][3/600] Loss_D: 0.2474 Loss_G: 2.2908 \n",
      "[3/10][4/600] Loss_D: 0.2847 Loss_G: 2.2983 \n",
      "[3/10][5/600] Loss_D: 0.2811 Loss_G: 2.1237 \n",
      "[3/10][6/600] Loss_D: 0.2802 Loss_G: 2.2336 \n",
      "[3/10][7/600] Loss_D: 0.2747 Loss_G: 2.3085 \n",
      "[3/10][8/600] Loss_D: 0.3050 Loss_G: 2.1997 \n",
      "[3/10][9/600] Loss_D: 0.2583 Loss_G: 2.2593 \n",
      "[3/10][10/600] Loss_D: 0.3080 Loss_G: 2.2021 \n",
      "[3/10][11/600] Loss_D: 0.3147 Loss_G: 2.1258 \n",
      "[3/10][12/600] Loss_D: 0.2858 Loss_G: 2.2248 \n",
      "[3/10][13/600] Loss_D: 0.3094 Loss_G: 2.1850 \n",
      "[3/10][14/600] Loss_D: 0.2807 Loss_G: 2.2453 \n",
      "[3/10][15/600] Loss_D: 0.2835 Loss_G: 2.2324 \n",
      "[3/10][16/600] Loss_D: 0.3015 Loss_G: 2.1715 \n",
      "[3/10][17/600] Loss_D: 0.2876 Loss_G: 2.1559 \n",
      "[3/10][18/600] Loss_D: 0.3218 Loss_G: 2.1557 \n",
      "[3/10][19/600] Loss_D: 0.2489 Loss_G: 2.3091 \n",
      "[3/10][20/600] Loss_D: 0.2650 Loss_G: 2.2936 \n",
      "[3/10][21/600] Loss_D: 0.2575 Loss_G: 2.4044 \n",
      "[3/10][22/600] Loss_D: 0.2399 Loss_G: 2.3986 \n",
      "[3/10][23/600] Loss_D: 0.2349 Loss_G: 2.3988 \n",
      "[3/10][24/600] Loss_D: 0.2417 Loss_G: 2.3146 \n",
      "[3/10][25/600] Loss_D: 0.2488 Loss_G: 2.4238 \n",
      "[3/10][26/600] Loss_D: 0.2608 Loss_G: 2.3433 \n",
      "[3/10][27/600] Loss_D: 0.2599 Loss_G: 2.3568 \n",
      "[3/10][28/600] Loss_D: 0.2311 Loss_G: 2.4055 \n",
      "[3/10][29/600] Loss_D: 0.2359 Loss_G: 2.4601 \n",
      "[3/10][30/600] Loss_D: 0.2417 Loss_G: 2.4399 \n",
      "[3/10][31/600] Loss_D: 0.2255 Loss_G: 2.5033 \n",
      "[3/10][32/600] Loss_D: 0.2412 Loss_G: 2.3684 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3/10][33/600] Loss_D: 0.2243 Loss_G: 2.5246 \n",
      "[3/10][34/600] Loss_D: 0.2256 Loss_G: 2.5248 \n",
      "[3/10][35/600] Loss_D: 0.2589 Loss_G: 2.3567 \n",
      "[3/10][36/600] Loss_D: 0.2175 Loss_G: 2.4599 \n",
      "[3/10][37/600] Loss_D: 0.2096 Loss_G: 2.5114 \n",
      "[3/10][38/600] Loss_D: 0.2422 Loss_G: 2.4380 \n",
      "[3/10][39/600] Loss_D: 0.2359 Loss_G: 2.4587 \n",
      "[3/10][40/600] Loss_D: 0.2078 Loss_G: 2.4850 \n",
      "[3/10][41/600] Loss_D: 0.2272 Loss_G: 2.4729 \n",
      "[3/10][42/600] Loss_D: 0.2516 Loss_G: 2.3602 \n",
      "[3/10][43/600] Loss_D: 0.2542 Loss_G: 2.2623 \n",
      "[3/10][44/600] Loss_D: 0.2474 Loss_G: 2.2892 \n",
      "[3/10][45/600] Loss_D: 0.2224 Loss_G: 2.4604 \n",
      "[3/10][46/600] Loss_D: 0.2256 Loss_G: 2.4626 \n",
      "[3/10][47/600] Loss_D: 0.2324 Loss_G: 2.4486 \n",
      "[3/10][48/600] Loss_D: 0.2505 Loss_G: 2.3134 \n",
      "[3/10][49/600] Loss_D: 0.2343 Loss_G: 2.3498 \n",
      "[3/10][50/600] Loss_D: 0.2434 Loss_G: 2.3075 \n",
      "[3/10][51/600] Loss_D: 0.2452 Loss_G: 2.3336 \n",
      "[3/10][52/600] Loss_D: 0.2385 Loss_G: 2.3888 \n",
      "[3/10][53/600] Loss_D: 0.2505 Loss_G: 2.2828 \n",
      "[3/10][54/600] Loss_D: 0.2711 Loss_G: 2.2471 \n",
      "[3/10][55/600] Loss_D: 0.2256 Loss_G: 2.3833 \n",
      "[3/10][56/600] Loss_D: 0.2783 Loss_G: 2.2394 \n",
      "[3/10][57/600] Loss_D: 0.2560 Loss_G: 2.3362 \n",
      "[3/10][58/600] Loss_D: 0.2713 Loss_G: 2.2445 \n",
      "[3/10][59/600] Loss_D: 0.3065 Loss_G: 2.0260 \n",
      "[3/10][60/600] Loss_D: 0.2618 Loss_G: 2.2118 \n",
      "[3/10][61/600] Loss_D: 0.2591 Loss_G: 2.3060 \n",
      "[3/10][62/600] Loss_D: 0.2561 Loss_G: 2.3151 \n",
      "[3/10][63/600] Loss_D: 0.2485 Loss_G: 2.2861 \n",
      "[3/10][64/600] Loss_D: 0.2362 Loss_G: 2.3594 \n",
      "[3/10][65/600] Loss_D: 0.2536 Loss_G: 2.2842 \n",
      "[3/10][66/600] Loss_D: 0.2139 Loss_G: 2.3839 \n",
      "[3/10][67/600] Loss_D: 0.2451 Loss_G: 2.3378 \n",
      "[3/10][68/600] Loss_D: 0.2330 Loss_G: 2.3536 \n",
      "[3/10][69/600] Loss_D: 0.2673 Loss_G: 2.2345 \n",
      "[3/10][70/600] Loss_D: 0.2675 Loss_G: 2.2131 \n",
      "[3/10][71/600] Loss_D: 0.2309 Loss_G: 2.3328 \n",
      "[3/10][72/600] Loss_D: 0.2889 Loss_G: 2.2311 \n",
      "[3/10][73/600] Loss_D: 0.2323 Loss_G: 2.2838 \n",
      "[3/10][74/600] Loss_D: 0.2140 Loss_G: 2.4000 \n",
      "[3/10][75/600] Loss_D: 0.2557 Loss_G: 2.3750 \n",
      "[3/10][76/600] Loss_D: 0.2283 Loss_G: 2.4403 \n",
      "[3/10][77/600] Loss_D: 0.2212 Loss_G: 2.4658 \n",
      "[3/10][78/600] Loss_D: 0.2241 Loss_G: 2.4105 \n",
      "[3/10][79/600] Loss_D: 0.2230 Loss_G: 2.3979 \n",
      "[3/10][80/600] Loss_D: 0.2089 Loss_G: 2.4535 \n",
      "[3/10][81/600] Loss_D: 0.2398 Loss_G: 2.4011 \n",
      "[3/10][82/600] Loss_D: 0.2251 Loss_G: 2.4114 \n",
      "[3/10][83/600] Loss_D: 0.1895 Loss_G: 2.4935 \n",
      "[3/10][84/600] Loss_D: 0.1922 Loss_G: 2.5542 \n",
      "[3/10][85/600] Loss_D: 0.2082 Loss_G: 2.6307 \n",
      "[3/10][86/600] Loss_D: 0.2269 Loss_G: 2.4262 \n",
      "[3/10][87/600] Loss_D: 0.2193 Loss_G: 2.3488 \n",
      "[3/10][88/600] Loss_D: 0.1805 Loss_G: 2.5767 \n",
      "[3/10][89/600] Loss_D: 0.2309 Loss_G: 2.4577 \n",
      "[3/10][90/600] Loss_D: 0.1929 Loss_G: 2.4550 \n",
      "[3/10][91/600] Loss_D: 0.2029 Loss_G: 2.5454 \n",
      "[3/10][92/600] Loss_D: 0.2076 Loss_G: 2.5329 \n",
      "[3/10][93/600] Loss_D: 0.2091 Loss_G: 2.4890 \n",
      "[3/10][94/600] Loss_D: 0.1895 Loss_G: 2.5387 \n",
      "[3/10][95/600] Loss_D: 0.2086 Loss_G: 2.4867 \n",
      "[3/10][96/600] Loss_D: 0.2118 Loss_G: 2.4914 \n",
      "[3/10][97/600] Loss_D: 0.2171 Loss_G: 2.4978 \n",
      "[3/10][98/600] Loss_D: 0.2561 Loss_G: 2.3044 \n",
      "[3/10][99/600] Loss_D: 0.2422 Loss_G: 2.3238 \n",
      "[3/10][100/600] Loss_D: 0.2071 Loss_G: 2.3828 \n",
      "[3/10][101/600] Loss_D: 0.2448 Loss_G: 2.3605 \n",
      "[3/10][102/600] Loss_D: 0.2153 Loss_G: 2.4572 \n",
      "[3/10][103/600] Loss_D: 0.2305 Loss_G: 2.4750 \n",
      "[3/10][104/600] Loss_D: 0.2565 Loss_G: 2.2718 \n",
      "[3/10][105/600] Loss_D: 0.2052 Loss_G: 2.3910 \n",
      "[3/10][106/600] Loss_D: 0.2505 Loss_G: 2.3608 \n",
      "[3/10][107/600] Loss_D: 0.2529 Loss_G: 2.3026 \n",
      "[3/10][108/600] Loss_D: 0.2092 Loss_G: 2.4922 \n",
      "[3/10][109/600] Loss_D: 0.2713 Loss_G: 2.3345 \n",
      "[3/10][110/600] Loss_D: 0.2499 Loss_G: 2.2768 \n",
      "[3/10][111/600] Loss_D: 0.2211 Loss_G: 2.3617 \n",
      "[3/10][112/600] Loss_D: 0.2275 Loss_G: 2.4079 \n",
      "[3/10][113/600] Loss_D: 0.2286 Loss_G: 2.4371 \n",
      "[3/10][114/600] Loss_D: 0.2331 Loss_G: 2.3653 \n",
      "[3/10][115/600] Loss_D: 0.2503 Loss_G: 2.3557 \n",
      "[3/10][116/600] Loss_D: 0.2404 Loss_G: 2.3869 \n",
      "[3/10][117/600] Loss_D: 0.2507 Loss_G: 2.2829 \n",
      "[3/10][118/600] Loss_D: 0.2251 Loss_G: 2.4118 \n",
      "[3/10][119/600] Loss_D: 0.2173 Loss_G: 2.4279 \n",
      "[3/10][120/600] Loss_D: 0.2266 Loss_G: 2.4712 \n",
      "[3/10][121/600] Loss_D: 0.2178 Loss_G: 2.3728 \n",
      "[3/10][122/600] Loss_D: 0.2325 Loss_G: 2.3397 \n",
      "[3/10][123/600] Loss_D: 0.2374 Loss_G: 2.4363 \n",
      "[3/10][124/600] Loss_D: 0.2347 Loss_G: 2.4931 \n",
      "[3/10][125/600] Loss_D: 0.2705 Loss_G: 2.2982 \n",
      "[3/10][126/600] Loss_D: 0.2661 Loss_G: 2.1386 \n",
      "[3/10][127/600] Loss_D: 0.2269 Loss_G: 2.4163 \n",
      "[3/10][128/600] Loss_D: 0.2126 Loss_G: 2.5447 \n",
      "[3/10][129/600] Loss_D: 0.2064 Loss_G: 2.5618 \n",
      "[3/10][130/600] Loss_D: 0.2737 Loss_G: 2.3229 \n",
      "[3/10][131/600] Loss_D: 0.2310 Loss_G: 2.3143 \n",
      "[3/10][132/600] Loss_D: 0.2550 Loss_G: 2.3527 \n",
      "[3/10][133/600] Loss_D: 0.2242 Loss_G: 2.5028 \n",
      "[3/10][134/600] Loss_D: 0.2110 Loss_G: 2.5115 \n",
      "[3/10][135/600] Loss_D: 0.1999 Loss_G: 2.4891 \n",
      "[3/10][136/600] Loss_D: 0.2083 Loss_G: 2.5321 \n",
      "[3/10][137/600] Loss_D: 0.2247 Loss_G: 2.4350 \n",
      "[3/10][138/600] Loss_D: 0.2111 Loss_G: 2.4616 \n",
      "[3/10][139/600] Loss_D: 0.2187 Loss_G: 2.4093 \n",
      "[3/10][140/600] Loss_D: 0.2159 Loss_G: 2.5114 \n",
      "[3/10][141/600] Loss_D: 0.2737 Loss_G: 2.2749 \n",
      "[3/10][142/600] Loss_D: 0.2048 Loss_G: 2.3782 \n",
      "[3/10][143/600] Loss_D: 0.2370 Loss_G: 2.3862 \n",
      "[3/10][144/600] Loss_D: 0.2057 Loss_G: 2.5139 \n",
      "[3/10][145/600] Loss_D: 0.2104 Loss_G: 2.5243 \n",
      "[3/10][146/600] Loss_D: 0.2127 Loss_G: 2.4674 \n",
      "[3/10][147/600] Loss_D: 0.2286 Loss_G: 2.3807 \n",
      "[3/10][148/600] Loss_D: 0.2142 Loss_G: 2.4231 \n",
      "[3/10][149/600] Loss_D: 0.2188 Loss_G: 2.5166 \n",
      "[3/10][150/600] Loss_D: 0.1991 Loss_G: 2.5601 \n",
      "[3/10][151/600] Loss_D: 0.2117 Loss_G: 2.5349 \n",
      "[3/10][152/600] Loss_D: 0.2021 Loss_G: 2.4924 \n",
      "[3/10][153/600] Loss_D: 0.2146 Loss_G: 2.4030 \n",
      "[3/10][154/600] Loss_D: 0.2331 Loss_G: 2.4450 \n",
      "[3/10][155/600] Loss_D: 0.2077 Loss_G: 2.5613 \n",
      "[3/10][156/600] Loss_D: 0.2175 Loss_G: 2.5152 \n",
      "[3/10][157/600] Loss_D: 0.2212 Loss_G: 2.3852 \n",
      "[3/10][158/600] Loss_D: 0.1985 Loss_G: 2.5080 \n",
      "[3/10][159/600] Loss_D: 0.2360 Loss_G: 2.4376 \n",
      "[3/10][160/600] Loss_D: 0.2290 Loss_G: 2.4049 \n",
      "[3/10][161/600] Loss_D: 0.2175 Loss_G: 2.4047 \n",
      "[3/10][162/600] Loss_D: 0.2087 Loss_G: 2.4932 \n",
      "[3/10][163/600] Loss_D: 0.2169 Loss_G: 2.4864 \n",
      "[3/10][164/600] Loss_D: 0.2236 Loss_G: 2.4488 \n",
      "[3/10][165/600] Loss_D: 0.2579 Loss_G: 2.3006 \n",
      "[3/10][166/600] Loss_D: 0.2050 Loss_G: 2.3873 \n",
      "[3/10][167/600] Loss_D: 0.2043 Loss_G: 2.4573 \n",
      "[3/10][168/600] Loss_D: 0.2159 Loss_G: 2.5381 \n",
      "[3/10][169/600] Loss_D: 0.2075 Loss_G: 2.6299 \n",
      "[3/10][170/600] Loss_D: 0.1966 Loss_G: 2.5660 \n",
      "[3/10][171/600] Loss_D: 0.1905 Loss_G: 2.5465 \n",
      "[3/10][172/600] Loss_D: 0.1853 Loss_G: 2.5870 \n",
      "[3/10][173/600] Loss_D: 0.1846 Loss_G: 2.5356 \n",
      "[3/10][174/600] Loss_D: 0.1760 Loss_G: 2.6330 \n",
      "[3/10][175/600] Loss_D: 0.2127 Loss_G: 2.5284 \n",
      "[3/10][176/600] Loss_D: 0.1620 Loss_G: 2.6085 \n",
      "[3/10][177/600] Loss_D: 0.1773 Loss_G: 2.6100 \n",
      "[3/10][178/600] Loss_D: 0.1596 Loss_G: 2.8061 \n",
      "[3/10][179/600] Loss_D: 0.1970 Loss_G: 2.5168 \n",
      "[3/10][180/600] Loss_D: 0.1780 Loss_G: 2.6251 \n",
      "[3/10][181/600] Loss_D: 0.2073 Loss_G: 2.4797 \n",
      "[3/10][182/600] Loss_D: 0.1593 Loss_G: 2.6439 \n",
      "[3/10][183/600] Loss_D: 0.1726 Loss_G: 2.5992 \n",
      "[3/10][184/600] Loss_D: 0.1775 Loss_G: 2.6709 \n",
      "[3/10][185/600] Loss_D: 0.1989 Loss_G: 2.5341 \n",
      "[3/10][186/600] Loss_D: 0.1872 Loss_G: 2.4759 \n",
      "[3/10][187/600] Loss_D: 0.2059 Loss_G: 2.5024 \n",
      "[3/10][188/600] Loss_D: 0.1997 Loss_G: 2.4791 \n",
      "[3/10][189/600] Loss_D: 0.2198 Loss_G: 2.4697 \n",
      "[3/10][190/600] Loss_D: 0.1870 Loss_G: 2.4992 \n",
      "[3/10][191/600] Loss_D: 0.1903 Loss_G: 2.6065 \n",
      "[3/10][192/600] Loss_D: 0.2095 Loss_G: 2.5028 \n",
      "[3/10][193/600] Loss_D: 0.2003 Loss_G: 2.4275 \n",
      "[3/10][194/600] Loss_D: 0.1897 Loss_G: 2.5432 \n",
      "[3/10][195/600] Loss_D: 0.1694 Loss_G: 2.7537 \n",
      "[3/10][196/600] Loss_D: 0.1863 Loss_G: 2.6377 \n",
      "[3/10][197/600] Loss_D: 0.2028 Loss_G: 2.5254 \n",
      "[3/10][198/600] Loss_D: 0.1812 Loss_G: 2.5216 \n",
      "[3/10][199/600] Loss_D: 0.1865 Loss_G: 2.5975 \n",
      "[3/10][200/600] Loss_D: 0.1816 Loss_G: 2.6276 \n",
      "[3/10][201/600] Loss_D: 0.2081 Loss_G: 2.5772 \n",
      "[3/10][202/600] Loss_D: 0.1838 Loss_G: 2.4833 \n",
      "[3/10][203/600] Loss_D: 0.2100 Loss_G: 2.5400 \n",
      "[3/10][204/600] Loss_D: 0.1945 Loss_G: 2.5649 \n",
      "[3/10][205/600] Loss_D: 0.2324 Loss_G: 2.3998 \n",
      "[3/10][206/600] Loss_D: 0.2299 Loss_G: 2.3816 \n",
      "[3/10][207/600] Loss_D: 0.2350 Loss_G: 2.3290 \n",
      "[3/10][208/600] Loss_D: 0.1874 Loss_G: 2.5338 \n",
      "[3/10][209/600] Loss_D: 0.2000 Loss_G: 2.5550 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3/10][210/600] Loss_D: 0.1969 Loss_G: 2.5393 \n",
      "[3/10][211/600] Loss_D: 0.1766 Loss_G: 2.6019 \n",
      "[3/10][212/600] Loss_D: 0.2101 Loss_G: 2.4679 \n",
      "[3/10][213/600] Loss_D: 0.1920 Loss_G: 2.5545 \n",
      "[3/10][214/600] Loss_D: 0.1823 Loss_G: 2.6100 \n",
      "[3/10][215/600] Loss_D: 0.2058 Loss_G: 2.4653 \n",
      "[3/10][216/600] Loss_D: 0.2087 Loss_G: 2.4960 \n",
      "[3/10][217/600] Loss_D: 0.1758 Loss_G: 2.6369 \n",
      "[3/10][218/600] Loss_D: 0.2001 Loss_G: 2.5861 \n",
      "[3/10][219/600] Loss_D: 0.2099 Loss_G: 2.5633 \n",
      "[3/10][220/600] Loss_D: 0.1854 Loss_G: 2.5657 \n",
      "[3/10][221/600] Loss_D: 0.1953 Loss_G: 2.5055 \n",
      "[3/10][222/600] Loss_D: 0.2181 Loss_G: 2.4188 \n",
      "[3/10][223/600] Loss_D: 0.1794 Loss_G: 2.5588 \n",
      "[3/10][224/600] Loss_D: 0.2462 Loss_G: 2.3636 \n",
      "[3/10][225/600] Loss_D: 0.1794 Loss_G: 2.5533 \n",
      "[3/10][226/600] Loss_D: 0.2209 Loss_G: 2.4189 \n",
      "[3/10][227/600] Loss_D: 0.2185 Loss_G: 2.5492 \n",
      "[3/10][228/600] Loss_D: 0.1724 Loss_G: 2.6864 \n",
      "[3/10][229/600] Loss_D: 0.2178 Loss_G: 2.4877 \n",
      "[3/10][230/600] Loss_D: 0.1989 Loss_G: 2.4418 \n",
      "[3/10][231/600] Loss_D: 0.1841 Loss_G: 2.5562 \n",
      "[3/10][232/600] Loss_D: 0.1846 Loss_G: 2.6532 \n",
      "[3/10][233/600] Loss_D: 0.1581 Loss_G: 2.8408 \n",
      "[3/10][234/600] Loss_D: 0.1747 Loss_G: 2.6823 \n",
      "[3/10][235/600] Loss_D: 0.1834 Loss_G: 2.5494 \n",
      "[3/10][236/600] Loss_D: 0.1696 Loss_G: 2.6837 \n",
      "[3/10][237/600] Loss_D: 0.1828 Loss_G: 2.6116 \n",
      "[3/10][238/600] Loss_D: 0.1658 Loss_G: 2.6562 \n",
      "[3/10][239/600] Loss_D: 0.1639 Loss_G: 2.7294 \n",
      "[3/10][240/600] Loss_D: 0.2047 Loss_G: 2.5834 \n",
      "[3/10][241/600] Loss_D: 0.1675 Loss_G: 2.6677 \n",
      "[3/10][242/600] Loss_D: 0.1558 Loss_G: 2.7027 \n",
      "[3/10][243/600] Loss_D: 0.1961 Loss_G: 2.5783 \n",
      "[3/10][244/600] Loss_D: 0.1938 Loss_G: 2.4792 \n",
      "[3/10][245/600] Loss_D: 0.1950 Loss_G: 2.5568 \n",
      "[3/10][246/600] Loss_D: 0.1469 Loss_G: 2.7361 \n",
      "[3/10][247/600] Loss_D: 0.1709 Loss_G: 2.7281 \n",
      "[3/10][248/600] Loss_D: 0.1740 Loss_G: 2.7884 \n",
      "[3/10][249/600] Loss_D: 0.1599 Loss_G: 2.7682 \n",
      "[3/10][250/600] Loss_D: 0.1846 Loss_G: 2.5607 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/PIL/ImageFile.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(im, fp, tile, bufsize)\u001b[0m\n\u001b[1;32m    463\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 464\u001b[0;31m         \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileno\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    465\u001b[0m         \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: '_idat' object has no attribute 'fileno'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-1602dc2de455>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m             vutils.save_image(fake.data,\n\u001b[1;32m     45\u001b[0m                         \u001b[0;34m'%s/fake_samples_epoch_%03d.png'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0moutf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m                         normalize=True)\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'%s/netG.pth'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0moutf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torchvision-0.1.9-py3.6.egg/torchvision/utils.py\u001b[0m in \u001b[0;36msave_image\u001b[0;34m(tensor, filename, nrow, padding, normalize, range, scale_each, pad_value)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0mndarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbyte\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m     \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, fp, format, **params)\u001b[0m\n\u001b[1;32m   1891\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1892\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1893\u001b[0;31m             \u001b[0msave_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1894\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1895\u001b[0m             \u001b[0;31m# do what we can to clean up\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/PIL/PngImagePlugin.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(im, fp, filename, chunk, check)\u001b[0m\n\u001b[1;32m    794\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    795\u001b[0m     ImageFile._save(im, _idat(fp, chunk),\n\u001b[0;32m--> 796\u001b[0;31m                     [(\"zip\", (0, 0)+im.size, 0, rawmode)])\n\u001b[0m\u001b[1;32m    797\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    798\u001b[0m     \u001b[0mchunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mb\"IEND\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mb\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/PIL/ImageFile.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(im, fp, tile, bufsize)\u001b[0m\n\u001b[1;32m    476\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 478\u001b[0;31m                     \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbufsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    479\u001b[0m                     \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    480\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "########### Training   ###########\n",
    "for epoch in range(1,niter+1):\n",
    "    for i, (images,_) in enumerate(data_loader):\n",
    "        ########### fDx ###########\n",
    "        netD.zero_grad()\n",
    "        # train with real data, resize real because last batch may has less than\n",
    "        # opt.batchSize images\n",
    "        real.data.resize_(images.size()).copy_(images)\n",
    "        label.data.resize_(images.size(0)).fill_(real_label)\n",
    "\n",
    "        output = netD(real)\n",
    "        errD_real = criterion(output, label)\n",
    "        errD_real.backward()\n",
    "\n",
    "        # train with fake data\n",
    "        label.data.fill_(fake_label)\n",
    "        noise.data.resize_(images.size(0), nz, 1, 1)\n",
    "        noise.data.normal_(0,1)\n",
    "\n",
    "        fake = netG(noise)\n",
    "        # detach gradients here so that gradients of G won't be updated\n",
    "        output = netD(fake.detach())\n",
    "        errD_fake = criterion(output,label)\n",
    "        errD_fake.backward()\n",
    "\n",
    "        errD = errD_fake + errD_real\n",
    "        optimizerD.step()\n",
    "\n",
    "        ########### fGx ###########\n",
    "        netG.zero_grad()\n",
    "        label.data.fill_(real_label)\n",
    "        output = netD(fake)\n",
    "        errG = criterion(output, label)\n",
    "        errG.backward()\n",
    "        optimizerG.step()\n",
    "\n",
    "        ########### Logging #########\n",
    "        print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f '\n",
    "                  % (epoch, niter, i, len(data_loader),\n",
    "                     errD.data[0], errG.data[0]))\n",
    "\n",
    "        ########## Visualize #########\n",
    "        if(i % 1 == 0):\n",
    "            vutils.save_image(fake.data,\n",
    "                        '%s/fake_samples_epoch_%03d.png' % (outf, epoch),\n",
    "                        normalize=True)\n",
    "\n",
    "torch.save(netG.state_dict(), '%s/netG.pth' % (outf))\n",
    "torch.save(netD.state_dict(), '%s/netD.pth' % (outf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "z = to_var(torch.randn(batch_size, 1,21,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=fake[22].cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 32, 32)"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 255.,  255.,  255., ...,  255.,  255.,  255.],\n",
       "        [ 255.,  255.,  255., ...,  255.,  255.,  255.],\n",
       "        [ 255.,  255.,  255., ...,  255.,  255.,  255.],\n",
       "        ..., \n",
       "        [ 255.,  255.,  255., ...,  255.,  255.,  255.],\n",
       "        [ 255.,  255.,  255., ...,  255.,  255.,  255.],\n",
       "        [ 255.,  255.,  255., ...,  255.,  255.,  255.]]], dtype=float32)"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "z = to_var(torch.randn(batch_size, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test=nn.ConvTranspose2d(     100, ngf*4 ,kernel_size=2, stride=1,padding=0, bias=False).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "G =nn.Sequential(\n",
    "            # input is (nc) x 64 x 64\n",
    "\n",
    "            nn.Sigmoid())\n",
    "G=G.cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "        z = Variable(torch.randn(1, 1,1,1)).cuda()\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fake_images = G(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m = nn.Tanh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c=m(D(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "noise = torch.FloatTensor(batch_size, 100, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m = nn.ConvTranspose2d(16, 1, 3, stride=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input = Variable(torch.randn(22, 16, 33, 33))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output = m(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m = nn.MaxPool1d(5, stride=2)\n",
    "input = Variable(torch.randn(20, 16, 50))\n",
    "output = m(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
