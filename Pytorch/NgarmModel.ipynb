{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CONTEXT_SIZE = 2\n",
    "EMBEDDING_DIM = 10\n",
    "# We will use Shakespeare Sonnet 2\n",
    "test_sentence = \"\"\"When forty winters shall besiege thy brow,\n",
    "And dig deep trenches in thy beauty's field,\n",
    "Thy youth's proud livery so gazed on now,\n",
    "Will be a totter'd weed of small worth held:\n",
    "Then being asked, where all thy beauty lies,\n",
    "Where all the treasure of thy lusty days;\n",
    "To say, within thine own deep sunken eyes,\n",
    "Were an all-eating shame, and thriftless praise.\n",
    "How much more praise deserv'd thy beauty's use,\n",
    "If thou couldst answer 'This fair child of mine\n",
    "Shall sum my count, and make my old excuse,'\n",
    "Proving his beauty by succession thine!\n",
    "This were to be new made when thou art old,\n",
    "And see thy blood warm when thou feel'st it cold.\"\"\".split()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trigram = [((test_sentence[i],test_sentence[i+1]),(test_sentence[i+2]))\n",
    "          for i in range(len(test_sentence) - 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocb = set(test_sentence)\n",
    "word_to_idx = {word: i for i, word in enumerate(vocb)}\n",
    "idx_to_word = {word_to_idx[word]: word for word in word_to_idx}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NgramModel(nn.Module):\n",
    "    def __init__(self, vocb_size, context_size, n_dim):\n",
    "        super(NgramModel, self).__init__()\n",
    "        self.n_word = vocb_size\n",
    "        self.embedding = nn.Embedding(self.n_word, n_dim)\n",
    "        self.linear1 = nn.Linear(context_size * n_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, self.n_word)\n",
    "\n",
    "    def forward(self, x):\n",
    "        emb = self.embedding(x)\n",
    "        emb = emb.view(1, -1)\n",
    "        out = self.linear1(emb)\n",
    "        out = F.relu(out)\n",
    "        out = self.linear2(out)\n",
    "        log_prob = F.log_softmax(out)\n",
    "        return log_prob\n",
    "\n",
    "\n",
    "ngrammodel = NgramModel(len(word_to_idx), CONTEXT_SIZE, 100)\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(ngrammodel.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n",
      "**********\n",
      "Loss: 5.329770\n",
      "epoch: 2\n",
      "**********\n",
      "Loss: 5.271808\n",
      "epoch: 3\n",
      "**********\n",
      "Loss: 5.214428\n",
      "epoch: 4\n",
      "**********\n",
      "Loss: 5.157653\n",
      "epoch: 5\n",
      "**********\n",
      "Loss: 5.101325\n",
      "epoch: 6\n",
      "**********\n",
      "Loss: 5.045189\n",
      "epoch: 7\n",
      "**********\n",
      "Loss: 4.989182\n",
      "epoch: 8\n",
      "**********\n",
      "Loss: 4.933473\n",
      "epoch: 9\n",
      "**********\n",
      "Loss: 4.877858\n",
      "epoch: 10\n",
      "**********\n",
      "Loss: 4.822221\n",
      "epoch: 11\n",
      "**********\n",
      "Loss: 4.766770\n",
      "epoch: 12\n",
      "**********\n",
      "Loss: 4.711290\n",
      "epoch: 13\n",
      "**********\n",
      "Loss: 4.655749\n",
      "epoch: 14\n",
      "**********\n",
      "Loss: 4.599836\n",
      "epoch: 15\n",
      "**********\n",
      "Loss: 4.543696\n",
      "epoch: 16\n",
      "**********\n",
      "Loss: 4.487429\n",
      "epoch: 17\n",
      "**********\n",
      "Loss: 4.430806\n",
      "epoch: 18\n",
      "**********\n",
      "Loss: 4.373873\n",
      "epoch: 19\n",
      "**********\n",
      "Loss: 4.316477\n",
      "epoch: 20\n",
      "**********\n",
      "Loss: 4.258440\n",
      "epoch: 21\n",
      "**********\n",
      "Loss: 4.200029\n",
      "epoch: 22\n",
      "**********\n",
      "Loss: 4.141170\n",
      "epoch: 23\n",
      "**********\n",
      "Loss: 4.081846\n",
      "epoch: 24\n",
      "**********\n",
      "Loss: 4.021859\n",
      "epoch: 25\n",
      "**********\n",
      "Loss: 3.961349\n",
      "epoch: 26\n",
      "**********\n",
      "Loss: 3.900334\n",
      "epoch: 27\n",
      "**********\n",
      "Loss: 3.838820\n",
      "epoch: 28\n",
      "**********\n",
      "Loss: 3.776951\n",
      "epoch: 29\n",
      "**********\n",
      "Loss: 3.714616\n",
      "epoch: 30\n",
      "**********\n",
      "Loss: 3.651756\n",
      "epoch: 31\n",
      "**********\n",
      "Loss: 3.588542\n",
      "epoch: 32\n",
      "**********\n",
      "Loss: 3.525037\n",
      "epoch: 33\n",
      "**********\n",
      "Loss: 3.460837\n",
      "epoch: 34\n",
      "**********\n",
      "Loss: 3.396495\n",
      "epoch: 35\n",
      "**********\n",
      "Loss: 3.331768\n",
      "epoch: 36\n",
      "**********\n",
      "Loss: 3.266836\n",
      "epoch: 37\n",
      "**********\n",
      "Loss: 3.201543\n",
      "epoch: 38\n",
      "**********\n",
      "Loss: 3.136095\n",
      "epoch: 39\n",
      "**********\n",
      "Loss: 3.070584\n",
      "epoch: 40\n",
      "**********\n",
      "Loss: 3.004931\n",
      "epoch: 41\n",
      "**********\n",
      "Loss: 2.939129\n",
      "epoch: 42\n",
      "**********\n",
      "Loss: 2.873296\n",
      "epoch: 43\n",
      "**********\n",
      "Loss: 2.807548\n",
      "epoch: 44\n",
      "**********\n",
      "Loss: 2.741841\n",
      "epoch: 45\n",
      "**********\n",
      "Loss: 2.676268\n",
      "epoch: 46\n",
      "**********\n",
      "Loss: 2.610595\n",
      "epoch: 47\n",
      "**********\n",
      "Loss: 2.545378\n",
      "epoch: 48\n",
      "**********\n",
      "Loss: 2.480200\n",
      "epoch: 49\n",
      "**********\n",
      "Loss: 2.415375\n",
      "epoch: 50\n",
      "**********\n",
      "Loss: 2.350829\n",
      "epoch: 51\n",
      "**********\n",
      "Loss: 2.286689\n",
      "epoch: 52\n",
      "**********\n",
      "Loss: 2.222910\n",
      "epoch: 53\n",
      "**********\n",
      "Loss: 2.159681\n",
      "epoch: 54\n",
      "**********\n",
      "Loss: 2.097047\n",
      "epoch: 55\n",
      "**********\n",
      "Loss: 2.034908\n",
      "epoch: 56\n",
      "**********\n",
      "Loss: 1.973555\n",
      "epoch: 57\n",
      "**********\n",
      "Loss: 1.912943\n",
      "epoch: 58\n",
      "**********\n",
      "Loss: 1.853064\n",
      "epoch: 59\n",
      "**********\n",
      "Loss: 1.793975\n",
      "epoch: 60\n",
      "**********\n",
      "Loss: 1.735889\n",
      "epoch: 61\n",
      "**********\n",
      "Loss: 1.678778\n",
      "epoch: 62\n",
      "**********\n",
      "Loss: 1.622424\n",
      "epoch: 63\n",
      "**********\n",
      "Loss: 1.567316\n",
      "epoch: 64\n",
      "**********\n",
      "Loss: 1.513191\n",
      "epoch: 65\n",
      "**********\n",
      "Loss: 1.460317\n",
      "epoch: 66\n",
      "**********\n",
      "Loss: 1.408543\n",
      "epoch: 67\n",
      "**********\n",
      "Loss: 1.358006\n",
      "epoch: 68\n",
      "**********\n",
      "Loss: 1.308619\n",
      "epoch: 69\n",
      "**********\n",
      "Loss: 1.260635\n",
      "epoch: 70\n",
      "**********\n",
      "Loss: 1.213888\n",
      "epoch: 71\n",
      "**********\n",
      "Loss: 1.168567\n",
      "epoch: 72\n",
      "**********\n",
      "Loss: 1.124541\n",
      "epoch: 73\n",
      "**********\n",
      "Loss: 1.081962\n",
      "epoch: 74\n",
      "**********\n",
      "Loss: 1.040691\n",
      "epoch: 75\n",
      "**********\n",
      "Loss: 1.000948\n",
      "epoch: 76\n",
      "**********\n",
      "Loss: 0.962479\n",
      "epoch: 77\n",
      "**********\n",
      "Loss: 0.925464\n",
      "epoch: 78\n",
      "**********\n",
      "Loss: 0.889808\n",
      "epoch: 79\n",
      "**********\n",
      "Loss: 0.855497\n",
      "epoch: 80\n",
      "**********\n",
      "Loss: 0.822578\n",
      "epoch: 81\n",
      "**********\n",
      "Loss: 0.790957\n",
      "epoch: 82\n",
      "**********\n",
      "Loss: 0.760679\n",
      "epoch: 83\n",
      "**********\n",
      "Loss: 0.731640\n",
      "epoch: 84\n",
      "**********\n",
      "Loss: 0.703927\n",
      "epoch: 85\n",
      "**********\n",
      "Loss: 0.677396\n",
      "epoch: 86\n",
      "**********\n",
      "Loss: 0.652023\n",
      "epoch: 87\n",
      "**********\n",
      "Loss: 0.627841\n",
      "epoch: 88\n",
      "**********\n",
      "Loss: 0.604747\n",
      "epoch: 89\n",
      "**********\n",
      "Loss: 0.582710\n",
      "epoch: 90\n",
      "**********\n",
      "Loss: 0.561743\n",
      "epoch: 91\n",
      "**********\n",
      "Loss: 0.541760\n",
      "epoch: 92\n",
      "**********\n",
      "Loss: 0.522708\n",
      "epoch: 93\n",
      "**********\n",
      "Loss: 0.504623\n",
      "epoch: 94\n",
      "**********\n",
      "Loss: 0.487393\n",
      "epoch: 95\n",
      "**********\n",
      "Loss: 0.471004\n",
      "epoch: 96\n",
      "**********\n",
      "Loss: 0.455396\n",
      "epoch: 97\n",
      "**********\n",
      "Loss: 0.440556\n",
      "epoch: 98\n",
      "**********\n",
      "Loss: 0.426468\n",
      "epoch: 99\n",
      "**********\n",
      "Loss: 0.413039\n",
      "epoch: 100\n",
      "**********\n",
      "Loss: 0.400283\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    print('epoch: {}'.format(epoch + 1))\n",
    "    print('*' * 10)\n",
    "    running_loss = 0\n",
    "    for data in trigram:\n",
    "        word, label = data\n",
    "        word = Variable(torch.LongTensor([word_to_idx[i] for i in word]))\n",
    "        label = Variable(torch.LongTensor([word_to_idx[label]]))\n",
    "        # forward\n",
    "        out = ngrammodel(word)\n",
    "        loss = criterion(out, label)\n",
    "        running_loss += loss.data[0]\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print('Loss: {:.6f}'.format(running_loss / len(word_to_idx)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('shall', 'besiege'), 'thy')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 72\n",
       " 83\n",
       "[torch.LongTensor of size 2]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "real word is And, predict word is And\n"
     ]
    }
   ],
   "source": [
    "word, label = trigram[5]\n",
    "word = Variable(torch.LongTensor([word_to_idx[i] for i in word]))\n",
    "out = ngrammodel(word)\n",
    "_, predict_label = torch.max(out, 1)\n",
    "predict_word = idx_to_word[predict_label.data[0][0]]\n",
    "print('real word is {}, predict word is {}'.format(label, predict_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
