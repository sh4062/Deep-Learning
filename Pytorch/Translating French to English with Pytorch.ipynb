{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#fast.ai Lesson 13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Translating French to English with Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import re, pickle, collections, bcolz, numpy as np, keras, sklearn, math, operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "import gensim\n",
    "import torch, torch.nn as nn\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path='./data/'\n",
    "dpath='./data/translate/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The French-English parallel corpus can be downloaded from http://www.statmt.org/wmt10/training-giga-fren.tar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fname=path+'giga-fren.release2.fixed'\n",
    "en_fname = fname+'.en'\n",
    "fr_fname = fname+'.fr'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123abc456\n",
      "123\n",
      "abc\n",
      "456\n",
      "What the fuck ?\n"
     ]
    }
   ],
   "source": [
    "#review\n",
    "import re\n",
    "a = \"123abc456\"\n",
    "b = \"What the fuck ?\"\n",
    "re_eq = re.compile('^(Wh[^?.!]+\\?)')\n",
    "print (re.search(\"([0-9]*)([a-z]*)([0-9]*)\",a).group())   #123abc456\n",
    "print (re.search(\"([0-9]*)([a-z]*)([0-9]*)\",a).group(1))   #123\n",
    "print (re.search(\"([0-9]*)([a-z]*)([0-9]*)\",a).group(2))   #abc\n",
    "print (re.search(\"([0-9]*)([a-z]*)([0-9]*)\",a).group(3) )  #456\n",
    "print (re_eq.search(b).group() )  #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We'll just learn to translate questions that begin with 'Wh' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52331"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re_eq = re.compile('^(Wh[^?.!]+\\?)')\n",
    "re_fq = re.compile('^([^?.!]+\\?)')\n",
    "\n",
    "lines = ((re_eq.search(eq), re_fq.search(fq)) \n",
    "         for eq, fq in zip(open(en_fname), open(fr_fname)))\n",
    "\n",
    "qs = [(e.group(), f.group()) for e,f in lines if e and f]; len(qs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('What is light ?', 'Qu’est-ce que la lumière?'),\n",
       " ('Who are we?', 'Où sommes-nous?'),\n",
       " ('Where did we come from?', \"D'où venons-nous?\"),\n",
       " ('What would we do without it?', 'Que ferions-nous sans elle ?'),\n",
       " ('What is the absolute location (latitude and longitude) of Badger, Newfoundland and Labrador?',\n",
       "  'Quelle sont les coordonnées (latitude et longitude) de Badger, à Terre-Neuve-etLabrador?'),\n",
       " ('What is the major aboriginal group on Vancouver Island?',\n",
       "  'Quel est le groupe autochtone principal sur l’île de Vancouver?')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qs[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(qs, open(dpath+'fr-en-qs.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "qs = pickle.load(open(dpath+'fr-en-qs.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "en_qs, fr_qs = zip(*qs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "re_apos = re.compile(r\"(\\w)'s\\b\")         # make 's a separate word\n",
    "re_mw_punc = re.compile(r\"(\\w[’'])(\\w)\")  # other ' in a word creates 2 words\n",
    "re_punc = re.compile(\"([\\\"().,;:/_?!—])\") # add spaces around punctuation\n",
    "re_mult_space = re.compile(r\"  *\")        # replace multiple spaces with just one\n",
    "\n",
    "def simple_toks(sent):\n",
    "    sent = re_apos.sub(r\"\\1 's\", sent)\n",
    "    sent = re_mw_punc.sub(r\"\\1 \\2\", sent)\n",
    "    sent = re_punc.sub(r\" \\1 \", sent).replace('-', ' ')\n",
    "    sent = re_mult_space.sub(' ', sent)\n",
    "    return sent.lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['qu’', 'est', 'ce', 'que', 'la', 'lumière', '?'],\n",
       " ['où', 'sommes', 'nous', '?'],\n",
       " [\"d'\", 'où', 'venons', 'nous', '?'],\n",
       " ['que', 'ferions', 'nous', 'sans', 'elle', '?']]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fr_qtoks = list(map(simple_toks, fr_qs)); fr_qtoks[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['what', 'is', 'light', '?'],\n",
       " ['who', 'are', 'we', '?'],\n",
       " ['where', 'did', 'we', 'come', 'from', '?'],\n",
       " ['what', 'would', 'we', 'do', 'without', 'it', '?']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_qtoks = list(map(simple_toks, en_qs)); en_qtoks[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rachel', \"'s\", 'baby', 'is', 'cuter', 'than', 'other', \"'s\", '.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_toks(\"Rachel's baby is cuter than other's.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PAD = 0; SOS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def toks2ids(sents):\n",
    "    voc_cnt = collections.Counter(t for sent in sents for t in sent)\n",
    "    vocab = sorted(voc_cnt, key=voc_cnt.get, reverse=True)\n",
    "    vocab.insert(PAD, \"<PAD>\")\n",
    "    vocab.insert(SOS, \"<SOS>\")\n",
    "    w2id = {w:i for i,w in enumerate(vocab)}\n",
    "    ids = [[w2id[t] for t in sent] for sent in sents]\n",
    "    return ids, vocab, w2id, voc_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fr_ids, fr_vocab, fr_w2id, fr_counts = toks2ids(fr_qtoks)\n",
    "en_ids, en_vocab, en_w2id, en_counts = toks2ids(en_qtoks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52331"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fr_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<PAD>',\n",
       " '<SOS>',\n",
       " '?',\n",
       " 'the',\n",
       " 'what',\n",
       " 'of',\n",
       " 'to',\n",
       " 'and',\n",
       " 'is',\n",
       " 'in',\n",
       " 'are',\n",
       " ',',\n",
       " 'a',\n",
       " 'for',\n",
       " 'do',\n",
       " 'why',\n",
       " 'be',\n",
       " 'you',\n",
       " 'or',\n",
       " 'who',\n",
       " 'on',\n",
       " 'this',\n",
       " 'that',\n",
       " 'have',\n",
       " 'your',\n",
       " 'when',\n",
       " 'can',\n",
       " 'should',\n",
       " 'will',\n",
       " 'which',\n",
       " 'does',\n",
       " 'i',\n",
       " 'with',\n",
       " 'where',\n",
       " 'if',\n",
       " 'it',\n",
       " 'about',\n",
       " 'would',\n",
       " '\"',\n",
       " '(',\n",
       " ')',\n",
       " 'canada',\n",
       " 'we',\n",
       " 'from',\n",
       " 'not',\n",
       " 'an',\n",
       " 'by',\n",
       " 'as',\n",
       " '/',\n",
       " 'information',\n",
       " 'these',\n",
       " 'health',\n",
       " \"'s\",\n",
       " 'was',\n",
       " 'they',\n",
       " 's',\n",
       " 'did',\n",
       " 'other',\n",
       " 'their',\n",
       " 'has',\n",
       " 'were',\n",
       " 'at',\n",
       " 'been',\n",
       " 'new',\n",
       " 'use',\n",
       " 'program',\n",
       " 'most',\n",
       " 'how',\n",
       " 'need',\n",
       " 'my',\n",
       " 'role',\n",
       " 'more',\n",
       " 'between',\n",
       " 'government',\n",
       " 'public',\n",
       " 'could',\n",
       " 'canadian',\n",
       " 'people',\n",
       " 'research',\n",
       " 'impact',\n",
       " 'there',\n",
       " 'happens',\n",
       " 'our',\n",
       " 'used',\n",
       " 'work',\n",
       " 'mean',\n",
       " 'so',\n",
       " 'such',\n",
       " 'policy',\n",
       " 'important',\n",
       " 'its',\n",
       " 'being',\n",
       " 'done',\n",
       " 'services',\n",
       " 'some',\n",
       " 'any',\n",
       " 'think',\n",
       " 'level',\n",
       " 'time',\n",
       " 't',\n",
       " 'make',\n",
       " 'service',\n",
       " 'development',\n",
       " 'take',\n",
       " 'process',\n",
       " 'community',\n",
       " 'all',\n",
       " 'kind',\n",
       " 'national',\n",
       " 'available',\n",
       " 'best',\n",
       " 'into',\n",
       " 'project',\n",
       " 'like',\n",
       " 'support',\n",
       " 'benefits',\n",
       " 'them',\n",
       " 'get',\n",
       " 'under',\n",
       " 'required',\n",
       " 'find',\n",
       " 'system',\n",
       " 'one',\n",
       " 'management',\n",
       " 'activities',\n",
       " 'change',\n",
       " 'made',\n",
       " 'future',\n",
       " 'changes',\n",
       " 'place',\n",
       " 'type',\n",
       " 'main',\n",
       " 'issues',\n",
       " 'care',\n",
       " 'first',\n",
       " 'ensure',\n",
       " 'social',\n",
       " 'international',\n",
       " 'business',\n",
       " 'current',\n",
       " 'rights',\n",
       " 'factors',\n",
       " 'help',\n",
       " 'risk',\n",
       " 'than',\n",
       " 'key',\n",
       " 'doing',\n",
       " 'types',\n",
       " 'taken',\n",
       " 'measures',\n",
       " 'european',\n",
       " 'might',\n",
       " 'funding',\n",
       " 'know',\n",
       " 'organization',\n",
       " 'out',\n",
       " 'see',\n",
       " 'training',\n",
       " 'plan',\n",
       " 'environmental',\n",
       " 'now',\n",
       " 'those',\n",
       " 'up',\n",
       " 'results',\n",
       " 'apply',\n",
       " 'women',\n",
       " 'years',\n",
       " 'following',\n",
       " 'want',\n",
       " 'challenges',\n",
       " 'needs',\n",
       " 'different',\n",
       " 'objectives',\n",
       " 'federal',\n",
       " 'tax',\n",
       " 'act',\n",
       " 'application',\n",
       " 'human',\n",
       " 'each',\n",
       " 'us',\n",
       " 'programs',\n",
       " 'after',\n",
       " 'economic',\n",
       " 'next',\n",
       " 'data',\n",
       " 'countries',\n",
       " 'access',\n",
       " 'knowledge',\n",
       " 'country',\n",
       " 'effects',\n",
       " 'impacts',\n",
       " 'products',\n",
       " 'needed',\n",
       " 'may',\n",
       " 'food',\n",
       " 'resources',\n",
       " 'term',\n",
       " 'steps',\n",
       " 'cost',\n",
       " 'had',\n",
       " 'within',\n",
       " 'over',\n",
       " 'play',\n",
       " 'must',\n",
       " 'world',\n",
       " 'trade',\n",
       " 'terms',\n",
       " 'involved',\n",
       " 'sector',\n",
       " 'costs',\n",
       " 'say',\n",
       " 'responsible',\n",
       " 'area',\n",
       " 'difference',\n",
       " 'specific',\n",
       " 'status',\n",
       " 'language',\n",
       " 'education',\n",
       " 'working',\n",
       " 'way',\n",
       " 'order',\n",
       " 'requirements',\n",
       " 'potential',\n",
       " 'areas',\n",
       " 'decision',\n",
       " 'year',\n",
       " 'go',\n",
       " 'industry',\n",
       " 'appropriate',\n",
       " 'effect',\n",
       " 'considered',\n",
       " 'effective',\n",
       " 'during',\n",
       " 'provide',\n",
       " 'action',\n",
       " 'eligible',\n",
       " 'life',\n",
       " 'financial',\n",
       " 'product',\n",
       " 'non',\n",
       " 'implications',\n",
       " 'assessment',\n",
       " 'then',\n",
       " 'situation',\n",
       " 'canadians',\n",
       " 'conditions',\n",
       " '_',\n",
       " 'family',\n",
       " 'long',\n",
       " 'two',\n",
       " 'happen',\n",
       " 'come',\n",
       " 'children',\n",
       " 'protection',\n",
       " 'water',\n",
       " 'market',\n",
       " 'related',\n",
       " 'report',\n",
       " 'group',\n",
       " 'address',\n",
       " 'form',\n",
       " 'going',\n",
       " 'member',\n",
       " 'part',\n",
       " 'value',\n",
       " ':',\n",
       " 'consider',\n",
       " 'through',\n",
       " 'expect',\n",
       " 'criteria',\n",
       " 'relationship',\n",
       " 'communities',\n",
       " 'no',\n",
       " 'kinds',\n",
       " 'eu',\n",
       " 'quality',\n",
       " 'policies',\n",
       " 'using',\n",
       " 'respect',\n",
       " 'while',\n",
       " 'good',\n",
       " 'commission',\n",
       " 'europe',\n",
       " 'right',\n",
       " 'aboriginal',\n",
       " 'benefit',\n",
       " 'before',\n",
       " 'based',\n",
       " 'better',\n",
       " 'risks',\n",
       " 'approach',\n",
       " 'experience',\n",
       " 'purpose',\n",
       " 'agreement',\n",
       " 'environment',\n",
       " 'job',\n",
       " 'he',\n",
       " 'his',\n",
       " 'date',\n",
       " 'general',\n",
       " 'groups',\n",
       " 'security',\n",
       " 'proposed',\n",
       " 'levels',\n",
       " 'barriers',\n",
       " 'issue',\n",
       " 'problems',\n",
       " 'major',\n",
       " 'position',\n",
       " 'regarding',\n",
       " 'learning',\n",
       " 'look',\n",
       " 'number',\n",
       " 'company',\n",
       " 'states',\n",
       " 'state',\n",
       " 'evidence',\n",
       " 'employment',\n",
       " '–',\n",
       " 'makes',\n",
       " 'society',\n",
       " 'projects',\n",
       " 'population',\n",
       " 'making',\n",
       " 'goals',\n",
       " 'practices',\n",
       " 'strategy',\n",
       " 'her',\n",
       " 'safety',\n",
       " 'legal',\n",
       " 'personal',\n",
       " 'members',\n",
       " 'department',\n",
       " 'achieve',\n",
       " 'learn',\n",
       " 'improve',\n",
       " 'strategies',\n",
       " 'employees',\n",
       " 'necessary',\n",
       " 'period',\n",
       " 'rate',\n",
       " 'activity',\n",
       " 'law',\n",
       " 'given',\n",
       " 'receive',\n",
       " 'priorities',\n",
       " 'regulations',\n",
       " 'actions',\n",
       " 'political',\n",
       " 'income',\n",
       " 'result',\n",
       " 'provided',\n",
       " 'contact',\n",
       " 'person',\n",
       " 'whether',\n",
       " 'initiative',\n",
       " 'school',\n",
       " 'percentage',\n",
       " 'implementation',\n",
       " 'particular',\n",
       " 'same',\n",
       " 'well',\n",
       " 'but',\n",
       " 'become',\n",
       " 'else',\n",
       " 'developing',\n",
       " 'only',\n",
       " 'return',\n",
       " 'special',\n",
       " 'high',\n",
       " 'success',\n",
       " 'learned',\n",
       " 'progress',\n",
       " 'drug',\n",
       " 'tools',\n",
       " 'included',\n",
       " 'study',\n",
       " 'investment',\n",
       " 'both',\n",
       " 'framework',\n",
       " 'local',\n",
       " 'standards',\n",
       " 'others',\n",
       " 'including',\n",
       " 'consequences',\n",
       " 'youth',\n",
       " 'air',\n",
       " 'associated',\n",
       " 'me',\n",
       " 'questions',\n",
       " 'cultural',\n",
       " 'performance',\n",
       " 'individual',\n",
       " 'common',\n",
       " 'opportunities',\n",
       " 'expected',\n",
       " 'outcomes',\n",
       " 'review',\n",
       " 'values',\n",
       " 'responsibilities',\n",
       " 'agency',\n",
       " 'offer',\n",
       " 'possible',\n",
       " 'reporting',\n",
       " 'advantages',\n",
       " 'likely',\n",
       " 'participation',\n",
       " 'pay',\n",
       " 'put',\n",
       " 'mechanisms',\n",
       " 'organizations',\n",
       " 'capacity',\n",
       " 'problem',\n",
       " 'evaluation',\n",
       " 'property',\n",
       " 'home',\n",
       " 'own',\n",
       " 'case',\n",
       " 'technology',\n",
       " 'sources',\n",
       " 'skills',\n",
       " 'what’',\n",
       " 'name',\n",
       " 'give',\n",
       " 'rules',\n",
       " 'child',\n",
       " 'systems',\n",
       " 'things',\n",
       " 'institutions',\n",
       " 'young',\n",
       " 'exist',\n",
       " 'day',\n",
       " 'further',\n",
       " 'natural',\n",
       " 'today',\n",
       " 'protect',\n",
       " 'committee',\n",
       " 'amount',\n",
       " 'lessons',\n",
       " 'taking',\n",
       " 'initiatives',\n",
       " 'responsibility',\n",
       " 'meet',\n",
       " 'interest',\n",
       " 'currently',\n",
       " 'many',\n",
       " 'e',\n",
       " '1',\n",
       " 'priority',\n",
       " 'view',\n",
       " 'survey',\n",
       " 'council',\n",
       " 'three',\n",
       " 'governments',\n",
       " 'against',\n",
       " 'site',\n",
       " 'contribution',\n",
       " 'strengths',\n",
       " 'planning',\n",
       " 'she',\n",
       " 'private',\n",
       " 'successful',\n",
       " 'processes',\n",
       " 'question',\n",
       " 'money',\n",
       " 'start',\n",
       " 'regional',\n",
       " 'additional',\n",
       " 'office',\n",
       " 'foreign',\n",
       " 'provincial',\n",
       " 'without',\n",
       " 'nature',\n",
       " 'already',\n",
       " 'official',\n",
       " 'total',\n",
       " 'gender',\n",
       " 'increase',\n",
       " 'canada’',\n",
       " 'existing',\n",
       " 'practice',\n",
       " 'forms',\n",
       " 'reduce',\n",
       " 'promote',\n",
       " 'age',\n",
       " 'legislation',\n",
       " 'union',\n",
       " 'sustainable',\n",
       " 'elements',\n",
       " 'relevant',\n",
       " 'documents',\n",
       " 'privacy',\n",
       " 'global',\n",
       " 'develop',\n",
       " 'list',\n",
       " 'control',\n",
       " 'various',\n",
       " 'treatment',\n",
       " 'roles',\n",
       " 'region',\n",
       " 'disease',\n",
       " 'medical',\n",
       " 'regulatory',\n",
       " 'whom',\n",
       " 'assistance',\n",
       " 'subject',\n",
       " 'bring',\n",
       " 'end',\n",
       " 'ways',\n",
       " 'field',\n",
       " 'regulation',\n",
       " 'past',\n",
       " 'nations',\n",
       " 'am',\n",
       " 'another',\n",
       " 'found',\n",
       " 'reasons',\n",
       " 'aspects',\n",
       " 'participate',\n",
       " 'account',\n",
       " 'claim',\n",
       " 'feel',\n",
       " 'plans',\n",
       " 'obtain',\n",
       " 'set',\n",
       " 'media',\n",
       " 'concerns',\n",
       " 'budget',\n",
       " 'land',\n",
       " 'line',\n",
       " 'decisions',\n",
       " 'rates',\n",
       " 'here',\n",
       " 'much',\n",
       " 'basis',\n",
       " 'events',\n",
       " 'contribute',\n",
       " 'options',\n",
       " 'happened',\n",
       " 'targets',\n",
       " 'tell',\n",
       " 'among',\n",
       " 'still',\n",
       " 'context',\n",
       " 'include',\n",
       " 'individuals',\n",
       " 'example',\n",
       " 'point',\n",
       " 'board',\n",
       " 'section',\n",
       " 'live',\n",
       " 'outside',\n",
       " 'conflict',\n",
       " 'authority',\n",
       " '—',\n",
       " 'positive',\n",
       " 'principles',\n",
       " 'delivery',\n",
       " 'drugs',\n",
       " 'culture',\n",
       " 'patients',\n",
       " 'scope',\n",
       " 'regard',\n",
       " 'proposal',\n",
       " 'design',\n",
       " 'affected',\n",
       " 'primary',\n",
       " 'matter',\n",
       " 'small',\n",
       " 'force',\n",
       " 'opinion',\n",
       " 'characteristics',\n",
       " 'present',\n",
       " 'workers',\n",
       " 'methods',\n",
       " 'growth',\n",
       " 'having',\n",
       " 'companies',\n",
       " ';',\n",
       " 'achieved',\n",
       " 'relation',\n",
       " 'employee',\n",
       " 'leave',\n",
       " 'self',\n",
       " 'face',\n",
       " 'someone',\n",
       " 'approaches',\n",
       " 'identified',\n",
       " 'parliament',\n",
       " 'internet',\n",
       " 'negotiations',\n",
       " 'asked',\n",
       " 'standard',\n",
       " 'students',\n",
       " 'climate',\n",
       " 'differences',\n",
       " 'able',\n",
       " 'implemented',\n",
       " 'affect',\n",
       " 'obligations',\n",
       " 'military',\n",
       " 'useful',\n",
       " 'weaknesses',\n",
       " 'influence',\n",
       " 'guidelines',\n",
       " 'languages',\n",
       " 'focus',\n",
       " 'changed',\n",
       " 'citizens',\n",
       " 'keep',\n",
       " 'effectiveness',\n",
       " 'developed',\n",
       " 'species',\n",
       " 'court',\n",
       " 'energy',\n",
       " 'implementing',\n",
       " 'pension',\n",
       " 'electronic',\n",
       " \"don'\",\n",
       " 'file',\n",
       " 'partners',\n",
       " 'code',\n",
       " 'power',\n",
       " 'lead',\n",
       " 'applied',\n",
       " 'last',\n",
       " 'authorities',\n",
       " 'men',\n",
       " 'production',\n",
       " 'families',\n",
       " 'co',\n",
       " 'gst',\n",
       " 'living',\n",
       " 'means',\n",
       " 'covered',\n",
       " 'capital',\n",
       " 'cf',\n",
       " 'goods',\n",
       " 'involvement',\n",
       " 'payments',\n",
       " 'partner',\n",
       " 'real',\n",
       " 'registered',\n",
       " 'technical',\n",
       " 'traditional',\n",
       " 'require',\n",
       " 'parties',\n",
       " 'labour',\n",
       " 'heritage',\n",
       " 'response',\n",
       " 'participants',\n",
       " 'clinical',\n",
       " 'advice',\n",
       " 'professional',\n",
       " 'staff',\n",
       " 'patent',\n",
       " 'guide',\n",
       " 'agreements',\n",
       " 'model',\n",
       " 'innovation',\n",
       " 'records',\n",
       " 'fish',\n",
       " 'average',\n",
       " 'source',\n",
       " 'trends',\n",
       " 'rural',\n",
       " 'interests',\n",
       " 'stakeholders',\n",
       " 'structure',\n",
       " 'measure',\n",
       " 'five',\n",
       " 'economy',\n",
       " 'university',\n",
       " 'prevent',\n",
       " 'understanding',\n",
       " 'forces',\n",
       " 'businesses',\n",
       " 'travel',\n",
       " 'agencies',\n",
       " 'towards',\n",
       " '2006',\n",
       " 'gaps',\n",
       " 'institution',\n",
       " 'transfer',\n",
       " 'team',\n",
       " 're',\n",
       " 'second',\n",
       " 'procedures',\n",
       " 'physical',\n",
       " 'fund',\n",
       " 'less',\n",
       " 'minister',\n",
       " 'decide',\n",
       " 'annual',\n",
       " 'communication',\n",
       " 'analysis',\n",
       " 'recommendations',\n",
       " 'departments',\n",
       " 'deal',\n",
       " 'request',\n",
       " 'building',\n",
       " 'even',\n",
       " 'just',\n",
       " 'objective',\n",
       " 'meant',\n",
       " 'meeting',\n",
       " 'definition',\n",
       " 'since',\n",
       " 'reason',\n",
       " 'monitoring',\n",
       " 'received',\n",
       " 'foods',\n",
       " 'determine',\n",
       " 'added',\n",
       " 'payment',\n",
       " 'implement',\n",
       " 'believe',\n",
       " 'sport',\n",
       " 'document',\n",
       " 'extent',\n",
       " 'paid',\n",
       " 'side',\n",
       " 'healthy',\n",
       " 'hst',\n",
       " 'ask',\n",
       " 'call',\n",
       " 'highest',\n",
       " 'choose',\n",
       " 'registration',\n",
       " 'features',\n",
       " 'event',\n",
       " 'back',\n",
       " 'funds',\n",
       " 'content',\n",
       " 'insurance',\n",
       " 'credit',\n",
       " 'c',\n",
       " 'applications',\n",
       " 'unit',\n",
       " 'third',\n",
       " 'compliance',\n",
       " 'overall',\n",
       " 'defence',\n",
       " 'greatest',\n",
       " 'test',\n",
       " 'maximum',\n",
       " 'happening',\n",
       " 'material',\n",
       " 'alternative',\n",
       " 'province',\n",
       " 'vision',\n",
       " 'search',\n",
       " 'balance',\n",
       " 'low',\n",
       " 'claims',\n",
       " 'addressed',\n",
       " 'domestic',\n",
       " 'centre',\n",
       " 'patient',\n",
       " 'body',\n",
       " 'free',\n",
       " 'really',\n",
       " 'causes',\n",
       " 'copyright',\n",
       " 'behind',\n",
       " 'sectors',\n",
       " 'strategic',\n",
       " 'critical',\n",
       " 'selection',\n",
       " 'minimum',\n",
       " 'cooperation',\n",
       " 'encourage',\n",
       " 'ethical',\n",
       " 'uses',\n",
       " 'target',\n",
       " 'studies',\n",
       " 'whose',\n",
       " 'outcome',\n",
       " 'infrastructure',\n",
       " 'similar',\n",
       " 'consumers',\n",
       " 'french',\n",
       " 'create',\n",
       " \"can'\",\n",
       " 'significant',\n",
       " 'concerned',\n",
       " 'fit',\n",
       " 'whole',\n",
       " 'resource',\n",
       " 'established',\n",
       " 'far',\n",
       " 'sort',\n",
       " 'mandate',\n",
       " 'financing',\n",
       " 'down',\n",
       " 'integration',\n",
       " '2',\n",
       " 'together',\n",
       " 'follow',\n",
       " 'career',\n",
       " 'consumer',\n",
       " 'expectations',\n",
       " 'exposure',\n",
       " 'exactly',\n",
       " 'full',\n",
       " 'degree',\n",
       " 'represent',\n",
       " 'hold',\n",
       " 'stage',\n",
       " 'bill',\n",
       " 'proportion',\n",
       " 'park',\n",
       " 'city',\n",
       " 'particularly',\n",
       " 'researchers',\n",
       " 'efforts',\n",
       " 'around',\n",
       " 'dialogue',\n",
       " 'early',\n",
       " 'contract',\n",
       " 'materials',\n",
       " 'components',\n",
       " 'also',\n",
       " 'intellectual',\n",
       " 'models',\n",
       " 'build',\n",
       " 'instruments',\n",
       " 'least',\n",
       " 'contributed',\n",
       " 'once',\n",
       " 'send',\n",
       " 'concern',\n",
       " 'provisions',\n",
       " 'off',\n",
       " 'looking',\n",
       " 'ones',\n",
       " 'stop',\n",
       " 'technologies',\n",
       " 'equipment',\n",
       " 'option',\n",
       " 'negative',\n",
       " 'agenda',\n",
       " 'convention',\n",
       " 'procedure',\n",
       " 'scientific',\n",
       " 'programme',\n",
       " 'supply',\n",
       " 'complete',\n",
       " 'nation',\n",
       " 'completed',\n",
       " 'known',\n",
       " 'submit',\n",
       " 'longer',\n",
       " 'operations',\n",
       " 'views',\n",
       " 'occur',\n",
       " 'purposes',\n",
       " 'attention',\n",
       " 'workplace',\n",
       " 'register',\n",
       " 'share',\n",
       " 'network',\n",
       " 'statements',\n",
       " 'short',\n",
       " 'relative',\n",
       " 'identify',\n",
       " 'difficult',\n",
       " 'competition',\n",
       " 'size',\n",
       " 'experiences',\n",
       " 'begin',\n",
       " 'rather',\n",
       " 'due',\n",
       " 'achieving',\n",
       " 'arrangements',\n",
       " 'governance',\n",
       " 'adverse',\n",
       " 'organizational',\n",
       " 'limits',\n",
       " 'testing',\n",
       " 'complaint',\n",
       " 'treaty',\n",
       " 'disadvantages',\n",
       " 'indicators',\n",
       " 'designated',\n",
       " 'reports',\n",
       " 'integrated',\n",
       " 'leadership',\n",
       " 'assess',\n",
       " 'prevention',\n",
       " 'united',\n",
       " 'partnership',\n",
       " 'coming',\n",
       " 'basic',\n",
       " 'higher',\n",
       " 'industrial',\n",
       " 'often',\n",
       " 'reserve',\n",
       " 'civil',\n",
       " 'suggest',\n",
       " 'receiving',\n",
       " 'regions',\n",
       " 'permit',\n",
       " 'external',\n",
       " 'reach',\n",
       " 'educational',\n",
       " 'operational',\n",
       " 'cancer',\n",
       " 'cannot',\n",
       " 'acceptable',\n",
       " 'awareness',\n",
       " 'commercial',\n",
       " 'contributions',\n",
       " 'recommend',\n",
       " 'above',\n",
       " 'reported',\n",
       " 'laws',\n",
       " 'employer',\n",
       " 'genetic',\n",
       " 'single',\n",
       " 'certain',\n",
       " '10',\n",
       " 'filing',\n",
       " 'party',\n",
       " 'deliver',\n",
       " 'provinces',\n",
       " 'partnerships',\n",
       " 'recent',\n",
       " 'recommended',\n",
       " 'ozone',\n",
       " 'something',\n",
       " 'corporate',\n",
       " 'promotion',\n",
       " 'themselves',\n",
       " 'history',\n",
       " 'enforcement',\n",
       " 'symptoms',\n",
       " 'clients',\n",
       " 'minority',\n",
       " 'ip',\n",
       " 'unique',\n",
       " 'markets',\n",
       " 'web',\n",
       " 'licence',\n",
       " 'distribution',\n",
       " 'export',\n",
       " 'per',\n",
       " 'cover',\n",
       " 'improvements',\n",
       " 'allow',\n",
       " 'providing',\n",
       " 'goal',\n",
       " 'conduct',\n",
       " 'open',\n",
       " 'cases',\n",
       " 'situations',\n",
       " 'located',\n",
       " '2007',\n",
       " 'function',\n",
       " 'post',\n",
       " 'accountability',\n",
       " 'border',\n",
       " 'move',\n",
       " 'sites',\n",
       " 'users',\n",
       " 'networks',\n",
       " 'increasing',\n",
       " 'demand',\n",
       " 'understand',\n",
       " 'works',\n",
       " 'large',\n",
       " 'safe',\n",
       " 'great',\n",
       " 'price',\n",
       " 'advantage',\n",
       " 'equity',\n",
       " 'actually',\n",
       " 'effectively',\n",
       " 'reasonable',\n",
       " 'violence',\n",
       " 'discrimination',\n",
       " 'essential',\n",
       " 'cra',\n",
       " 'notice',\n",
       " 'criminal',\n",
       " 'buy',\n",
       " 'across',\n",
       " 'operation',\n",
       " 'north',\n",
       " 'reference',\n",
       " 'perspective',\n",
       " 'housing',\n",
       " 'providers',\n",
       " 'facilities',\n",
       " '2005',\n",
       " 'message',\n",
       " 'persons',\n",
       " 'collective',\n",
       " 'substances',\n",
       " 'smoking',\n",
       " ...]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word vectors\n",
    "\n",
    "\n",
    "Stanford's GloVe word vectors can be downloaded from https://nlp.stanford.edu/projects/glove/ \n",
    "\n",
    "For French word vectors, we're using those from http://fauconnier.github.io/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_glove(loc):\n",
    "    return (bcolz.open(loc+'.txt')[:],\n",
    "        pickle.load(open(loc+'_words.pkl','rb'), encoding='latin1'),\n",
    "        pickle.load(open(loc+'_idx.pkl','rb'), encoding='latin1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for line in f:\n",
    "    en_vecs = line.split()\n",
    "    en_wv_word = en_vecs[0]\n",
    "    en_wv_idx = np.asarray(en_vecs[1:], dtype='float32')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_io.TextIOWrapper name='./data/nlp/glove.6B.100d.txt' mode='r' encoding='utf-8'>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/nlp/glove.6B.100d.txt',encoding=\"utf-8\", mode=\"r\") as lines:\n",
    "    en_w2v = {line.split()[0]: np.array(list(map(float, line.split()[1:])))\n",
    "           for line in lines}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.32307 , -0.87616 ,  0.21977 ,  0.25268 ,  0.22976 ,  0.7388  ,\n",
       "       -0.37954 , -0.35307 , -0.84369 , -1.1113  , -0.30266 ,  0.33178 ,\n",
       "       -0.25113 ,  0.30448 , -0.077491, -0.89815 ,  0.092496, -1.1407  ,\n",
       "       -0.58324 ,  0.66869 , -0.23122 , -0.95855 ,  0.28262 , -0.078848,\n",
       "        0.75315 ,  0.26584 ,  0.3422  , -0.33949 ,  0.95608 ,  0.065641,\n",
       "        0.45747 ,  0.39835 ,  0.57965 ,  0.39267 , -0.21851 ,  0.58795 ,\n",
       "       -0.55999 ,  0.63368 , -0.043983, -0.68731 , -0.37841 ,  0.38026 ,\n",
       "        0.61641 , -0.88269 , -0.12346 , -0.37928 , -0.38318 ,  0.23868 ,\n",
       "        0.6685  , -0.43321 , -0.11065 ,  0.081723,  1.1569  ,  0.78958 ,\n",
       "       -0.21223 , -2.3211  , -0.67806 ,  0.44561 ,  0.65707 ,  0.1045  ,\n",
       "        0.46217 ,  0.19912 ,  0.25802 ,  0.057194,  0.53443 , -0.43133 ,\n",
       "       -0.34311 ,  0.59789 , -0.58417 ,  0.068995,  0.23944 , -0.85181 ,\n",
       "        0.30379 , -0.34177 , -0.25746 , -0.031101, -0.16285 ,  0.45169 ,\n",
       "       -0.91627 ,  0.64521 ,  0.73281 , -0.22752 ,  0.30226 ,  0.044801,\n",
       "       -0.83741 ,  0.55006 , -0.52506 , -1.7357  ,  0.4751  , -0.70487 ,\n",
       "        0.056939, -0.7132  ,  0.089623,  0.41394 , -1.3363  , -0.61915 ,\n",
       "       -0.33089 , -0.52881 ,  0.16483 , -0.98878 ])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_w2v['king']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# en_vecs, en_wv_word, en_wv_idx = load_glove('/data/datasets/nlp/glove/results/glove.6B.100d')\n",
    "# en_w2v = {w: en_vecs[en_wv_idx[w]] for w in en_wv_word}\n",
    "# n_en_vec, dim_en_vec = en_vecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('./data/nlp/glove.6B.100d.txt',encoding=\"utf-8\", mode=\"r\") as lines:\n",
    "#     en_wv_word=[line.split()[0]for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(en_wv_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_en_vec=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v_path='./data/nlp/frWac_non_lem_no_postag_no_phrase_200_skip_cut100.bin'\n",
    "fr_model = gensim.models.KeyedVectors.load_word2vec_format(w2v_path, binary=True)\n",
    "fr_voc = fr_model.vocab\n",
    "dim_fr_vec = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "155562"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fr_voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_emb(w2v, targ_vocab, dim_vec):\n",
    "    vocab_size = len(targ_vocab)\n",
    "    emb = np.zeros((vocab_size, dim_vec))\n",
    "    found=0\n",
    "\n",
    "    for i, word in enumerate(targ_vocab):\n",
    "        try: emb[i] = w2v[word]; found+=1\n",
    "        except KeyError: emb[i] = np.random.normal(scale=0.6, size=(dim_vec,))\n",
    "\n",
    "    return emb, found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.keyedvectors.KeyedVectors at 0x7fba885249b0>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fr_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((19549, 100), 17251)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_embs, found = create_emb(en_w2v, en_vocab, dim_en_vec); en_embs.shape, found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((26709, 200), 21878)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fr_embs, found = create_emb(fr_model, fr_vocab, dim_fr_vec); fr_embs.shape, found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each sentence has to be of equal length. Keras has a convenient function `pad_sequences` to truncate and/or pad each sentence as required - even although we're not using keras for the neural net, we can still use any functions from it we need!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((52331, 30), (52331, 30), (19549, 100), (26709, 200))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "maxlen = 30\n",
    "en_padded = pad_sequences(en_ids, maxlen, 'int64', \"post\", \"post\")\n",
    "fr_padded = pad_sequences(fr_ids, maxlen, 'int64', \"post\", \"post\")\n",
    "en_padded.shape, fr_padded.shape, en_embs.shape,fr_embs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(47097, 30), (5234, 30), (47097, 30), (5234, 30)]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import model_selection\n",
    "fr_train, fr_test, en_train, en_test = model_selection.train_test_split(\n",
    "    fr_padded, en_padded, test_size=0.1)\n",
    "\n",
    "[o.shape for o in (fr_train, fr_test, en_train, en_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([   9,   46,  203,   11,   12,   83,   51,   21, 1002,   36,   21,\n",
       "        3379,   15,  183,   14, 5705,  109, 1469,    2,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0]),\n",
       " array([  4,  11,   9,  24, 449,  11,  27,   3, 682,  18, 759,  14,   6,\n",
       "        408, 427, 283,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0]))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fr_train[0], en_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "### Encoder-Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turning a sequence into a representation can be done using an RNN (called the 'encoder'. This approach is useful because RNN's are able to keep track of state and memory, which is obviously important in forming a complete understanding of a sentence.\n",
    "* `bidirectional=True` passes the original sequence through an RNN, and the reversed sequence through a different RNN and concatenates the results. This allows us to look forward and backwards.\n",
    "* We do this because in language things that happen later often influence what came before (i.e. in Spanish, \"el chico, la chica\" means the boy, the girl; the word for \"the\" is determined by the gender of the subject, which comes after)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def long_t(arr): return Variable(torch.LongTensor(arr)).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_emb_t = torch.FloatTensor(fr_embs).cuda()\n",
    "en_emb_t = torch.FloatTensor(en_embs).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_emb(emb_mat, non_trainable=False):\n",
    "    output_size, emb_size = emb_mat.size()\n",
    "    emb = nn.Embedding(output_size, emb_size)\n",
    "    emb.load_state_dict({'weight': emb_mat})\n",
    "    if non_trainable:\n",
    "        for param in emb.parameters(): \n",
    "            param.requires_grad = False\n",
    "    return emb, emb_size, output_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, embs, hidden_size, n_layers=2):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.emb, emb_size, output_size = create_emb(embs, True)\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.gru = nn.GRU(emb_size, hidden_size, batch_first=True, num_layers=n_layers)\n",
    "#                          ,bidirectional=True)\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        return self.gru(self.emb(input), hidden)\n",
    "\n",
    "    def initHidden(self, batch_size):\n",
    "        return Variable(torch.zeros(self.n_layers, batch_size, self.hidden_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encode(inp, encoder):\n",
    "    batch_size, input_length = inp.size()\n",
    "    hidden = encoder.initHidden(batch_size).cuda()\n",
    "    enc_outputs, hidden = encoder(inp, hidden)\n",
    "    return long_t([SOS]*batch_size), enc_outputs, hidden    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we arrive at a vector representation of the sequence which captures everything we need to translate it. We feed this vector into more RNN's, which are trying to generate the labels. After this, we make a classification for what each word is in the output sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, embs, hidden_size, n_layers=2):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.emb, emb_size, output_size = create_emb(embs)\n",
    "        self.gru = nn.GRU(emb_size, hidden_size, batch_first=True, num_layers=n_layers)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, inp, hidden):\n",
    "        emb = self.emb(inp).unsqueeze(1)\n",
    "        res, hidden = self.gru(emb, hidden)\n",
    "        res = F.log_softmax(self.out(res[:,0]))\n",
    "        return res, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This graph demonstrates the accuracy decay for a neural translation task. With an encoding/decoding technique, larger input sequences result in less accuracy.\n",
    "\n",
    "<img src=\"https://smerity.com/media/images/articles/2016/bahdanau_attn.png\" width=\"600\">\n",
    "\n",
    "This can be mitigated using an attentional model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding broadcasting to Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using *broadcasting* makes a lot of numerical programming far simpler. Here's a couple of examples, using numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1, 2, 3]), (3,))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v=np.array([1,2,3]); v, v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1, 2, 3],\n",
       "        [2, 4, 6],\n",
       "        [3, 6, 9]]), (3, 3))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m=np.array([v,v*2,v*3]); m, m.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2,  4,  6],\n",
       "       [ 3,  6,  9],\n",
       "       [ 4,  8, 12]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m+v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1],\n",
       "        [2],\n",
       "        [3]]), (3, 1))"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1=np.expand_dims(v,-1); v1, v1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2,  3,  4],\n",
       "       [ 4,  6,  8],\n",
       "       [ 6,  9, 12]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m+v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unit_prefix(x, n=1):\n",
    "    for i in range(n): x = x.unsqueeze(0)\n",
    "    return x\n",
    "\n",
    "def align(x, y, start_dim=2):\n",
    "    xd, yd = x.dim(), y.dim()\n",
    "    if xd > yd: y = unit_prefix(y, xd - yd)\n",
    "    elif yd > xd: x = unit_prefix(x, yd - xd)\n",
    "\n",
    "    xs, ys = list(x.size()), list(y.size())\n",
    "    nd = len(ys)\n",
    "    for i in range(start_dim, nd):\n",
    "        td = nd-i-1\n",
    "        if   ys[td]==1: ys[td] = xs[td]\n",
    "        elif xs[td]==1: xs[td] = ys[td]\n",
    "    return x.expand(*xs), y.expand(*ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def aligned_op(x,y,f): return f(*align(x,y,0))\n",
    "\n",
    "def add(x, y): return aligned_op(x, y, operator.add)\n",
    "def sub(x, y): return aligned_op(x, y, operator.sub)\n",
    "def mul(x, y): return aligned_op(x, y, operator.mul)\n",
    "def div(x, y): return aligned_op(x, y, operator.truediv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dot(x, y):\n",
    "    assert(1<y.dim()<5)\n",
    "    x, y = align(x, y)\n",
    "    \n",
    "    if y.dim() == 2: return x.mm(y)\n",
    "    elif y.dim() == 3: return x.bmm(y)\n",
    "    else:\n",
    "        xs,ys = x.size(), y.size()\n",
    "        res = torch.zeros(*(xs[:-1] + (ys[-1],)))\n",
    "        for i in range(xs[0]): res[i].baddbmm_(x[i], (y[i]))\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Arr(*sz): return torch.randn(sz)/math.sqrt(sz[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m = Arr(3, 2); m2 = Arr(4, 3)\n",
    "v = Arr(2)\n",
    "b = Arr(4,3,2); t = Arr(5,4,3,2)\n",
    "\n",
    "mt,bt,tt = m.transpose(0,1), b.transpose(1,2), t.transpose(2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_eq(x,y): assert(torch.equal(x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "check_eq(dot(m,mt),m.mm(mt))\n",
    "check_eq(dot(v,mt), v.unsqueeze(0).mm(mt))\n",
    "check_eq(dot(b,bt),b.bmm(bt))\n",
    "check_eq(dot(b,mt),b.bmm(unit_prefix(mt).expand_as(bt)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "exp = t.view(-1,3,2).bmm(tt.contiguous().view(-1,2,3)).view(5,4,3,3)\n",
    "check_eq(dot(t,tt),exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "check_eq(add(m,v),m+unit_prefix(v).expand_as(m))\n",
    "check_eq(add(v,m),m+unit_prefix(v).expand_as(m))\n",
    "check_eq(add(m,t),t+unit_prefix(m,2).expand_as(t))\n",
    "check_eq(sub(m,v),m-unit_prefix(v).expand_as(m))\n",
    "check_eq(mul(m,v),m*unit_prefix(v).expand_as(m))\n",
    "check_eq(div(m,v),m/unit_prefix(v).expand_as(m))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Var(*sz): return Parameter(Arr(*sz)).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, embs, hidden_size, n_layers=2, p=0.1):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.emb, emb_size, output_size = create_emb(embs)\n",
    "        self.W1 = Var(hidden_size, hidden_size)\n",
    "        self.W2 = Var(hidden_size, hidden_size)\n",
    "        self.W3 = Var(emb_size+hidden_size, hidden_size)\n",
    "        self.b2 = Var(hidden_size)\n",
    "        self.b3 = Var(hidden_size)\n",
    "        self.V = Var(hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, num_layers=2)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, inp, hidden, enc_outputs):\n",
    "        emb_inp = self.emb(inp)\n",
    "        w1e = dot(enc_outputs, self.W1)\n",
    "        w2h = add(dot(hidden[-1], self.W2), self.b2).unsqueeze(1)\n",
    "        u = F.tanh(add(w1e, w2h))\n",
    "        a = mul(self.V,u).sum(2)\n",
    "        a = F.softmax(a).unsqueeze(2)\n",
    "        Xa = mul(a, enc_outputs).sum(1)\n",
    "        res = dot(torch.cat([emb_inp, Xa.squeeze(1)], 1), self.W3)\n",
    "        res = add(res, self.b3).unsqueeze(0)\n",
    "        res, hidden = self.gru(res, hidden)\n",
    "        res = F.log_softmax(self.out(res.squeeze(0)))\n",
    "        return res, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch(x, y, batch_size=16):\n",
    "    idxs = np.random.permutation(len(x))[:batch_size]\n",
    "    return x[idxs], y[idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 30])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_size = 128\n",
    "fra, eng = get_batch(fr_train, en_train, 4)\n",
    "inp = long_t(fra)\n",
    "targ = long_t(eng)\n",
    "emb, emb_size, output_size = create_emb(en_emb_t)\n",
    "emb.cuda()\n",
    "inp.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = Var(hidden_size, hidden_size)\n",
    "W2 = Var(hidden_size, hidden_size)\n",
    "W3 = Var(emb_size+hidden_size, hidden_size)\n",
    "b2 = Var(1,hidden_size)\n",
    "b3 = Var(1,hidden_size)\n",
    "V = Var(1,1,hidden_size)\n",
    "gru = nn.GRU(hidden_size, hidden_size, num_layers=2).cuda()\n",
    "out = nn.Linear(hidden_size, output_size).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = EncoderRNN(fr_emb_t, hidden_size).cuda()\n",
    "# decoder = AttnDecoderRNN(en_emb_t, hidden_size).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'torch.nn.functional' from '/home/cs/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py'>"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 30, 128]), torch.Size([2, 4, 128]))"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_inputs, enc_outputs, hidden = encode(inp, encoder)\n",
    "enc_outputs.size(), hidden.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 100])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_inp = emb(dec_inputs); emb_inp.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 30, 128])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1e = dot(enc_outputs, W1); w1e.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1, 128])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2h = dot(hidden[-1], W2)\n",
    "w2h = (w2h+b2.expand_as(w2h)).unsqueeze(1); w2h.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 30, 128])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u = F.tanh(w1e + w2h.expand_as(w1e))\n",
    "u.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 30, 128])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = (V.expand_as(u)*u)\n",
    "a.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=a.sum(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 30])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = F.softmax(a).unsqueeze(2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 30, 1])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 1.0000\n",
       " 1.0000\n",
       " 1.0000\n",
       " 1.0000\n",
       "[torch.cuda.FloatTensor of size 4 (GPU 0)]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.sum(1).squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 128])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xa = (a.expand_as(enc_outputs) * enc_outputs).sum(1); Xa.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 128])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = dot(torch.cat([emb_inp, Xa.squeeze(1)], 1), W3)\n",
    "res = (res+b3.expand_as(res)).unsqueeze(0); res.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 4, 128]), torch.Size([2, 4, 128]))"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res, hidden = gru(res, hidden); res.size(), hidden.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 19549])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = F.log_softmax(out(res.squeeze(0))); res.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(inp, targ, encoder, decoder, enc_opt, dec_opt, crit):\n",
    "    decoder_input, encoder_outputs, hidden = encode(inp, encoder)\n",
    "    target_length = targ.size()[1]\n",
    "    \n",
    "    enc_opt.zero_grad(); dec_opt.zero_grad()\n",
    "    loss = 0\n",
    "\n",
    "    for di in range(target_length):\n",
    "        decoder_output, hidden = decoder(decoder_input, hidden, encoder_outputs)\n",
    "        decoder_input = targ[:, di]\n",
    "        loss += crit(decoder_output, decoder_input)\n",
    "\n",
    "    loss.backward()\n",
    "    enc_opt.step(); dec_opt.step()\n",
    "    return loss.data[0] / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def req_grad_params(o):\n",
    "    return (p for p in o.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainEpochs(encoder, decoder, n_epochs, print_every=1000, lr=0.01):\n",
    "    loss_total = 0 # Reset every print_every\n",
    "    \n",
    "    enc_opt = optim.RMSprop(req_grad_params(encoder), lr=lr)\n",
    "    dec_opt = optim.RMSprop(decoder.parameters(), lr=lr)\n",
    "    crit = nn.NLLLoss().cuda()\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        fra, eng = get_batch(fr_train, en_train, 64)\n",
    "        inp = long_t(fra)\n",
    "        targ = long_t(eng)\n",
    "        loss = train(inp, targ, encoder, decoder, enc_opt, dec_opt, crit)\n",
    "        loss_total += loss\n",
    "\n",
    "        if epoch % print_every == print_every-1:\n",
    "            print('%d %d%% %.4f' % (epoch, epoch / n_epochs * 100, loss_total / print_every))\n",
    "            loss_total = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 128\n",
    "encoder = EncoderRNN(fr_emb_t, hidden_size).cuda()\n",
    "decoder = AttnDecoderRNN(en_emb_t, hidden_size).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "499 4% 2.3886\n",
      "999 9% 1.8825\n",
      "1499 14% 1.6757\n",
      "1999 19% 1.5435\n",
      "2499 24% 1.4516\n",
      "2999 29% 1.3766\n",
      "3499 34% 1.3176\n",
      "3999 39% 1.2719\n",
      "4499 44% 1.2305\n",
      "4999 49% 1.2076\n",
      "5499 54% 1.1753\n",
      "5999 59% 1.1456\n",
      "6499 64% 1.1268\n",
      "6999 69% 1.1034\n",
      "7499 74% 1.0838\n",
      "7999 79% 1.0719\n",
      "8499 84% 1.0487\n",
      "8999 89% 1.0319\n",
      "9499 94% 1.0279\n",
      "9999 99% 1.0180\n"
     ]
    }
   ],
   "source": [
    "trainEpochs(encoder, decoder, 10000, print_every=500, lr=0.005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(inp):\n",
    "    decoder_input, encoder_outputs, hidden = encode(inp, encoder)\n",
    "    target_length = maxlen\n",
    "\n",
    "    decoded_words = []\n",
    "    for di in range(target_length):\n",
    "        decoder_output, hidden = decoder(decoder_input, hidden, encoder_outputs)\n",
    "        topv, topi = decoder_output.data.topk(1)\n",
    "        ni = topi[0][0]\n",
    "        if ni==PAD: break\n",
    "        decoded_words.append(en_vocab[ni])\n",
    "        decoder_input = long_t([ni])\n",
    "    \n",
    "    return decoded_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sent2ids(sent):\n",
    "    ids = [fr_w2id[t] for t in simple_toks(sent)]\n",
    "    return pad_sequences([ids], maxlen, 'int64', \"post\", \"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fr2en(sent): \n",
    "    ids = long_t(sent2ids(sent))\n",
    "    trans = evaluate(ids)\n",
    "    return ' '.join(trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the population of Canada? Quelle est la population du Canada ?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'what is the population of canada ?'"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i=8\n",
    "print(en_qs[i],fr_qs[i])\n",
    "fr2en(fr_qs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a=\"Quelle est tu fais\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'what is your favorite ?'"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fr2en(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
